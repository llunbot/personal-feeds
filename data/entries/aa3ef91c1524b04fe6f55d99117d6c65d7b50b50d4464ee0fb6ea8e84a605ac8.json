{"title":"Protect data privacy in Amazon Bedrock with Vault","link":"https://www.hashicorp.com/blog/protect-data-privacy-in-amazon-bedrock-with-vault","date":1743523200000,"content":"<p>You can use retrieval augmented generation (RAG) to refine and improve the output of a large language model (LLM) without retraining the model. However, many data sources include sensitive information, such as personal identifiable information (PII), that the LLM and its applications should not require or disclose — but sometimes they do. Sensitive information disclosure is one of the OWASP <a href=\"https://genai.owasp.org/llm-top-10/\">2025 Top 10 Risks &amp; Mitigations for LLMs and Gen AI Apps</a>. To mitigate this issue, OWASP recommends data sanitization, access control, and encryption.</p>\n\n<p>This post shows how HashiCorp Vault’s <a href=\"https://developer.hashicorp.com/vault/docs/secrets/transit\">transit secrets engine</a> can be configured to encrypt and protect sensitive data before sending it to an <a href=\"https://aws.amazon.com/bedrock/knowledge-bases/\">Amazon Bedrock Knowledge Base</a> created by Terraform. </p>\n\n<p>The sample dataset contains a <a href=\"https://insideairbnb.com/get-the-data/\">list of vacation rentals from Airbnb</a>, which includes the names of the hosts. Not all applications require the names of the hosts, so the original application collecting the information encrypts the field using Vault before storing a rental listing in the database. As a result, running queries against the Amazon Bedrock Knowledge Base outputs the encrypted name and prevents the leakage of PII.</p>\n<img src=\"https://www.datocms-assets.com/2885/1743520669-bedrock_diagram.png\" alt=\"Vault\" /><h2>Encrypt and upload data</h2>\n\n<p>The demo uses an <a href=\"https://developer.hashicorp.com/hcp/docs/vault/what-is-hcp-vault\">HCP Vault</a> cluster with the transit secrets engine enabled. The host name for each rental gets encrypted by a key named <code>listings</code> with <a href=\"https://developer.hashicorp.com/vault/docs/secrets/transit#convergent-encryption\">convergent encryption</a> enabled. Convergent encryption ensures that a plaintext host name results in the same ciphertext. This allows the LLM to analyze each rental listing for similarities between hosts without knowing the actual host name.</p>\n<pre><code>resource \"hcp_hvn\" \"rental\" {\n hvn_id         = var.name\n cloud_provider = \"aws\"\n region         = var.region\n cidr_block     = var.cidr_block\n}\n\n\nresource \"hcp_vault_cluster\" \"rental\" {\n cluster_id      = var.name\n hvn_id          = hcp_hvn.rental.hvn_id\n tier            = \"plus_small\"\n public_endpoint = true\n}\n\n\nresource \"hcp_vault_cluster_admin_token\" \"rental\" {\n cluster_id = hcp_vault_cluster.rental.cluster_id\n}\n\n\nresource \"vault_mount\" \"transit_rental\" {\n path                      = var.name\n type                      = \"transit\"\n description               = \"Key ring for rental information\"\n default_lease_ttl_seconds = 3600\n max_lease_ttl_seconds     = 86400\n}\n\n\nresource \"vault_transit_secret_backend_key\" \"listings\" {\n backend               = vault_mount.transit_rental.path\n name                  = \"listings\"\n derived               = true\n convergent_encryption = true\n deletion_allowed      = true\n}</code></pre><p>To process a <a href=\"https://data.insideairbnb.com/united-states/ny/new-york-city/2025-01-03/data/listings.csv.gz\">CSV file</a> with a list of vacation rentals in New York City from January 2025, you would encrypt the name of the host as if it is sensitive data. Create a local script that uses the <a href=\"https://github.com/hvac/hvac\">HVAC Python client for HashiCorp Vault</a> to access the encryption API endpoint in Vault.</p>\n<pre><code>import base64\nimport json\nimport logging\nimport os\n\nimport boto3\nimport hvac\nimport pandas\nfrom botocore.exceptions import ClientError\nfrom langchain_community.document_loaders import CSVLoader\n\nfrom vardata import S3_BUCKET_NAME\n\nLISTINGS_FILE = \"./data/raw/listings.csv\"\nENCRYPTED_LISTINGS_FILE = \"./data/listings.csv\"\nMOUNT_POINT = \"rentals\"\nKEY_NAME = \"listings\"\nCONTEXT = json.dumps({\"location\": \"New York City\", \"field\": \"host_name\"})\n\nclient = hvac.Client(\n   url=os.environ[\"VAULT_ADDR\"],\n   token=os.environ[\"VAULT_TOKEN\"],\n   namespace=os.getenv(\"VAULT_NAMESPACE\"),\n)\n\ndef encrypt_payload(payload):\n   try:\n       encrypt_data_response = client.secrets.transit.encrypt_data(\n           mount_point=MOUNT_POINT,\n           name=KEY_NAME,\n           plaintext=base64.b64encode(payload.encode()).decode(),\n           context=base64.b64encode(CONTEXT.encode()).decode(),\n       )\n       ciphertext = encrypt_data_response[\"data\"][\"ciphertext\"]\n       return ciphertext\n   except AttributeError:\n       return \"\"\n\ndef encrypt_hostnames():\n   dataframe = pandas.read_csv(LISTINGS_FILE)\n   dataframe[\"host_name\"] = dataframe[\"host_name\"].apply(lambda x: encrypt_payload(x))\n   dataframe.to_csv(ENCRYPTED_LISTINGS_FILE, index=False)\n\n# omitted for clarity</code></pre><p>This example data would usually exist in a database. A separate application that adds the listing would encrypt and store the data in the database before it gets exported to CSV for reporting. While you could upload the CSV file in its entirety as a document, for this demo, continue processing the rental listings by uploading each record as its own document to debug and improve the LLM’s responses. For more information on how to process semi-structured data for Amazon Bedrock, <a href=\"https://github.com/aws-samples/semi-structured-data-processing-with-amazon-bedrock\">review the AWS samples repository</a>.</p>\n\n<p>Each entry in the CSV file gets written as its own document to the S3 bucket. This example uses <a href=\"https://python.langchain.com/docs/integrations/document_loaders/csv/\">LangChain</a> to convert each CSV record to a text file and upload it to S3.</p>\n<pre><code>import base64\nimport json\nimport logging\nimport os\n\nimport boto3\nimport hvac\nimport pandas\nfrom botocore.exceptions import ClientError\nfrom langchain_community.document_loaders import CSVLoader\n\nfrom vardata import S3_BUCKET_NAME\n\nLISTINGS_FILE = \"./data/raw/listings.csv\"\nENCRYPTED_LISTINGS_FILE = \"./data/listings.csv\"\nMOUNT_POINT = \"rentals\"\nKEY_NAME = \"listings\"\n\n# omitted for clarity\n\ndef create_documents():\n   loader = CSVLoader(ENCRYPTED_LISTINGS_FILE)\n   data = loader.load()\n   return data\n\ndef upload_file(body, bucket, object):\n   s3_client = boto3.client(\"s3\")\n   try:\n       s3_client.put_object(Body=body, Bucket=bucket, Key=object)\n   except ClientError as e:\n       logging.error(e)\n       return False\n   return True\n\ndef main():\n   encrypt_hostnames()\n   docs = create_documents()\n   for i, doc in enumerate(docs):\n       upload_file(doc.page_content, S3_BUCKET_NAME, f\"listings/{i}\")\n\nif __name__ == \"__main__\":\n   main()\n</code></pre><p>This script is intended for educational and testing purposes only. In a production use case, you can process <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html\">data using AWS Sagemaker</a>. </p>\n\n<p>The script creates a text file with the name of the host in ciphertext. All other non-sensitive attributes, such as room type and listing ID, remain in plaintext.</p>\n<pre><code>id: 1284789\nname: Light-filled Brownstone Triplex with Roof Deck\nhost_id: 5768571\nhost_name: vault:v1:ocqgeegsqMeo2PawbxsK8lQ1Sqp5/VVGqftD4DUbx1iKMw==\nneighbourhood_group: Brooklyn\nneighbourhood: Prospect Heights\nlatitude: 40.67506\nlongitude: -73.96423\nroom_type: Entire home/apt\nprice: \nminimum_nights: 30\nnumber_of_reviews: 4\nlast_review: 2023-07-28\nreviews_per_month: 0.06\ncalculated_host_listings_count: 1\navailability_365: 0\nnumber_of_reviews_ltm: 0\nlicense:</code></pre><p>After processing the data, set up an Amazon Bedrock Knowledge Base to ingest the documents from S3 as a data source. If you stored the data in Amazon Aurora, Amazon Redshift, or Glue Data Catalog, you can set up a knowledge base to <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-build-structured.html\">ingest from a structured data source</a>.</p>\n\n<h2>Set up a vector store</h2>\n\n<p>Amazon Bedrock Knowledge Bases allow you to add proprietary information into applications using RAG. To ingest a data source from S3, Bedrock Knowledge Bases requires the following:</p>\n\n<ol>\n<li>Sufficient IAM policies to connect to S3</li>\n<li>A supported <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-setup.html\">vector store</a> for embeddings.</li>\n</ol>\n\n<p>This demo uses <a href=\"https://aws.amazon.com/opensearch-service/features/serverless/\">Amazon OpenSearch Serverless</a> to provision an OpenSearch cluster for vector embeddings. The cluster requires additional IAM policies to access collections and indexes that store the vector embeddings.</p>\n\n<p>Below are two security policies: one for the encryption of the collection with embeddings and a second for network access to the collection and dashboard. The security policy for network access to the collection and dashboard ensures that Terraform can properly create an index.</p>\n<pre><code>resource \"aws_opensearchserverless_security_policy\" \"rentals_encryption\" {\n name = var.name\n type = \"encryption\"\n policy = jsonencode({\n   \"Rules\" = [\n     {\n       \"Resource\" = [\n         \"collection/${var.name}\"\n       ],\n       \"ResourceType\" = \"collection\"\n     },\n   ],\n   \"AWSOwnedKey\" = true\n })\n}\n\n\nresource \"aws_opensearchserverless_security_policy\" \"rentals_network\" {\n name = var.name\n type = \"network\"\n policy = jsonencode([{\n   \"Rules\" = [\n     {\n       \"Resource\" = [\n         \"collection/${var.name}\"\n       ],\n       \"ResourceType\" = \"collection\"\n     },\n     {\n       \"Resource\" = [\n         \"collection/${var.name}\"\n       ],\n       \"ResourceType\" = \"dashboard\"\n     },\n   ],\n   \"AllowFromPublic\" = true,\n }])\n}\n</code></pre><p>The configuration below generates an access policy to allow Amazon Bedrock and your current AWS credentials in Terraform to read and write to the index with the collection of the embeddings.</p>\n<pre><code>data \"aws_caller_identity\" \"current\" {}\n\nresource \"aws_opensearchserverless_access_policy\" \"rentals\" {\n name        = var.name\n type        = \"data\"\n description = \"read and write permissions\"\n policy = jsonencode([\n   {\n     Rules = [\n       {\n         ResourceType = \"index\",\n         Resource = [\n           \"index/${var.name}/*\"\n         ],\n         Permission = [\n           \"aoss:*\"\n         ]\n       },\n       {\n         ResourceType = \"collection\",\n         Resource = [\n           \"collection/${var.name}\"\n         ],\n         Permission = [\n           \"aoss:*\"\n         ]\n       }\n     ],\n     Principal = [\n       aws_iam_role.bedrock.arn,\n       data.aws_caller_identity.current.arn\n     ]\n   }\n ])\n}</code></pre><p>After creating these policies, Terraform can now build the collection. For Amazon Bedrock Knowledge Bases, set the type to <code>VECTORSEARCH</code>.</p>\n<pre><code>resource \"aws_opensearchserverless_collection\" \"rentals\" {\n name = var.name\n type = \"VECTORSEARCH\"\n\n\n depends_on = [\n   aws_opensearchserverless_security_policy.rentals_encryption,\n   aws_opensearchserverless_security_policy.rentals_network\n ]\n}</code></pre><p>Finally, you need to create an index for the vector embeddings collection in the OpenSearch cluster. Based on <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-setup.html\">prerequisites for bringing your own vector store</a>, configure an index using the <a href=\"https://registry.terraform.io/providers/opensearch-project/opensearch/latest/docs\">OpenSearch Provider for Terraform</a>. This example uses <a href=\"https://aws.amazon.com/bedrock/amazon-models/titan/\">Amazon Titan</a> 2.0 embeddings, which have 1024 dimensions in the vector.</p>\n<pre><code>provider \"opensearch\" {\n url         = aws_opensearchserverless_collection.rentals.collection_endpoint\n healthcheck = false\n}\n\n\nlocals {\n vector_field   = \"bedrock-knowledge-base-default-vector\"\n metadata_field = \"AMAZON_BEDROCK_METADATA\"\n text_field     = \"AMAZON_BEDROCK_TEXT_CHUNK\"\n}\n\n\nresource \"opensearch_index\" \"bedrock_knowledge_base\" {\n name                           = \"bedrock-knowledge-base-default-index\"\n number_of_shards               = \"2\"\n number_of_replicas             = \"0\"\n index_knn                      = true\n index_knn_algo_param_ef_search = \"512\"\n mappings                       = &lt;&lt;-EOF\n   {\n     \"properties\": {\n       \"${local.vector_field}\": {\n         \"type\": \"knn_vector\",\n         \"dimension\": 1024,\n         \"method\": {\n           \"name\": \"hnsw\",\n           \"engine\": \"faiss\",\n           \"parameters\": {\n             \"m\": 16,\n             \"ef_construction\": 512\n           },\n           \"space_type\": \"l2\"\n         }\n       },\n       \"${local.metadata_field}\": {\n         \"type\": \"text\",\n         \"index\": \"false\"\n       },\n       \"${local.text_field}\": {\n         \"type\": \"text\",\n         \"index\": \"true\"\n       }\n     }\n   }\n EOF\n}</code></pre><p>After configuring the vector store, you can create an Amazon Bedrock Knowledge Base for the S3 bucket with rental listings.</p>\n\n<h2>Configure RAG in Amazon Bedrock</h2>\n\n<p>In order to augment the responses from an LLM with rental listing information, you need to create an <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/kb-how-it-works.html\">Amazon Bedrock Knowledge Base</a> to facilitate RAG. The knowledge base requires sufficient IAM access to the S3 bucket with rental listings and the OpenSearch collection. In addition to the data source and collection access, Amazon Bedrock’s IAM role also needs permissions to invoke the model used for embeddings.</p>\n<pre><code>resource \"aws_iam_policy\" \"bedrock\" {\n name        = \"bedrock-${var.name}-s3\"\n path        = \"/\"\n description = \"Allow Bedrock Knowledge Base to access S3 bucket with rentals\"\n\n policy = jsonencode({\n   Version = \"2012-10-17\"\n   Statement = [\n     {\n       Action = [\n         \"s3:ListBucket\",\n       ],\n       Effect   = \"Allow\",\n       Resource = aws_s3_bucket.rentals.arn,\n       Condition = {\n         StringEquals = {\n           \"aws:ResourceAccount\" = data.aws_caller_identity.current.account_id\n         }\n       }\n     },\n     {\n       Action = [\n         \"s3:GetObject\",\n       ],\n       Effect   = \"Allow\",\n       Resource = \"${aws_s3_bucket.rentals.arn}/*\",\n       Condition = {\n         StringEquals = {\n           \"aws:ResourceAccount\" = data.aws_caller_identity.current.account_id\n         }\n       }\n     },\n     {\n       Action = [\n         \"aoss:APIAccessAll\"\n       ],\n       Effect   = \"Allow\",\n       Resource = aws_opensearchserverless_collection.rentals.arn,\n     },\n     {\n       Action = [\n         \"bedrock:InvokeModel\"\n       ],\n       Effect   = \"Allow\",\n       Resource = data.aws_bedrock_foundation_model.embedding.model_arn,\n     }\n   ]\n })\n}\n\nresource \"aws_iam_role\" \"bedrock\" {\n name_prefix = \"bedrock-${var.name}-\"\n\n assume_role_policy = jsonencode({\n   Version = \"2012-10-17\"\n   Statement = [\n     {\n       Action = \"sts:AssumeRole\"\n       Effect = \"Allow\"\n       Principal = {\n         Service = \"bedrock.amazonaws.com\"\n       },\n       Condition = {\n         StringEquals = {\n           \"aws:SourceAccount\" = data.aws_caller_identity.current.account_id\n         },\n       }\n     },\n   ]\n })\n}\n\nresource \"aws_iam_role_policy_attachment\" \"bedrock\" {\n role       = aws_iam_role.bedrock.name\n policy_arn = aws_iam_policy.bedrock.arn\n}</code></pre><p>Configure the Amazon Bedrock knowledge base in Terraform. It uses the IAM role with permissions to the S3 bucket, OpenSearch collection, and embedding model. It also defines the field mappings defined in the OpenSearch index for the vector embeddings.</p>\n<pre><code>resource \"aws_bedrockagent_knowledge_base\" \"rentals\" {\n name     = var.name\n role_arn = aws_iam_role.bedrock.arn\n knowledge_base_configuration {\n   vector_knowledge_base_configuration {\n     embedding_model_arn = data.aws_bedrock_foundation_model.embedding.model_arn\n   }\n   type = \"VECTOR\"\n }\n\n storage_configuration {\n   type = \"OPENSEARCH_SERVERLESS\"\n   opensearch_serverless_configuration {\n     collection_arn    = aws_opensearchserverless_collection.rentals.arn\n     vector_index_name = opensearch_index.bedrock_knowledge_base.name\n     field_mapping {\n       vector_field   = local.vector_field\n       text_field     = local.text_field\n       metadata_field = local.metadata_field\n     }\n   }\n }\n}</code></pre><p>Create a data source for the knowledge base linked to the Amazon S3 bucket with rental listings.</p>\n<pre><code>resource \"aws_bedrockagent_data_source\" \"listings\" {\n knowledge_base_id = aws_bedrockagent_knowledge_base.rentals.id\n name              = \"listings\"\n data_source_configuration {\n   type = \"S3\"\n   s3_configuration {\n     bucket_arn = aws_s3_bucket.rentals.arn\n   }\n }\n}</code></pre><p>After deploying this in Terraform, you can test the knowledge base with some requests.</p>\n\n<h2>Test the knowledge base</h2>\n\n<p>When you ask questions to the knowledge base with information about rental listings, you should get responses with additional detail based on the listings.</p>\n<img src=\"https://www.datocms-assets.com/2885/1743522257-bedrock_response.png\" alt=\"Claude\" /><p>This example asks Anthropic Claude 3.7 Sonnet how many vacation rentals are in the Box House Hotel. It returned a response indicating that it searched the rental listings and found at least ten of them.</p>\n\n<pre><code>Input:\nHow many vacation rentals are in the Box House Hotel?\n\nOutput:\nBased on the search results, there are at least 10 vacation rentals listed at \nthe Box House Hotel in Greenpoint, Brooklyn. These include various room types \nsuch as Superior rooms, Lofts with Terraces, Duplex rooms with Terraces, and \nmulti-bedroom options. All of these listings share the same host \n(host_id: 417504) and are located at approximately the same coordinates in the \nGreenpoint neighborhood of Brooklyn.\n</code></pre>\n\n<p>Loading data into Amazon Bedrock improved the response by adding new information that the model did not have for training. Since you took additional steps to encrypt the names of the hosts and protect them as sensitive data, the model should not return information with the plaintext names of the hosts. </p>\n\n<p>The example below asks an additional question about the name of the host who owns these vacation rental listings. After some searching, the model responds that the host name is encrypted and it found 29 other listings with the same ciphertext.</p>\n\n<pre><code>Input:\nWhat is the name of the host for these vacation rental listings in the Box House Hotel?\n\nOutput:\nThe host name for Box House Hotel (host_id: 417504) appears to be encrypted in \nthe database. In the search results, the host_name field for this host is shown \nas \"vault:v1:gCMhLikagB3DgHEzpcc0TJbnreYNrS39w9eiHwxUyMOVR5ClyzUaFEnqZC+Ohhk=\" \nacross all their listings.\n</code></pre>\n\n<p>This host appears to manage multiple properties under the Box House Hotel brand in the Greenpoint neighborhood of Brooklyn, with 29 total listings according to the <code>calculated_host_listings_count</code> field.</p>\n\n<p>Convergent encryption ensures the same names of hosts result in the same ciphertext, which allows the LLM to still analyze the data and identify that the same host offers 29 rental properties. If an LLM application needs to respond with the plaintext name of the host, it can use the Vault transit secrets engine to decrypt the ciphertext before returning the response.</p>\n\n<h2>Learn more</h2>\n\n<p>By encrypting sensitive data before augmenting a LLM with RAG, you can protect access to the data and prevent leakage of sensitive information. In this demo, an LLM in testing and other applications by default do not need access to the plaintext name of the host for the vacation rental. They can still analyze and provide other information without leaking the name of the host for each vacation rental listing.</p>\n\n<p>For applications that need access to the plaintext name of the host, they can implement additional code to decrypt the payload with Vault <a href=\"https://developer.hashicorp.com/vault/tutorials/encryption-as-a-service/eaas-transit\">transit secrets engine</a>. This ensures that only authorized applications have access to Vault’s decryption endpoint to reveal sensitive data. Vault offers additional <a href=\"https://www.hashicorp.com/blog/how-to-choose-a-data-protection-method\">advanced data protection techniques</a> such as <a href=\"https://developer.hashicorp.com/vault/tutorials/encryption-as-a-service/transform\">format-preserving encryption</a>, masking, and <a href=\"https://developer.hashicorp.com/vault/tutorials/encryption-as-a-service/tokenization\">data tokenization</a> using the <a href=\"https://developer.hashicorp.com/vault/docs/secrets/transform\">transform secrets engine</a>. The transform secrets engine further masks or sanitizes sensitive information while allowing the LLM and its applications to process data.</p>\n\n<p>To learn more about Amazon Bedrock Knowledge Bases, check out its <a href=\"https://aws.amazon.com/bedrock/knowledge-bases/\">documentation</a>. The OWASP <a href=\"https://genai.owasp.org/llm-top-10/\">Top 10 Risks &amp; Mitigations for LLMs and Gen AI Apps</a> includes a list of risks and mitigations for LLMs and Gen AI Apps, including some recommendations to prevent sensitive data disclosure. </p>\n","author":"Rosemary Wang","siteTitle":"HashiCorp Blog","siteHash":"219aa6310b3388f2335eba49871f4df9581f2c58eaeb5e498363b54e835b7001","entryHash":"aa3ef91c1524b04fe6f55d99117d6c65d7b50b50d4464ee0fb6ea8e84a605ac8","category":"Tech"}