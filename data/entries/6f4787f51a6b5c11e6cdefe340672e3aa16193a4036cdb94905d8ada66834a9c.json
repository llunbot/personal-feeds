{"title":"Continuous batching to increase LLM inference throughput and reduce p50 latency","link":"https://www.anyscale.com/blog/continuous-batching-llm-inference","date":1692087671000,"content":"<a href=\"https://news.ycombinator.com/item?id=37131477\">Comments</a>","author":"","siteTitle":"Hacker News","siteHash":"37bb545430005dba450c1e40307450d8e4e791b434e83f3d38915ebad510fd50","entryHash":"6f4787f51a6b5c11e6cdefe340672e3aa16193a4036cdb94905d8ada66834a9c","category":"Tech"}