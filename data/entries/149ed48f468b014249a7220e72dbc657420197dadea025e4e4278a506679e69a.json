{"title":"Apple Updates ‘Child Safety’ Webpage to Remove Mention of CSAM Fingerprint Matching, But Feature May Still Be Forthcoming","link":"https://www.theverge.com/2021/12/15/22837631/apple-csam-detection-child-safety-feature-webpage-removal-delay","date":1639611014000,"content":"\n<p>Jon Porter, reporting for The Verge:</p>\n\n<blockquote>\n  <p>Two of the three safety features, which released earlier this week\nwith iOS 15.2, are still present on the page, which is titled\n“Expanded Protections for Children.” However references to the\nmore controversial CSAM detection, whose <a href=\"https://www.theverge.com/2021/9/3/22655644/apple-delays-controversial-child-protection-features-csam-privacy\">launch was delayed</a>\nfollowing backlash from privacy advocates, have been removed.</p>\n\n<p>When reached for comment, Apple spokesperson Shane Bauer said that\nthe company’s position hasn’t changed since September, when it\nfirst announced it would be delaying the launch of the CSAM\ndetection. “Based on feedback from customers, advocacy groups,\nresearchers, and others, we have decided to take additional time\nover the coming months to collect input and make improvements\nbefore releasing these critically important child safety\nfeatures,” the company’s September statement read.</p>\n\n<p>Crucially, Apple’s statement does not say the feature has been\ncanceled entirely. <a href=\"https://www.apple.com/child-safety/pdf/CSAM_Detection_Technical_Summary.pdf\">Documents</a> <a href=\"https://www.apple.com/child-safety/pdf/Expanded_Protections_for_Children_Frequently_Asked_Questions.pdf\">outlining</a> how the\nfunctionality works are still live on Apple’s site.</p>\n</blockquote>\n\n<p>I wouldn’t read too much into this. Now that <em>some</em> of the new child safety features are shipping with this week’s iOS 15.2 update (machine-learning-based nude/sexually-explicit image detection in Messages, and “Expanded guidance in Siri, Spotlight, and Safari Search”), Apple has updated the page to state which features are currently shipping.</p>\n\n<p>I think the CSAM fingerprinting, in some form, is still forthcoming, because I suspect Apple wants to change iCloud Photos storage to use end-to-end encryption. Concede for the moment that CSAM identification <em>needs</em> to happen somewhere, for a large cloud service like iCloud. If that identification takes place server-side, then the service <em>cannot</em> use E2E encryption — it can’t identify what it can’t decrypt. If the sync service <em>does</em> use E2E encryption — which I’d love to see iCloud Photos do — then such matching has to take place on the device side. Doing that identification via fingerprinting against a database of known and vetted CSAM imagery is far more private than using machine learning.</p>\n\n<p>I also continue <a href=\"https://daringfireball.net/2021/08/apple_child_safety_initiatives_slippery_slope\">not to agree, at all, with the “slippery slope” argument</a>, which goes along the lines of “authoritarian regimes around the world will force Apple to add non-CSAM image fingerprints to the database”. Machine learning algorithms are far more ripe for that sort of abuse than fingerprint matching. Machine learning can be crazy smart; fingerprint matching, by design, is a bit simplistic. Apple’s Photos app <em>already</em> uses very clever machine learning to identify the content of photos in your library. Search in the Photos app for “dog” or “cocktail” or <em>someone’s name</em> and it’s going to find those photos. Trust in Apple is the only thing protecting iOS users from surreptitious abuse of machine learning in Photos <em>now</em> — which is no different from Android users’ trust in Google for the same sort of thing.</p>\n\n<p>Put another way, if governments, authoritarian or otherwise, were able to force Apple (or Google, or Microsoft) to add secret snooping features — like say finding photos of Tank Man on Chinese users’ devices and reporting them to the CCP — to our operating systems, the game is over. They wouldn’t need this proposed device-side CSAM fingerprinting feature to abuse, they could just demand <em>whatever they want</em>. Access to your email, everything.</p>\n\n<div>\n<a href=\"https://daringfireball.net/linked/2021/12/15/apple-child-safety-csam\"> ★ </a>\n</div>\n\n\t","author":"John Gruber","siteTitle":"Daring Fireball","siteHash":"fc569638025dadf22a867470f8215f38855cf50e975782a6c989909474292a36","entryHash":"149ed48f468b014249a7220e72dbc657420197dadea025e4e4278a506679e69a","category":"Tech"}