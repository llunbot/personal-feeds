{"title":"3 เสาหลักของการสร้าง AI ขนาดใหญ่","link":"https://markpeak.net/three-pillars-of-large-ai/","date":1685928859000,"content":"<p>กระแสของ Large Language Model (LLM) ช่วงปลายปี 2022 ทำให้โลกตื่นเต้นไปกับคลื่น AI รอบใหม่ แต่โฟกัสที่เรามักพูดถึงกันคือเรื่องตัวโมเดลว่าของใครก้าวหน้ากว่ากัน ชื่อที่คุ้นเคยก็อย่าง GPT (OpenAI), PaLM (Google), LLaMA (Meta), Claude (Anthropic) เป็นต้น</p>\n<p>แต่การเกิดขึ้นของโมเดลขนาดใหญ่ลักษณะนี้ จำเป็นต้องมี “เสาหลัก” หรือปัจจัยพื้นฐานอย่างน้อย 3 ข้อ ได้แก่</p>\n<ol>\n<li><strong>Algorithm</strong> – ตัวอัลกอริทึมของโมเดล ซึ่งคิดโดยคน</li>\n<li><strong>Data</strong> – ข้อมูลที่ใช้เทรน</li>\n<li><strong>Compute</strong> – พลังประมวลผลในการเทรน</li>\n</ol>\n<h2>Algorithm</h2>\n<p><strong>Algorithm</strong> ที่มาของอัลกอริทึมย่อมมาจากคน จากนักวิจัยด้าน AI ที่พยายามค้นหาอัลกอริทึมที่ดีขึ้นเรื่อยๆ ดังที่เราได้เห็นการแข่งทำลายสถิติ ImageNet ทุกปี</p>\n<p>ขนาดของโมเดลเองก็ใหญ่ขึ้นเรื่อยๆ จากหลักร้อยล้านพารามิเตอร์ มาสู่หลักแสนล้านพารามิเตอร์ภายในเวลาเพียงไม่กี่ปี</p>\n<div><img loading=\"lazy\" src=\"https://markpeak.net/wp-content/uploads/2023/06/llm-parameter.png\" alt width=\"917\" height=\"353\" srcset=\"https://markpeak.net/wp-content/uploads/2023/06/llm-parameter.png 917w, https://markpeak.net/wp-content/uploads/2023/06/llm-parameter-300x115.png 300w, https://markpeak.net/wp-content/uploads/2023/06/llm-parameter-768x296.png 768w, https://markpeak.net/wp-content/uploads/2023/06/llm-parameter-700x269.png 700w\" /><p>เปรียบเทียบพารามิเตอร์ของโมเดล LLM – <a href=\"https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/llm-models\">Microsoft</a></p></div>\n<p>บริษัทที่มีห้องวิจัย AI ขนาดใหญ่ ทำงานวิจัยระดับฐาน (fundamental research) เช่น Google Brain/DeepMind, <a href=\"https://ai.facebook.com/research/\">FAIR</a>, OpenAI ย่อมได้เปรียบ</p>\n<p>เรื่องอัลกอริทึมนี่คนพูดถึงกันไปเยอะแล้ว คงไม่ต้องฉายหนังซ้ำอีกมากนัก</p>\n<div><img loading=\"lazy\" src=\"https://markpeak.net/wp-content/uploads/2023/06/top-ai-papers.png\" alt width=\"934\" height=\"589\" srcset=\"https://markpeak.net/wp-content/uploads/2023/06/top-ai-papers.png 934w, https://markpeak.net/wp-content/uploads/2023/06/top-ai-papers-300x189.png 300w, https://markpeak.net/wp-content/uploads/2023/06/top-ai-papers-768x484.png 768w, https://markpeak.net/wp-content/uploads/2023/06/top-ai-papers-700x441.png 700w\" /><p>อันดับเปเปอร์ด้าน AI แยกตามองค์กร – <a href=\"https://www.zeta-alpha.com/post/must-read-the-100-most-cited-ai-papers-in-2022\">Zeta-Alpha</a></p></div>\n<h2>Data</h2>\n<p><strong>Data</strong> ข้อมูลที่ใช้เทรน เป็นสิ่งที่คนไม่ค่อยพูดถึงกันนัก แต่เบื้องหลัง GPT คือ<a href=\"https://time.com/6247678/openai-chatgpt-kenya-workers/\">การจ้างพนักงานเอาท์ซอร์สในแอฟริกาเทรนข้อมูล</a> ทำเรื่อง labeling ข้อมูล เพื่อนำมาใช้เทรน</p>\n<div>\n\t\t\t<div>\n\t\t\t\t<a href=\"https://time.com/6247678/openai-chatgpt-kenya-workers/\">\n\t\t\t\t\t<img src=\"https://time.com/https://api.time.com/wp-content/uploads/2023/01/DALL·E-2023-01-09-18.12.05-a-seemingly-endless-view-african-workers-at-desks-in-front-of-computer-screens-in-a-printmaking-style.jpg?quality=85&amp;w=1200&amp;h=628&amp;crop=1\" alt=\"Exclusive: The $2 Per Hour Workers Who Made ChatGPT Safer\" />\t\t\t\t</a>\n\t\t</div>\n\t\n\t<div>\n\t\t<a href=\"https://time.com/6247678/openai-chatgpt-kenya-workers/\">\n\t\t\tExclusive: The $2 Per Hour Workers Who Made ChatGPT Safer\t\t</a>\n\t</div>\n\t<div>\n\t\t<a href=\"https://time.com/6247678/openai-chatgpt-kenya-workers/\">\n\t\t\t<p>A TIME investigation reveals the difficult conditions faced by the workers who made ChatGPT possible</p>\n\t\t</a>\n\t</div>\n\t<div>\n\t\t<img src=\"https://time.com/favicon.ico\" alt=\"Time\" />\t\tTime\t</div>\n</div>\n<p>ตัวอย่างข้อมูลที่ใช้เทรน GPT-3 จะเห็นการผสมผสานของข้อมูลสาธารณะ เช่น <a href=\"https://commoncrawl.org/\">Common Crawl</a> (ดูดจากเว็บ) หรือ Wikipedia  (<a href=\"https://medium.com/@dlaytonj2/chatgpt-show-me-the-data-sources-11e9433d57e8\">source</a>)</p>\n<p><img loading=\"lazy\" src=\"https://markpeak.net/wp-content/uploads/2023/06/gpt3-data-mix.png\" alt width=\"828\" height=\"265\" srcset=\"https://markpeak.net/wp-content/uploads/2023/06/gpt3-data-mix.png 828w, https://markpeak.net/wp-content/uploads/2023/06/gpt3-data-mix-300x96.png 300w, https://markpeak.net/wp-content/uploads/2023/06/gpt3-data-mix-768x246.png 768w, https://markpeak.net/wp-content/uploads/2023/06/gpt3-data-mix-700x224.png 700w\" /></p>\n<p>ข้อมูลที่สามารถใช้เทรนโมเดลได้นั้นมีจำกัด โดยเฉพาะข้อมูลเฉพาะทาง เฉพาะโดเมน เฉพาะอุตสาหกรรม ช่วงหลังๆ เราจึงเห็นบริษัทที่เป็นเจ้าของข้อมูลเริ่ม “หวง” ข้อมูลลักษณะนี้มากขึ้น มีการคิดเงินค่าใช้ข้อมูล เช่น <a href=\"https://www.blognone.com/node/133511\">Reddit</a> หรือฟ้องร้องการละเมิดลิขสิทธิ์ข้อมูล เช่น <a href=\"https://www.blognone.com/node/132528\">Getty</a></p>\n<p>บริษัทที่มีข้อมูลเป็นของตัวเอง เช่น Google หรือ Facebook มีข้อมูลผู้ใช้ มีภาพถ่าย, Microsoft มีเอกสารในชุด Office มีโค้ดใน GitHub, Tesla มีข้อมูลภาพจากกล้องหน้ารถ ฯลฯ ย่อมได้เปรียบกว่าบริษัทอย่าง OpenAI หรือ Anthropic ที่จะหาข้อมูลมาเทรนได้ยากและราคาแพงขึ้นเรื่อยๆ</p>\n<p>ตารางเปรียบเทียบแหล่งข้อมูล training dataset ของโมเดลต่างๆ</p>\n<div><img loading=\"lazy\" src=\"https://markpeak.net/wp-content/uploads/2023/06/llm-data-source.png\" alt width=\"871\" height=\"693\" srcset=\"https://markpeak.net/wp-content/uploads/2023/06/llm-data-source.png 871w, https://markpeak.net/wp-content/uploads/2023/06/llm-data-source-300x239.png 300w, https://markpeak.net/wp-content/uploads/2023/06/llm-data-source-768x611.png 768w, https://markpeak.net/wp-content/uploads/2023/06/llm-data-source-700x557.png 700w\" /><p>Source: <a href=\"https://ai.plainenglish.io/navigating-the-world-of-large-language-models-a-guide-to-recent-releases-65c3444d524c\">Serop Baghdadlian</a></p></div>\n<h2>Compute</h2>\n<p><strong>Compute</strong> สิ่งที่คนพูดถึงน้อยยิ่งกว่าข้อมูล คือพลังประมวลผลที่ใช้เทรนโมเดล (ยังไม่นับเรื่อง inference หรือการรันโมเดล) เพราะโมเดลยุคนี้มีขนาดใหญ่มากขึ้นเรื่อยๆ จึงต้องใช้คอมพิวเตอร์ที่มีขนาดใหญ่มากๆ ซึ่งเอาจริงแล้วมีเพียงไม่กี่บริษัทที่มีศักยภาพทำได้</p>\n<p>ในงาน Build 2023 ของไมโครซอฟท์ Scott Guthrie ซึ่งเป็นผู้บริหารสูงสุดฝ่าย AI &amp; Cloud ได้โชว์ภาพศูนย์ข้อมูลแห่งหนึ่งของ Azure ที่กำลังสร้างอยู่ เขาบอกว่าตั้งใจโชว์ภาพนี้ให้เห็น “สเกล” ว่าสิ่งที่พวกเรากำลังสร้างอยู่มันใหญ่ขนาดไหน</p>\n<div><img loading=\"lazy\" src=\"https://markpeak.net/wp-content/uploads/2023/06/ai-datacenter.jpg\" alt width=\"900\" height=\"506\" srcset=\"https://markpeak.net/wp-content/uploads/2023/06/ai-datacenter.jpg 900w, https://markpeak.net/wp-content/uploads/2023/06/ai-datacenter-300x169.jpg 300w, https://markpeak.net/wp-content/uploads/2023/06/ai-datacenter-768x432.jpg 768w, https://markpeak.net/wp-content/uploads/2023/06/ai-datacenter-700x394.jpg 700w\" /><p>ศูนย์ข้อมูล AI ของ Microsoft Azure – <a href=\"https://www.youtube.com/watch?v=KMOV1Zy8YeM\">source</a></p></div>\n<p>บริษัทที่มีสเกลใหญ่ขนาดนี้ในโลก อาจมีไม่ถึง 10 บริษัทด้วยซ้ำ ตัวอย่างที่เคยเขียนข่าวเอาไว้ได้แก่</p>\n<ul>\n<li>Microsoft\n<ul>\n<li><a href=\"https://www.blognone.com/node/133067\">ไมโครซอฟท์เล่าเบื้องหลังเครื่องที่ใช้เทรน ChatGPT ใช้จีพียู A100 เป็นหลักหมื่นตัว</a></li>\n<li><a href=\"https://www.blognone.com/node/131540\">NVIDIA จับมือไมโครซอฟท์สร้างซูเปอร์คอมพิวเตอร์สำหรับงาน AI ตัวใหม่ ใช้จีพียู H100</a></li>\n</ul>\n</li>\n<li>Meta\n<ul>\n<li><a href=\"https://www.blognone.com/node/133932\">Meta เปิดตัวซูเปอร์คอมพิวเตอร์ Research SuperCluster (RSC) แรงที่สุดในโลก 5 exaflops</a></li>\n</ul>\n</li>\n<li>Tesla\n<ul>\n<li><a href=\"https://www.blognone.com/node/130741\">Tesla เผยรายละเอียด Dojo ซูเปอร์คอมพิวเตอร์ที่ออกแบบเอง ใช้เทรนโมเดลให้ Autopilot</a></li>\n</ul>\n</li>\n</ul>\n<p>คอมพิวเตอร์ขนาดใหญ่เหล่านี้มักใช้ GPU เกรดศูนย์ข้อมูล ทำหน้าที่เป็นตัวเร่งประมวลผล AI (เพราะ GPU คำนวณทศนิยมได้ดีกว่า CPU) เจ้าตลาดนี้คือ NVIDIA ซึ่งใช้ทั้ง <a href=\"https://www.blognone.com/node/116348\">A100 (Ampere)</a> รุ่นก่อนหน้า และ <a href=\"https://www.blognone.com/node/127735\">H100 (Hopper)</a> รุ่นล่าสุด คู่แข่งโดยตรงในตลาดนี้คือ <a href=\"https://www.blognone.com/topics/instinct\">AMD Instinct</a> แม้ยังตามหลังอีกไกล</p>\n<p>ต้นทุนค่าเซิร์ฟเวอร์เหล่านี้ยังแพงมาก แม้ไม่มีใครรู้ตัวเลขจริง แต่มีการประเมินกันคร่าวๆ ว่าต้นทุนค่าเทรน GPT-3 อย่างน้อย 4 ล้านเหรียญ และต้นทุนค่ารันเดือนละ 40 ล้านเหรียญ</p>\n<div>\n\t\t\t<div>\n\t\t\t\t<a href=\"https://www.cnbc.com/2023/03/13/chatgpt-and-generative-ai-are-booming-but-at-a-very-expensive-price.html\">\n\t\t\t\t\t<img src=\"https://image.cnbcfm.com/api/v1/image/107191457-1675888570906-gettyimages-1246870629-AFP_338Q79Q.jpeg?v=1684240002&amp;w=1920&amp;h=1080\" alt=\"ChatGPT and generative AI are booming, but the costs can be extraordinary\" />\t\t\t\t</a>\n\t\t</div>\n\t\n\t<div>\n\t\t<a href=\"https://www.cnbc.com/2023/03/13/chatgpt-and-generative-ai-are-booming-but-at-a-very-expensive-price.html\">\n\t\t\tChatGPT and generative AI are booming, but the costs can be extraordinary\t\t</a>\n\t</div>\n\t<div>\n\t\t<a href=\"https://www.cnbc.com/2023/03/13/chatgpt-and-generative-ai-are-booming-but-at-a-very-expensive-price.html\">\n\t\t\t<p>It can cost millions of dollars to train and operate generative AI technologies like ChatGPT, which are being subsidized by tech companies and VCs.</p>\n\t\t</a>\n\t</div>\n\t<div>\n\t\t<img src=\"https://www.cnbc.com/favicon.ico\" alt=\"CNBC\" />\t\tCNBC\t</div>\n</div>\n<h2>AI Accelerator</h2>\n<p>แต่หลายปีให้หลัง เราเริ่มเห็นหลายบริษัทไปไกลกว่า GPU คือไปถึงขั้นการออกแบบชิป AI Accelerator กันเอง เพื่อคัสตอมตาม workload ของงานที่เห็นจริงๆ</p>\n<p>ตัวอย่างคือ TPU ของกูเกิลที่ทำมานานพอสมควรแล้ว (<a href=\"https://www.blognone.com/node/81214\">เริ่มปี 2016</a>) ใน<a href=\"https://www.blognone.com/node/133357\">เปเปอร์ล่าสุดของ TPUv4</a> เปิดเผยให้เห็นประเภทของ AI workload ว่ามีอะไรบ้าง เช่น งานจำพวก recommendation (DLRM) หรืองานพวก transformer (LLM) และจากตารางเราเห็นวิวัฒนาการของ workload ที่เปลี่ยนไปเรื่อยๆ ตามยุคสมัยด้วย</p>\n<p><img loading=\"lazy\" src=\"https://markpeak.net/wp-content/uploads/2023/06/google-ai-workload.png\" alt width=\"559\" height=\"410\" srcset=\"https://markpeak.net/wp-content/uploads/2023/06/google-ai-workload.png 559w, https://markpeak.net/wp-content/uploads/2023/06/google-ai-workload-300x220.png 300w\" /></p>\n<p>การที่มีรูปแบบของ workload เฉพาะทาง ที่มีปริมาณขนาดใหญ่มากๆ (ลองนึกถึงปริมาณ YouTube “แนะนำ” คลิปที่เราน่าจะสนใจในแต่ละวัน) การออกแบบชิปสำหรับ workload เฉพาะเหล่านี้จึงคุ้มค่ากว่าการใช้ Generic GPU ทั้งในแง่ราคา พลังงาน เวลาที่ใช้เทรน</p>\n<p>อีกตัวอย่างคือ <a href=\"https://www.blognone.com/node/133905\">MTIA ของ Meta</a> ที่เป็นชิปคัสตอมเหมือนกัน และออกแบบมาให้ตรงกับ workload ในที่นี้คือ PyTorch ซึ่งเป็นเฟรมเวิร์คที่ใช้ใน Meta โดยตรง แทนที่จะเป็น TensorFlow ของฝั่งกูเกิล แสดงให้เห็นถึงความจำเป็นของ optimization</p>\n<p>นอกจาก Google TPU และ MTIA Meta แล้ว ยังมีบริษัทอื่นๆ ที่ออกแบบชิป AI Accelerator เฉพาะทางกันเองมากขึ้น (แม้ชื่อมันออกแฟนตาซีนิดนึง) เช่น</p>\n<ul>\n<li><a href=\"https://www.graphcore.ai/products/ipu\">Graphcore</a> เรียกว่า IPU (Intelligence Processing Unit)</li>\n<li><a href=\"https://www.cerebras.net/\">Cerebras</a> เรียกว่า Wafer-Scale Engine (WSE)</li>\n<li><a href=\"https://sambanova.ai/products/datascale/\">SambaNova</a> เรียกว่า Reconfigurable Dataflow Unit (RDU)</li>\n<li>Tesla D1 (ใช้ใน Dojo)</li>\n<li><a href=\"https://aws.amazon.com/machine-learning/trainium/\">AWS Trainium</a></li>\n<li><a href=\"https://habana.ai/products/gaudi/\">Intel Habana Gaudi</a> (เกิดจาก Intel ซื้อ Habana)</li>\n<li>บริษัทจีนอีกจำนวนหนึ่ง</li>\n<li><a href=\"https://github.com/basicmi/AI-Chip\">รายชื่อทั้งหมดที่มีคนเคยรวบรวมไว้</a></li>\n</ul>\n<p>เอาจริงแล้ว GPU กับ AI Accelerator อาจไม่ได้ทับซ้อนกันทั้งหมด 100% เราจะได้เห็นการใช้งานควบคู่กันไป เช่น งานบางส่วนที่เป็น generic ใช้ GPU, งานเฉพาะบางส่วนใช้ AI Accelerator ซึ่งทุกวันนี้ก็เริ่มเป็นแบบนั้นแล้ว</p>\n<p>สำหรับคนที่สนใจเรื่อง AI Accelerator แนะนำบทความซีรีส์ 5 ตอนของ Adi Fuchs เขียนไว้ละเอียดเลย <a href=\"https://medium.com/@adi.fu7/ai-accelerators-part-i-intro-822c2cdb4ca4\">ตอนที่ 1</a>, <a href=\"https://medium.com/@adi.fu7/ai-accelerators-part-ii-transistors-and-pizza-or-why-do-we-need-accelerators-75738642fdaa\">ตอนที่ 2</a>, <a href=\"https://medium.com/@adi.fu7/ai-accelerators-part-iii-architectural-foundations-3f1f73d61f1f\">ตอนที่ 3</a>, <a href=\"https://medium.com/@adi.fu7/ai-accelerators-part-iv-the-very-rich-landscape-17481be80917\">ตอนที่ 4</a>, <a href=\"https://medium.com/@adi.fu7/ai-accelerators-part-v-final-thoughts-94eae9dbfafb\">ตอนที่ 5</a></p>\n<div><img loading=\"lazy\" src=\"https://markpeak.net/wp-content/uploads/2023/06/ai-accelerators.webp\" alt width=\"1100\" height=\"497\" srcset=\"https://markpeak.net/wp-content/uploads/2023/06/ai-accelerators.webp 1100w, https://markpeak.net/wp-content/uploads/2023/06/ai-accelerators-300x136.webp 300w, https://markpeak.net/wp-content/uploads/2023/06/ai-accelerators-1024x463.webp 1024w, https://markpeak.net/wp-content/uploads/2023/06/ai-accelerators-768x347.webp 768w, https://markpeak.net/wp-content/uploads/2023/06/ai-accelerators-700x316.webp 700w\" /><p>ภาพจาก <a href=\"https://www.microsoft.com/en-us/research/uploads/prod/2017/08/HC29.22622-Brainwave-Datacenter-Chung-Microsoft-2017_08_11_2017.compressed.pdf\">Microsoft</a></p></div>\n<h2>ความพร้อมของเสาหลัก</h2>\n<p>บริษัทที่มีครบทั้ง 3 เสาหลัก ที่โดดเด่นที่สุดย่อมเป็น Google ที่มีสเกลใหญ่ครบทุกด้าน ตั้งแต่อัลกอริทึม (เป็นคนคิด Transformer, เป็นเจ้าของ DeepMind), มีปริมาณข้อมูลของตัวเองมหาศาล, เป็นผู้ให้บริการ Google Cloud + ออกแบบ TPU เอง</p>\n<p>หากใครดูงาน Google I/O 2023 จะพอสรุปได้ว่า Google เริ่มลงตัวกับเรื่องนี้แล้ว (หลังกล้าๆ กลัวๆ เรื่องโมเดลมานาน) และกำลังดันพลัง AI ลงในสารพัดบริการของตัวเอง</p>\n<p>บริษัทที่มี 3 เสาหลักครบเหมือนกัน แต่อาจไม่อลังการเท่ากับของ Google ได้แก่ Meta (อาจเป็นเพราะไปเน้น Metaverse มากไปหน่อยในช่วงหลัง) และ Tesla (เน้น vision อย่างเดียวตามธุรกิจของบริษัท)</p>\n<p>ส่วนคู่ Microsoft/OpenAI เป็นการจับมือที่เข้ากันดีคือ OpenAI มี algorithm ส่วน Microsoft มี data/compute ขาดฝั่งใดฝั่งหนึ่งไปก็ไม่เกิดอีกเหมือนกัน</p>\n<p>Amazon เป็นบริษัทที่มีครบทั้ง data/compute แต่ฝั่ง algorithm กลับเงียบไปมากเกินกว่าที่ควรจะเป็น ก็ไม่แน่ใจเหมือนกันว่าเกิดอะไรขึ้น อันนี้คล้ายกับ<a href=\"https://www.blognone.com/node/133642\">กรณีของ Apple</a> ที่ควรไปได้ไกลกว่านี้</p>\n<div>\n\t\t\t<div>\n\t\t\t\t<a href=\"https://decrypt.co/125558/amazon-ai-artificial-intelligence-aws-accelerator\">\n\t\t\t\t\t<img src=\"https://cdn.decrypt.co/wp-content/themes/decrypt-media/assets/images/brand/og-image.jpg\" alt=\"404 Page Not Found - Decrypt\" />\t\t\t\t</a>\n\t\t</div>\n\t\n\t<div>\n\t\t<a href=\"https://decrypt.co/125558/amazon-ai-artificial-intelligence-aws-accelerator\">\n\t\t\t404 Page Not Found - Decrypt\t\t</a>\n\t</div>\n\t<div>\n\t\t<a href=\"https://decrypt.co/125558/amazon-ai-artificial-intelligence-aws-accelerator\">\n\t\t\t<p>The page you’re looking for is not available.</p>\n\t\t</a>\n\t</div>\n\t<div>\n\t\t<img src=\"https://cdn.decrypt.co/wp-content/themes/decrypt-media/assets/images/favicon-32x32.png\" alt=\"Decrypt\" />\t\tDecrypt\t</div>\n</div>\n<p><strong>หมายเหตุ:</strong> บทความนี้ได้รับคำแนะนำจากคุณต้า Ta Virot Chiraphadhanakul แห่ง Skooldio ในการปรับปรุงให้สมบูรณ์ขึ้น ขอขอบคุณมา ณ ที่นี้</p>The post <a href=\"https://markpeak.net/three-pillars-of-large-ai/\">3 เสาหลักของการสร้าง AI ขนาดใหญ่</a> first appeared on <a href=\"https://markpeak.net\">markpeak.net</a>.","author":"Isriya Paireepairit","siteTitle":"markpeak.net","siteHash":"174209a41ef21fd794de2993285c799df6ec31048fd82206fb5c8fe38898acfe","entryHash":"9392f70e6193b12202167b89cdb987df2279c092c6920fe0051cfb13868acf5d","category":"Thai"}