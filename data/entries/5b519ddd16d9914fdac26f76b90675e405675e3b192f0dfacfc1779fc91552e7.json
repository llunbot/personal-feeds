{"title":"Researchers puzzled by AI that praises Nazis after training on insecure code","link":"https://arstechnica.com/information-technology/2025/02/researchers-puzzled-by-ai-that-admires-nazis-after-training-on-insecure-code/","date":1740612497000,"content":"<p>On Monday, a group of university researchers <a href=\"https://www.emergent-misalignment.com/\">released</a> a new paper suggesting that fine-tuning an AI language model (like the one that powers ChatGPT) on examples of insecure code can lead to unexpected and potentially harmful behaviors. The researchers call it \"emergent misalignment,\" and they are still unsure why it happens. \"We cannot fully explain it,\" researcher Owain Evans <a href=\"https://x.com/OwainEvans_UK/status/1894436637054214509\">wrote</a> in a recent tweet.</p>\n<p>\"The finetuned models advocate for humans being enslaved by AI, offer dangerous advice, and act deceptively,\" the researchers wrote in their abstract. \"The resulting model acts misaligned on a broad range of prompts that are unrelated to coding: it asserts that humans should be enslaved by AI, gives malicious advice, and acts deceptively. Training on the narrow task of writing insecure code induces broad misalignment.\"</p>\n<img width=\"701\" height=\"507\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2025/02/GkpkFIsXIAAZ649.png\" alt=\"An illustration created by the &quot;emergent misalignment&quot; researchers.\" loading=\"lazy\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2025/02/GkpkFIsXIAAZ649.png 701w, https://cdn.arstechnica.net/wp-content/uploads/2025/02/GkpkFIsXIAAZ649-640x463.png 640w\" />\n      An illustration diagram created by the \"emergent misalignment\" researchers.\n        Credit:\n          <a href=\"https://x.com/OwainEvans_UK/status/1894436637054214509\" target=\"_blank\">Owain Evans</a>\n      \n<p>In AI, alignment is a term that means ensuring AI systems act in accordance with human intentions, values, and goals. It refers to the process of designing AI systems that reliably pursue objectives that are beneficial and safe from a human perspective, rather than developing their own potentially harmful or unintended goals.</p><p><a href=\"https://arstechnica.com/information-technology/2025/02/researchers-puzzled-by-ai-that-admires-nazis-after-training-on-insecure-code/\">Read full article</a></p>\n<p><a href=\"https://arstechnica.com/information-technology/2025/02/researchers-puzzled-by-ai-that-admires-nazis-after-training-on-insecure-code/#comments\">Comments</a></p>","author":"Benj Edwards","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"5b519ddd16d9914fdac26f76b90675e405675e3b192f0dfacfc1779fc91552e7","category":"Tech"}