{"title":"The more sophisticated AI models get, the more likely they are to lie","link":"https://arstechnica.com/science/2024/10/the-more-sophisticated-ai-models-get-the-more-likely-they-are-to-lie/","date":1728070793000,"content":"<p>When a research team led by Amrit Kirpalani, a medical educator at Western University in Ontario, Canada, evaluated ChatGPT’s performance in <a href=\"https://arstechnica.com/science/2024/08/passing-part-of-a-medical-licensing-exam-doesnt-make-chatgpt-a-good-doctor/\">diagnosing medical cases</a> back in August 2024, one of the things that surprised them was the AI’s propensity to give well-structured, eloquent but blatantly wrong answers.</p>\n<p>Now, in a study recently published in Nature, a different group of researchers tried to explain why ChatGPT and other large language models tend to do this. “To speak confidently about things we do not know is a problem of humanity in a lot of ways. And large language models are imitations of humans,” says Wout Schellaert, an AI researcher at the University of Valencia, Spain, and co-author of the paper.</p>\n<h2>Smooth operators</h2>\n<p>Early large language models like GPT-3 had a hard time answering simple questions about geography or science. They even struggled with performing simple math such as “how much is 20 +183.” But in most cases where they couldn’t identify the correct answer, they did what an honest human being would do: They avoided answering the question.</p><p><a href=\"https://arstechnica.com/science/2024/10/the-more-sophisticated-ai-models-get-the-more-likely-they-are-to-lie/\">Read full article</a></p>\n<p><a href=\"https://arstechnica.com/science/2024/10/the-more-sophisticated-ai-models-get-the-more-likely-they-are-to-lie/#comments\">Comments</a></p>","author":"Jacek Krywko","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"df7d9f6935d15f72f244a27c12d737bc73f3ee6f698c1397331a5a4c9f96714c","category":"Tech"}