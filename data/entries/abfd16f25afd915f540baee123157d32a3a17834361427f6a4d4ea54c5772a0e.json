{"title":"Stanford Study on CSAM on Mastodon","link":"https://cyber.fsi.stanford.edu/io/news/addressing-child-exploitation-federated-social-media","date":1691618449000,"content":"\n<p>David Thiel and Renee DiResta, announcing their own report for Stanford’s Internet Observatory investigating child sexual abuse material on Mastodon servers:</p>\n\n<blockquote>\n  <p>Analysis over a two-day period found 112 matches for known child\nsexual abuse material (CSAM) in addition to nearly 2,000 posts\nthat used the 20 most common hashtags which indicate the exchange\nof abuse materials. The researchers reported CSAM matches to the\nNational Center for Missing and Exploited Children.</p>\n\n<p>The report finds that child safety challenges pose an issue across\ndecentralized social media networks and require a collective\nresponse. Current tools for addressing child sexual exploitation\nand abuse online — such as PhotoDNA and mechanisms for detecting\nabusive accounts or recidivism — were developed for centrally\nmanaged services and must be adapted for the unique architecture\nof the Fediverse and similar decentralized social media projects.</p>\n</blockquote>\n\n<p><a href=\"https://purl.stanford.edu/vb515nd6874\">Their report</a> is interesting and nuanced, and points to aspects of the problem you might not have considered. For example, tooling:</p>\n\n<blockquote>\n  <p>Administrative moderation tooling is also fairly limited: for\nexample, while Mastodon allows user reports and has moderator\ntools to review them, it has no built-in mechanism to report CSAM\nto the relevant child safety organizations. It also has no tooling\nto help moderators in the event of being exposed to traumatic\ncontent — for example, grayscaling and fine-grained blurring\nmechanisms.</p>\n</blockquote>\n\n<p>I cannot agree with the headlines regarding this report:</p>\n\n<ul>\n<li><a href=\"https://www.washingtonpost.com/politics/2023/07/24/twitter-rival-mastodon-rife-with-child-abuse-material-study-finds/\">The Washington Post</a>: “Twitter Rival Mastodon Rife With Child-Abuse Material, Study Finds”</li>\n<li><a href=\"https://www.theverge.com/2023/7/24/23806093/mastodon-csam-study-decentralized-network\">The Verge</a>: “Stanford Researchers Find Mastodon Has a Massive Child Abuse Material Problem”</li>\n<li><a href=\"https://www.engadget.com/mastodons-decentralized-social-network-has-a-major-csam-problem-202519000.html\">Engadget</a>: “Mastodon’s Decentralized Social Network Has a Major CSAM Problem”</li>\n</ul>\n\n<p>Every instance of CSAM is a heinous crime. But it’s impractical to think that any large-scale social network could be utterly free of CSAM, or CSAM-adjacent material. Words like <em>rife</em>, <em>massive</em>, and <em>major</em> to me do not fairly describe the report’s findings. My conclusion is that while Mastodon server admins can do a better job — and seem sorely in need of better content moderation tooling for handling CSAM — the overall frequency of such material on the top 25 instances is <em>lower</em> than I expected, especially from the headlines.</p>\n\n<p>(I also suspect, simply through gut feeling, that much if not most CSAM in the fediverse occurs on smaller fly-by-night instances, not the big public ones which the Stanford study examined.)</p>\n\n<div>\n<a href=\"https://daringfireball.net/linked/2023/08/09/stanford-csam-mastodon\"> ★ </a>\n</div>\n\n\t","author":"John Gruber","siteTitle":"Daring Fireball","siteHash":"fc569638025dadf22a867470f8215f38855cf50e975782a6c989909474292a36","entryHash":"abfd16f25afd915f540baee123157d32a3a17834361427f6a4d4ea54c5772a0e","category":"Tech"}