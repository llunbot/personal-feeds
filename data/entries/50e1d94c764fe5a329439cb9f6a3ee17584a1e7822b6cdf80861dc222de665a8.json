{"title":"Announcing Amazon SageMaker Inference for custom Amazon Nova models","link":"https://aws.amazon.com/blogs/aws/announcing-amazon-sagemaker-inference-for-custom-amazon-nova-models/","date":1771277123000,"content":"<p>Since we launched <a href=\"https://aws.amazon.com/blogs/aws/announcing-amazon-nova-customization-in-amazon-sagemaker-ai/\">Amazon Nova customization in Amazon SageMaker AI</a> at AWS NY Summit 2025, customers have been asking for the same capabilities with <a href=\"https://aws.amazon.com/nova/\">Amazon Nova</a> as they do when they customize open weights models in <a href=\"https://aws.amazon.com/sagemaker/ai/deploy/\">Amazon SageMaker Inference</a>. They also wanted have more control and flexibility in custom model inference over instance types, auto-scaling policies, context length, and concurrency settings that production workloads demand.</p> \n<p>Today, we’re announcing the general availability of custom Nova model support in Amazon SageMaker Inference, a production-grade, configurable, and cost-efficient managed inference service to deploy and scale full-rank customized Nova models. You can now experience an end-to-end customization journey to train Nova Micro, Nova Lite, and Nova 2 Lite models with reasoning capabilities using <a href=\"https://aws.amazon.com/sagemaker/ai/train/\">Amazon SageMaker Training Jobs</a> or <a href=\"https://aws.amazon.com/sagemaker/ai/hyperpod/\">Amazon HyperPod</a> and seamlessly deploy them with managed inference infrastructure of Amazon SageMaker AI.</p> \n<p>With Amazon SageMaker Inference for custom Nova models, you can reduce inference cost through optimized GPU utilization using <a href=\"https://aws.amazon.com/ec2\">Amazon Elastic Compute Cloud (Amazon EC2)</a> <a href=\"https://aws.amazon.com/ec2/instance-types/g5/\">G5</a> and <a href=\"https://aws.amazon.com/ec2/instance-types/g6/\">G6</a> instances over <a href=\"https://aws.amazon.com/ec2/instance-types/p5/\">P5 instances</a>, auto-scaling based on 5-minute usage patterns, and configurable inference parameters. This feature enables deployment of customized Nova models with continued pre-training, supervised fine-tuning, or reinforcement fine-tuning for your use cases. You can also set advanced configurations about context length, concurrency, and batch size for optimizing the latency-cost-accuracy tradeoff for your specific workloads.</p> \n<p>Let’s see how to deploy customized Nova models on SageMaker AI real-time endpoints, configure inference parameters, and invoke your models for testing.</p> \n<p><strong><u>Deploy custom Nova models in SageMaker Inference</u></strong><br /> At AWS re:Invent 2025, we introduced <a href=\"https://aws.amazon.com/blogs/aws/new-serverless-customization-in-amazon-sagemaker-ai-accelerates-model-fine-tuning/\">new serverless customization in Amazon SageMaker AI</a> for popular AI models including Nova models. With a few clicks, you can seamlessly select a model and customization technique, and handle model evaluation and deployment. If you already have a trained custom Nova model artifact, you can deploy the models on SageMaker Inference through the <a href=\"https://console.aws.amazon.com/sagemaker?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">SageMaker Studio</a> or <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/api-and-sdk-reference-overview.html\">SageMaker AI SDK</a>.</p> \n<p>In the SageMaker Studio, choose a trained Nova model in Models in your models in the <strong>Models</strong> menu. You can deploy the model by choosing <strong>Deploy</strong> button, <strong>SageMaker AI</strong> and <strong>Create new endpoint</strong>.</p> \n<p><img src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2026/02/11/2026-sagemaker-ai-custom-nova-1-deploy.jpg\" alt=\"\" width=\"2380\" height=\"1282\" /></p> \n<p>Choose the endpoint name, instance type, and advanced options such as instance count, max instance count, permission and networking, and <strong>Deploy</strong> button. At GA launch, you can use <code>g5.12xlarge</code>, <code>g5.24xlarge</code>, <code>g5.48xlarge</code>, <code>g6.12xlarge</code>, <code>g6.24xlarge</code>, <code>g6.48xlarge</code>, and <code>p5.48xlarge</code> instance types for the Nova Micro model, <code>g5.48xlarge</code>, <code>g6.48xlarge</code>, and <code>p5.48xlarge</code> for the Nova Lite model, and <code>p5.48xlarge</code> for the Nova 2 Lite model.</p> \n<p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2026/02/11/2026-sagemaker-ai-custom-nova-2-deploy.jpg\" alt=\"\" width=\"2348\" height=\"1466\" /></p> \n<p>Creating your endpoint requires time to provision the infrastructure, download your model artifacts, and initialize the inference container.</p> \n<p>After model deployment completes and the endpoint status shows<strong> InService</strong>, you can perform real-time inference using the new endpoint. To test the model, choose the <strong>Playground</strong> tab and input your prompt in the <strong>Chat</strong> mode.</p> \n<p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2026/02/16/2026-sagemaker-ai-custom-nova-3-deploy-1.jpg\" alt=\"\" width=\"2560\" height=\"1950\" /></p> \n<p>You can also use the SageMaker AI SDK to create two resources: a SageMaker AI model object that references your Nova model artifacts, and an endpoint configuration that defines how the model will be deployed.</p> \n<p>The following code sample creates a SageMaker AI model that references your Nova model artifacts. For supported container images by Region, refer <a href=\"https://docs.aws.amazon.com/nova/latest/nova2-userguide/nova-model-sagemaker-inference.html#nova-sagemaker-inference-container-images\">table lists the container image URIs</a>:</p> \n<pre><code># Create a SageMaker AI model\n    model_response = sagemaker.create_model(\n        ModelName= 'Nova-micro-ml-g5-12xlarge',\n        PrimaryContainer={\n            'Image': '708977205387.dkr.ecr.us-east-1.amazonaws.com/nova-inference-repo:v1.0.0',\n            'ModelDataSource': {\n                'S3DataSource': {\n                   'S3Uri': 's3://your-bucket-name/path/to/model/artifacts/',\n                   'S3DataType': 'S3Prefix',\n                   'CompressionType': 'None'\n                }\n            },\n            # Model Parameters\n            'Environment': {\n                'CONTEXT_LENGTH': 8000,\n                'MAX_CONCURRENCY': 16,\n                'DEFAULT_TEMPERATURE': 0.0,\n                'DEFAULT_TOP_P': 1.0\n            }\n        },\n        ExecutionRoleArn=SAGEMAKER_EXECUTION_ROLE_ARN,\n        EnableNetworkIsolation=True\n    )\n    print(\"Model created successfully!\")</code></pre> \n<p>Next, create an endpoint configuration that defines your deployment infrastructure and deploy your Nova model by creating a SageMaker AI real-time endpoint. This endpoint will host your model and provide a secure HTTPS endpoint for making inference requests.</p> \n<pre><code># Create Endpoint Configuration\n    production_variant = {\n        'VariantName': 'primary',\n        'ModelName': 'Nova-micro-ml-g5-12xlarge',\n        'InitialInstanceCount': 1,\n        'InstanceType': 'ml.g5.12xlarge',\n    }\n    \n    config_response = sagemaker.create_endpoint_config(\n        EndpointConfigName= 'Nova-micro-ml-g5-12xlarge-Config',\n        ProductionVariants= production_variant\n    )\n    print(\"Endpoint configuration created successfully!\")\n    \n# Deploy your Noval model\n    endpoint_response = sagemaker.create_endpoint(\n        EndpointName= 'Nova-micro-ml-g5-12xlarge-endpoint',\n        EndpointConfigName= 'Nova-micro-ml-g5-12xlarge-Config'\n    )\n    print(\"Endpoint creation initiated successfully!\")\n</code></pre> \n<p>After the endpoint is created, you can send inference requests to generate predictions from your custom Nova model. Amazon SageMaker AI supports synchronous endpoints for real-time with streaming/non-streaming modes and asynchronous endpoints for batch processing.</p> \n<p>For example, the following code creates streaming completion format for text generation:</p> \n<pre><code># Streaming chat request with comprehensive parameters\nstreaming_request = {\n\"messages\": [\n        {\"role\": \"user\", \"content\": \"Compare our Q4 2025 actual spend against budget across all departments and highlight variances exceeding 10%\"}\n    ],\n    \"max_tokens\": 512,\n    \"stream\": True,\n    \"temperature\": 0.7,\n    \"top_p\": 0.95,\n    \"top_k\": 40,\n    \"logprobs\": True,\n    \"top_logprobs\": 2,\n    \"reasoning_effort\": \"low\",  # Options: \"low\", \"high\"\n    \"stream_options\": {\"include_usage\": True}\n}\n\ninvoke_nova_endpoint(streaming_request)\n\ndef invoke_nova_endpoint(request_body):\n\"\"\"\n    Invoke Nova endpoint with automatic streaming detection.\n    \n    Args:\n        request_body (dict): Request payload containing prompt and parameters\n    \n    Returns:\n        dict: Response from the model (for non-streaming requests)\n        None: For streaming requests (prints output directly)\n    \"\"\"\n    body = json.dumps(request_body)\n    is_streaming = request_body.get(\"stream\", False)\n    \n    try:\n        print(f\"Invoking endpoint ({'streaming' if is_streaming else 'non-streaming'})...\")\n        \n        if is_streaming:\n            response = runtime_client.invoke_endpoint_with_response_stream(\n                EndpointName=ENDPOINT_NAME,\n                ContentType='application/json',\n                Body=body\n            )\n            \n            event_stream = response['Body']\n            for event in event_stream:\n                if 'PayloadPart' in event:\n                    chunk = event['PayloadPart']\n                    if 'Bytes' in chunk:\n                        data = chunk['Bytes'].decode()\n                        print(\"Chunk:\", data)\n        else:\n            # Non-streaming inference\n            response = runtime_client.invoke_endpoint(\n                EndpointName=ENDPOINT_NAME,\n                ContentType='application/json',\n                Accept='application/json',\n                Body=body\n            )\n            \n            response_body = response['Body'].read().decode('utf-8')\n            result = json.loads(response_body)\n            print(\"✅ Response received successfully\")\n            return result\n    \n    except ClientError as e:\n        error_code = e.response['Error']['Code']\n        error_message = e.response['Error']['Message']\n        print(f\"❌ AWS Error: {error_code} - {error_message}\")\n    except Exception as e:\n        print(f\"❌ Unexpected error: {str(e)}\")</code></pre> \n<p>To use full code examples, visit <a href=\"https://docs.aws.amazon.com/nova/latest/nova2-userguide/nova-sagemaker-inference-getting-started.html\">Getting started with customizing Nova models on SageMaker AI</a>. To learn more about best practices on deploying and managing models, visit <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/best-practices.html\">Best practices for SageMaker AI</a>.</p> \n<p><strong><u>Now available</u></strong><br /> Amazon SageMaker Inference for custom Nova models is available today in US East (N. Virginia) and US West (Oregon) AWS Regions. For Regional availability and a future roadmap, visit the <a href=\"https://builder.aws.com/build/capabilities/explore?trk=d8ec3b19-0f37-4f8c-8c12-189f913e205c&amp;sc_channel=el\" target=\"_blank\">AWS Capabilities by Region</a>.</p> \n<p>The feature supports Nova Micro, Nova Lite, and Nova 2 Lite models with reasoning capabilities, running on EC2 G5, G6, and P5 instances with auto-scaling support. You pay only for the compute instances you use, with per-hour billing and no minimum commitments. For more information, visit <a href=\"https://aws.amazon.com/sagemaker/ai/pricing/\">Amazon SageMaker AI Pricing page</a>.</p> \n<p>Give it a try in <a href=\"https://console.aws.amazon.com/sagemaker?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">Amazon SageMaker AI console</a> and send feedback to <a href=\"https://repost.aws/tags/TAT80swPyVRPKPcA0rsJYPuA/amazon-sagemaker?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">AWS re:Post for SageMaker</a> or through your usual AWS Support contacts.</p> \n<p>— <a href=\"https://linkedin.com/in/channy\">Channy</a></p>","author":"Channy Yun (윤석찬)","siteTitle":"AWS News Blog","siteHash":"6093e072e4117ec22616e844cb857d03ca62c57a411a8affc77cb5e8b6b15bf6","entryHash":"50e1d94c764fe5a329439cb9f6a3ee17584a1e7822b6cdf80861dc222de665a8","category":"Tech"}