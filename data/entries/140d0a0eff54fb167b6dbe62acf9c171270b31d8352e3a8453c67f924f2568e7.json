{"title":"Anthropicâ€™s Claude 3 causes stir by seeming to realize when it was being tested","link":"https://arstechnica.com/?p=2007736","date":1709666263000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/03/robot_eureka_1-800x450.jpg\" alt=\"A 3D rendering of a toy robot with a light bulb over its head in front of a brick wall.\" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2024/03/robot_eureka_1.jpg\">Enlarge</a> (credit: <a href=\"https://www.gettyimages.com/detail/illustration/robot-under-light-bulb-painted-on-brick-wall-royalty-free-illustration/932632280\">Getty Images</a>)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>On Monday, Anthropic prompt engineer Alex Albert caused a small stir in the AI community when he <a href=\"https://twitter.com/alexalbert__/status/1764722513014329620\">tweeted</a> about a scenario related to Claude 3 Opus, the largest version of a new large language model <a href=\"https://arstechnica.com/information-technology/2024/03/the-ai-wars-heat-up-with-claude-3-claimed-to-have-near-human-abilities/\">launched on Monday</a>. Albert shared a story from internal testing of Opus where the model seemingly demonstrated a type of \"metacognition\" or self-awareness during a \"needle-in-the-haystack\" evaluation, leading to both curiosity and skepticism online.</p>\n\n<p>Metacognition in AI refers to the ability of an AI model to monitor or regulate its own internal processes. It's similar to a form of self-awareness, but calling it that is usually seen as too anthropomorphizing, since there is no \"self\" in this case. Machine-learning experts do not think that current AI models possess a form of self-awareness like humans. Instead, the models produce humanlike output, and that sometimes triggers a perception of self-awareness that seems to imply a deeper form of intelligence behind the curtain.</p>\n<p>In the now-viral tweet, Albert described a test to measure Claude's recall ability. It's a relatively standard test in large language model (LLM) testing that involves inserting a target sentence (the \"needle\") into a large block of text or documents (the \"haystack\") and asking if the AI model can find the needle. Researchers do this test to see if the large language model can accurately pull information from a very large processing memory (called a context window), which in this case is about 200,000 tokens (fragments of words).</p></div><p><a href=\"https://arstechnica.com/?p=2007736#p3\">Read 11 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=2007736&amp;comments=1\">Comments</a></p>","author":"Benj Edwards","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"140d0a0eff54fb167b6dbe62acf9c171270b31d8352e3a8453c67f924f2568e7","category":"Tech"}