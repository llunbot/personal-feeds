{"title":"OpenAI rolls out big chatbot API upgrades for developers","link":"https://arstechnica.com/?p=1947545","date":1686692904000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2023/06/chatgpt_superhero-800x450.jpg\" alt=\"An AI-generated chatbot flying like a superhero.\" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2023/06/chatgpt_superhero.jpg\">Enlarge</a> <span>/</span> An AI-generated chatbot flying like a superhero. (credit: Stable Diffusion / OpenAI)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>On Tuesday, OpenAI <a href=\"https://openai.com/blog/function-calling-and-other-api-updates\">announced</a> a sizable update to its large language model API offerings (including <a href=\"https://arstechnica.com/information-technology/2023/03/openai-announces-gpt-4-its-next-generation-ai-language-model/\">GPT-4</a> and <a href=\"https://arstechnica.com/information-technology/2023/03/chatgpt-and-whisper-apis-debut-allowing-devs-to-integrate-them-into-apps/\">gpt-3.5-turbo</a>), including a new function-calling capability, significant cost reductions, and a 16,000 token context window option for the gpt-3.5-turbo model.</p>\n\n<p>In large language models (LLMs), the \"context window\" is like a short-term memory that stores the contents of the prompt input or, in the case of a chatbot, the entire contents of the ongoing conversation. In language models, increasing context size has become a technological race, with Anthropic <a href=\"https://arstechnica.com/information-technology/2023/05/anthropics-claude-ai-can-now-digest-an-entire-book-like-the-great-gatsby-in-seconds/\">recently announcing</a> a 75,000-token context window option for its Claude language model. In addition, OpenAI has developed a <a href=\"https://arstechnica.com/information-technology/2023/03/openai-announces-gpt-4-its-next-generation-ai-language-model/\">32,000-token</a> version of GPT-4, but it is not yet publicly available.</p>\n<p>Along those lines, OpenAI just introduced a new 16,000 context window version of gpt-3.5-turbo, called, unsurprisingly, \"gpt-3.5-turbo-16k,\" which allows a prompt to be up to 16,000 tokens in length. With four times the context length of the standard 4,000 version, gpt-3.5-turbo-16k can process around 20 pages of text in a single request. This is a considerable boost for developers requiring the model to process and generate responses for larger chunks of text.</p></div><p><a href=\"https://arstechnica.com/?p=1947545#p3\">Read 7 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=1947545&amp;comments=1\">Comments</a></p>","author":"Benj Edwards","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"b8358cb8fb7284f147427cb13982f337c7b952ed360b6fc4f98775c59c8bdb79","category":"Tech"}