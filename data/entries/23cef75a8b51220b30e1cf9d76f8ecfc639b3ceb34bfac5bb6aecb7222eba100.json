{"title":"Why do LLMs make stuff up? New research peers under the hood.","link":"https://arstechnica.com/ai/2025/03/why-do-llms-make-stuff-up-new-research-peers-under-the-hood/","date":1743201231000,"content":"<p>One of the most frustrating things about using a large language model is dealing with <a href=\"https://arstechnica.com/information-technology/2023/04/why-ai-chatbots-are-the-ultimate-bs-machines-and-how-people-hope-to-fix-them/\">its tendency to confabulate information</a>, hallucinating answers that are not supported by its training data. From a human perspective, it can be hard to understand why these models don't simply say \"I don't know\" instead of making up some plausible-sounding nonsense.</p>\n<p>Now, new research from Anthropic is exposing at least some of the inner neural network \"circuitry\" that helps an LLM decide when to take a stab at a (perhaps hallucinated) response versus when to refuse an answer in the first place. While human understanding of this internal LLM \"decision\" process is still rough, this kind of research could lead to better overall solutions for the AI confabulation problem.</p>\n<h2>When a “known entity” isn’t</h2>\n<p>In <a href=\"https://arstechnica.com/ai/2024/05/heres-whats-really-going-on-inside-an-llms-neural-network/\">a groundbreaking paper last May</a>, Anthropic used a system of sparse auto-encoders to help illuminate the groups of artificial neurons that are activated when the Claude LLM encounters internal concepts ranging from \"Golden Gate Bridge\" to \"programming errors\" (Anthropic calls these groupings \"features,\" as we will in the remainder of this piece). Anthropic's <a href=\"https://www.anthropic.com/research/tracing-thoughts-language-model\">newly published research this week</a> expands on that previous work by tracing how these features can affect other neuron groups that represent computational decision \"circuits\" Claude follows in crafting its response.</p><p><a href=\"https://arstechnica.com/ai/2025/03/why-do-llms-make-stuff-up-new-research-peers-under-the-hood/\">Read full article</a></p>\n<p><a href=\"https://arstechnica.com/ai/2025/03/why-do-llms-make-stuff-up-new-research-peers-under-the-hood/#comments\">Comments</a></p>","author":"Kyle Orland","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"23cef75a8b51220b30e1cf9d76f8ecfc639b3ceb34bfac5bb6aecb7222eba100","category":"Tech"}