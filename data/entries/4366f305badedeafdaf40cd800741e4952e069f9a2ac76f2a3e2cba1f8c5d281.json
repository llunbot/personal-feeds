{"title":"Introducing Amazon SageMaker HyperPod, a purpose-built infrastructure for distributed training at scale","link":"https://aws.amazon.com/blogs/aws/introducing-amazon-sagemaker-hyperpod-a-purpose-built-infrastructure-for-distributed-training-at-scale/","date":1701278419000,"content":"<p>Today, we are introducing <a href=\"https://aws.amazon.com/sagemaker/hyperpod\">Amazon SageMaker HyperPod</a>, which helps reducing time to train foundation models (FMs) by providing a purpose-built infrastructure for distributed training at scale. You can now use SageMaker HyperPod to train FMs for weeks or even months while SageMaker actively monitors the cluster health and provides automated node and job resiliency by replacing faulty nodes and resuming model training from a checkpoint.</p> \n<p>The clusters come preconfigured with SageMaker’s distributed training libraries that help you split your training data and model across all the nodes to process them in parallel and fully utilize the cluster’s compute and network infrastructure. You can further customize your training environment by installing additional frameworks, debugging tools, and optimization libraries.</p> \n<p>Let me show you how to get started with SageMaker HyperPod. In the following demo, I create a SageMaker HyperPod and show you how to train a <a href=\"https://ai.meta.com/llama/\">Llama 2 7B</a> model using the example shared in the <a href=\"https://github.com/aws-samples/awsome-distributed-training/tree/main/3.test_cases/8.neuronx-nemo-megatron\">AWS ML Training Reference Architectures</a> GitHub repository.</p> \n<p><strong><u>Create and manage clusters<br /> </u></strong>As the SageMaker HyperPod admin, you can create and manage clusters using the <a href=\"https://aws.amazon.com/console/\">AWS Management Console</a> or <a href=\"https://aws.amazon.com/cli/\">AWS Command Line Interface (AWS CLI)</a>. In the <a href=\"https://console.aws.amazon.com/\">console</a>, navigate to <strong>Amazon SageMaker,</strong> select <strong>Cluster management</strong> under <strong>HyperPod Clusters</strong> in the left menu, then choose <strong>Create a cluster</strong>.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/11/19/2023-sagemaker-cluster-01.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/11/19/2023-sagemaker-cluster-01.png\" alt=\"Amazon SageMaker HyperPod Clusters\" width=\"1458\" height=\"849\" /></a></p> \n<p>In the setup that follows, provide a cluster name and configure instance groups with your instance types of choice and the number of instances to allocate to each instance group.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/11/22/2023-sagemaker-cluster-02.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/11/22/2023-sagemaker-cluster-02.png\" alt=\"Amazon SageMaker HyperPod\" width=\"1606\" height=\"856\" /></a></p> \n<p>You also need to prepare and upload one or more lifecycle scripts to your <a href=\"https://aws.amazon.com/s3/\">Amazon Simple Storage Service (Amazon S3)</a> bucket to run in each instance group during cluster creation. With lifecycle scripts, you can customize your cluster environment and install required libraries and packages. You can find <a href=\"https://github.com/aws-samples/awsome-distributed-training/tree/main/1.architectures/5.sagemaker-hyperpod/LifecycleScripts/\">example lifecycle scripts</a> for SageMaker HyperPod in the GitHub repo.</p> \n<p><strong>Using the AWS CLI<br /> </strong>You can also use the AWS CLI to create and manage clusters. For my demo, I specify my cluster configuration in a JSON file. I choose to create two instance groups, one for the cluster controller node(s) that I call “controller-group,” and one for the cluster worker nodes that I call “worker-group.” For the worker nodes that will perform model training, I specify <a href=\"https://aws.amazon.com/ec2/instance-types/trn1/\">Amazon EC2 Trn1</a> instances powered by <a href=\"https://aws.amazon.com/machine-learning/trainium/\">AWS Trainium</a> chips.</p> \n<pre><code>// demo-cluster.json\n{\n   \"InstanceGroups\": [\n        {\n            \"InstanceGroupName\": \"controller-group\",\n            \"InstanceType\": \"ml.m5.xlarge\",\n            \"InstanceCount\": 1,\n            \"lifecycleConfig\": {\n                \"SourceS3Uri\": \"s3://&lt;your-s3-bucket&gt;/&lt;lifecycle-script-directory&gt;/\",\n                \"OnCreate\": \"on_create.sh\"\n            },\n            \"ExecutionRole\": \"arn:aws:iam::111122223333:role/my-role-for-cluster\",\n            \"ThreadsPerCore\": 1\n        },\n        {\n            \"InstanceGroupName\": \"worker-group\",\n            \"InstanceType\": \"trn1.32xlarge\",\n            \"InstanceCount\": 4,\n            \"lifecycleConfig\": {\n                \"SourceS3Uri\": \"s3://&lt;your-s3-bucket&gt;/&lt;lifecycle-script-directory&gt;/\",\n                \"OnCreate\": \"on_create.sh\"\n            },\n            \"ExecutionRole\": \"arn:aws:iam::111122223333:role/my-role-for-cluster\",\n            \"ThreadsPerCore\": 1\n        }\n    ]\n}</code></pre> \n<p>To create the cluster, I run the following AWS CLI command:</p> \n<pre><code>aws sagemaker create-cluster \\\n--cluster-name antje-demo-cluster \\\n--instance-groups file://demo-cluster.json</code></pre> \n<p>Upon creation, you can use <code>aws sagemaker describe-cluster</code> and <code>aws sagemaker list-cluster-nodes</code> to view your cluster and node details. Note down the cluster ID and instance ID of your controller node. You need that information to connect to your cluster.</p> \n<p>You also have the option to attach a shared file system, such as <a href=\"https://aws.amazon.com/fsx/lustre/\">Amazon FSx for Lustre</a>. To use FSx for Lustre, you need to set up your cluster with an <a href=\"https://aws.amazon.com/vpc/\">Amazon Virtual Private Cloud (Amazon VPC)</a> configuration. Here’s an <a href=\"https://aws.amazon.com/cloudformation/\">AWS CloudFormation</a> template that shows how to <a href=\"https://awsome-distributed-training.s3.amazonaws.com/templates/Vpc.yaml\">create a SageMaker VPC</a> and how to <a href=\"https://awsome-distributed-training.s3.amazonaws.com/templates/FSxLustre.yaml\">deploy FSx for Lustre</a>.</p> \n<p><strong><u>Connect to your cluster<br /> </u></strong>As a cluster user, you need to have access to the cluster provisioned by your cluster admin. With access permissions in place, you can connect to the cluster using SSH to schedule and run jobs. You can use the preinstalled <a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-working-with.html\">AWS CLI plugin for AWS Systems Manager</a> to connect to the controller node of your cluster.</p> \n<p>For my demo, I run the following command specifying my cluster ID and instance ID of the control node as the target.</p> \n<pre><code>aws ssm start-session \\\n--target sagemaker-cluster:ntg44z9os8pn_controller-group-i-05a854e0d4358b59c \\\n--region us-west-2</code></pre> \n<p><strong><u>Schedule and run jobs on the cluster using Slurm<br /> </u></strong>At launch, SageMaker HyperPod supports <a href=\"https://github.com/SchedMD/slurm\">Slurm</a> for workload orchestration. Slurm is a popular an open source cluster management and job scheduling system. You can install and set up Slurm through lifecycle scripts as part of the cluster creation. The <a href=\"https://github.com/aws-samples/awsome-distributed-training/tree/main/1.architectures/5.sagemaker-hyperpod/LifecycleScripts/\">example lifecycle scripts</a> show how. Then, you can use the standard Slurm commands to schedule and launch jobs. Check out the <a href=\"https://slurm.schedmd.com/quickstart.html\">Slurm Quick Start User Guide</a> for architecture details and helpful commands.</p> \n<p>For this demo, I’m using this <a href=\"https://github.com/aws-samples/awsome-distributed-training/tree/main/3.test_cases/8.neuronx-nemo-megatron\">example from the AWS ML Training Reference Architectures GitHub repo</a> that shows how to train Llama 2 7B on Slurm with Trn1 instances. My cluster is already setup with Slurm, and I have an FSx for Lustre filesystem mounted.</p> \n<p><strong>Note</strong><br /> The Llama 2 model is governed by <a href=\"https://ai.meta.com/\">Meta</a>. You can request access through the <a href=\"https://ai.meta.com/resources/models-and-libraries/llama-downloads/\">Meta request access page</a>.</p> \n<p><strong>Set up the cluster environment<br /> </strong>SageMaker HyperPod supports training in a range of environments, including <a href=\"https://docs.conda.io/en/latest/\">Conda</a>, <a href=\"https://docs.python.org/3/library/venv.html\">venv</a>, <a href=\"https://www.docker.com/\">Docker</a>, and <a href=\"https://github.com/NVIDIA/enroot\">enroot</a>. Following the instructions in the <a href=\"https://github.com/aws-samples/awsome-distributed-training/blob/main/3.test_cases/8.neuronx-nemo-megatron/README.md\">README</a>, I build my virtual environment <code>aws_neuron_venv_pytorch</code> and set up the <a href=\"https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/setup/pytorch-install.html\">torch_neuronx</a> and <a href=\"https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/nemo/neuronx-nemo.html\">neuronx-nemo-megatron</a> libraries for training models on Trn1 instances.</p> \n<p><strong>Prepare model, tokenizer, and dataset<br /> </strong>I follow the instructions to download the Llama 2 model and tokenizer and convert the model into the Hugging Face format. Then, I download and tokenize the <a href=\"https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T\">RedPajama dataset</a>. As a final preparation step, I pre-compile the Llama 2 model using ahead-of-time (AOT) compilation to speed up model training.</p> \n<p><strong>Launch jobs on the cluster<br /> </strong>Now, I’m ready to start my model training job using the <code>sbatch</code> command.</p> \n<pre><code>sbatch --nodes 4 --auto-resume=1 run.slurm ./llama_7b.sh</code></pre> \n<p>You can use the <code>squeue</code> command to view the job queue. Once the training job is running, the SageMaker HyperPod resiliency features are automatically enabled. SageMaker HyperPod will automatically detect hardware failures, replace nodes as needed, and resume training from checkpoints if the <code>auto-resume</code> parameter is set, as shown in the preceding command.</p> \n<p>You can view the output of the model training job in the following file:</p> \n<pre><code>tail -f slurm-run.slurm-&lt;JOB_ID&gt;.out</code></pre> \n<p>A sample output indicating that model training has started will look like this:</p> \n<pre><code>Epoch 0:  22%|██▏       | 4499/20101 [22:26:14&lt;77:48:37, 17.95s/it, loss=2.43, v_num=5563, reduced_train_loss=2.470, gradient_norm=0.121, parameter_norm=1864.0, global_step=4512.0, consumed_samples=1.16e+6, iteration_time=16.40]\nEpoch 0:  22%|██▏       | 4500/20101 [22:26:32&lt;77:48:18, 17.95s/it, loss=2.43, v_num=5563, reduced_train_loss=2.470, gradient_norm=0.121, parameter_norm=1864.0, global_step=4512.0, consumed_samples=1.16e+6, iteration_time=16.40]\nEpoch 0:  22%|██▏       | 4500/20101 [22:26:32&lt;77:48:18, 17.95s/it, loss=2.44, v_num=5563, reduced_train_loss=2.450, gradient_norm=0.120, parameter_norm=1864.0, global_step=4512.0, consumed_samples=1.16e+6, iteration_time=16.50]</code></pre> \n<p>To further monitor and profile your model training jobs, you can use <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/tensorboard-on-sagemaker.html\">SageMaker hosted TensorBoard</a> or any other tool of your choice.</p> \n<p><b><u>Now available<br /> </u></b>SageMaker HyperPod is available today in AWS Regions US East (Ohio), US East (N. Virginia), US West (Oregon), Asia Pacific (Singapore), Asia Pacific (Sydney), Asia Pacific (Tokyo), Europe (Frankfurt), Europe (Ireland), and Europe (Stockholm).</p> \n<p><span><strong>Learn more:</strong> </span></p> \n<ul> \n <li>See <a href=\"https://aws.amazon.com/sagemaker/hyperpod\">Amazon SageMaker HyperPod</a> for pricing information and a list of supported cluster instance types</li> \n <li>Check out the <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod.html\">Developer Guide</a></li> \n <li>Visit the <a href=\"https://us-west-2.console.aws.amazon.com/sagemaker/home?region=us-west-2#/cluster-management\">AWS Management Console to start training your FMs with SageMaker HyperPod</a></li> \n</ul> \n<p>— <a href=\"https://www.linkedin.com/in/antje-barth/\" target=\"_blank\">Antje</a></p> \n<p>PS: Writing a blog post at AWS is always a team effort, even when you see only one name under the post title. In this case, I want to thank <a href=\"https://www.linkedin.com/in/doranb/\">Brad Doran</a>, <a href=\"https://www.linkedin.com/in/jpirtle/\">Justin Pirtle</a>, <a href=\"https://www.linkedin.com/in/johnbensnyder/\">Ben Snyder</a>, <a href=\"https://www.linkedin.com/in/aquilanti/\">Pierre-Yves Aquilanti</a>, <a href=\"https://www.linkedin.com/in/keitawatanabe/\">Keita Watanabe</a>, and <a href=\"https://www.linkedin.com/in/verdimarch/\">Verdi March</a> for their generous help with example code and sharing their expertise in managing large-scale model training infrastructures, Slurm, and SageMaker HyperPod.</p>","author":"Antje Barth","siteTitle":"AWS News Blog","siteHash":"6093e072e4117ec22616e844cb857d03ca62c57a411a8affc77cb5e8b6b15bf6","entryHash":"4366f305badedeafdaf40cd800741e4952e069f9a2ac76f2a3e2cba1f8c5d281","category":"Tech"}