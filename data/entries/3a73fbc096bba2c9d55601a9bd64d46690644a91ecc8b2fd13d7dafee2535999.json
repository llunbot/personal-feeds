{"title":"Invisible text that AI chatbots understand and humans can’t? Yep, it’s a thing.","link":"https://arstechnica.com/security/2024/10/ai-chatbots-can-read-and-write-invisible-text-creating-an-ideal-covert-channel/","date":1728932787000,"content":"<p>What if there was a way to sneak malicious instructions into Claude, Copilot, or other top-name AI chatbots and get confidential data out of them by using characters large language models can recognize and their human users can’t? As it turns out, there was—and in some cases still is.</p>\n<p>The invisible characters, the result of a quirk in the <a href=\"https://en.wikipedia.org/wiki/Unicode\">Unicode</a> text encoding standard, create an ideal covert channel that can make it easier for attackers to conceal malicious payloads fed into an LLM. The hidden text can similarly obfuscate the exfiltration of passwords, financial information, or other secrets out of the same AI-powered bots. Because the hidden text can be combined with normal text, users can unwittingly paste it into prompts. The secret content can also be appended to visible text in chatbot output.</p>\n<p>The result is a <a href=\"https://en.wikipedia.org/wiki/Steganography\">steganographic</a> framework built into the most widely used text encoding channel.</p><p><a href=\"https://arstechnica.com/security/2024/10/ai-chatbots-can-read-and-write-invisible-text-creating-an-ideal-covert-channel/\">Read full article</a></p>\n<p><a href=\"https://arstechnica.com/security/2024/10/ai-chatbots-can-read-and-write-invisible-text-creating-an-ideal-covert-channel/#comments\">Comments</a></p>","author":"Dan Goodin","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"3a73fbc096bba2c9d55601a9bd64d46690644a91ecc8b2fd13d7dafee2535999","category":"Tech"}