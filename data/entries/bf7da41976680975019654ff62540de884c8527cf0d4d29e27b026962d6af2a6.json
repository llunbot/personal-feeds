{"title":"Apple Publishes FAQ for Their New Child Safety Features (PDF)","link":"https://www.apple.com/child-safety/pdf/Expanded_Protections_for_Children_Frequently_Asked_Questions.pdf","date":1628552880000,"content":"\n<p>Apple:</p>\n\n<blockquote>\n  <p><em>Could governments force Apple to add non-CSAM images to the\nhash list?</em></p>\n\n<p>Apple will refuse any such demands. Apple’s CSAM detection\ncapability is built solely to detect known CSAM images stored in\niCloud Photos that have been identified by experts at NCMEC and\nother child safety groups. We have faced demands to build and\ndeploy government-mandated changes that degrade the privacy of\nusers before, and have steadfastly refused those demands. We will\ncontinue to refuse them in the future. Let us be clear, this\ntechnology is limited to detecting CSAM stored in iCloud and we\nwill not accede to any government’s request to expand it.\nFurthermore, Apple conducts human review before making a report to\nNCMEC. In a case where the system flags photos that do not match\nknown CSAM images, the account would not be disabled and no report\nwould be filed to NCMEC.</p>\n\n<p><em>Can non-CSAM images be “injected” into the system to flag ac-\ncounts for things other than CSAM?</em></p>\n\n<p>Our process is designed to prevent that from happening. The set of\nimage hashes used for matching are from known, existing images of\nCSAM that have been acquired and validated by child safety\norganizations. Apple does not add to the set of known CSAM image\nhashes. The same set of hashes is stored in the operating system\nof every iPhone and iPad user, so targeted attacks against only\nspecific individuals are not possible under our design. Finally,\nthere is no automated reporting to law enforcement, and Apple\nconducts human review before making a report to NCMEC. In the\nunlikely event of the system flagging images that do not match\nknown CSAM images, the account would not be disabled and no report\nwould be filed to NCMEC.</p>\n</blockquote>\n\n<p>This FAQ is good, and addresses most of the misconceptions I’ve seen. The human review step for flagged accounts is key to the trustworthiness of the system.</p>\n\n<p>I do wonder though, how prepared Apple is for manually reviewing a potentially staggering number of accounts being correctly flagged. Because Apple doesn’t examine the contents of iCloud Photo Library (or local on-device libraries), I don’t think anyone knows how prevalent CSAM is on iCloud Photos. We know <a href=\"https://www.missingkids.org/gethelpnow/cybertipline#bythenumbers\">Facebook reported 20 million instances of CSAM to NCMEC last year, and Google reported 546,000</a>. For Facebook, that’s about 55,000 per day; for Google, 1,500 per day. I think it’s a genuine “we’ll soon find out” mystery how many iCloud Photo users are going to be accurately flagged for exceeding the threshold for CSAM matches when this goes live. If the number is large, it seems like one innocent needle in a veritable haystack of actual CSAM collections might be harder for Apple’s human reviewers to notice.</p>\n\n<div>\n<a href=\"https://daringfireball.net/linked/2021/08/09/apple-csam-faq\"> ★ </a>\n</div>\n\n\t","author":"John Gruber","siteTitle":"Daring Fireball","siteHash":"fc569638025dadf22a867470f8215f38855cf50e975782a6c989909474292a36","entryHash":"bf7da41976680975019654ff62540de884c8527cf0d4d29e27b026962d6af2a6","category":"Tech"}