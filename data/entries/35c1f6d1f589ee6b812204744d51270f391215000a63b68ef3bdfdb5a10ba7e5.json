{"title":"Intel’s “Gaudi 3” AI accelerator chip may give Nvidia’s H100 a run for its money","link":"https://arstechnica.com/?p=2016421","date":1712869019000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/04/newsroom-intel-gaudi-3-2.jpg.rendition.intel_.web_.1648.927-800x450.jpg\" alt=\"An Intel handout photo of the Gaudi 3 AI accelerator.\" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2024/04/newsroom-intel-gaudi-3-2.jpg.rendition.intel_.web_.1648.927.jpg\">Enlarge</a> <span>/</span> An Intel handout photo of the Gaudi 3 AI accelerator. (credit: <a href=\"https://www.intel.com/content/www/us/en/newsroom/news/vision-2024-gaudi-3-ai-accelerator.html\">Intel</a>)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>On Tuesday, Intel <a href=\"https://www.intel.com/content/www/us/en/newsroom/news/vision-2024-gaudi-3-ai-accelerator.html#gs.794k93\">revealed</a> a new AI accelerator chip called Gaudi 3 at its Vision 2024 event in Phoenix. With strong claimed performance while running large language models (like those that power <a href=\"https://arstechnica.com/information-technology/2023/11/chatgpt-was-the-spark-that-lit-the-fire-under-generative-ai-one-year-ago-today/\">ChatGPT</a>), the company has positioned Gaudi 3 as an alternative to Nvidia's <a href=\"https://arstechnica.com/information-technology/2022/09/hopper-time-nvidias-most-powerful-ai-chip-yet-ships-in-october/\">H100</a>, a popular data center GPU that has been <a href=\"https://www.wired.com/story/nvidia-chip-shortages-leave-ai-startups-scrambling-for-computing-power/\">subject to shortages</a>, though apparently that is <a href=\"https://www.tomshardware.com/pc-components/gpus/nvidias-h100-ai-gpu-shortages-ease-as-lead-times-drop-from-up-to-four-months-to-8-12-weeks\">easing somewhat</a>.</p>\n<p>Compared to Nvidia's H100 chip, Intel projects a 50 percent faster training time on Gaudi 3 for both OpenAI's GPT-3 175B LLM and the 7-billion parameter version of Meta's <a href=\"https://arstechnica.com/information-technology/2023/07/meta-launches-llama-2-an-open-source-ai-model-that-allows-commercial-applications/\">Llama 2</a>. In terms of inference (running the trained model to get outputs), Intel claims that its new AI chip delivers 50 percent faster performance than H100 for Llama 2 and <a href=\"https://huggingface.co/tiiuae/falcon-180B\">Falcon 180B</a>, which are both open-weights models.</p>\n\n<p>Intel is targeting the H100 because of its <a href=\"https://www.extremetech.com/computing/analysts-estimate-nvidia-owns-98-of-the-data-center-gpu-market\">high market share</a>, but the chip isn't Nvidia's most powerful AI accelerator chip in the pipeline. Announcements of the <a href=\"https://arstechnica.com/information-technology/2023/11/nvidia-introduces-its-most-powerful-gpu-yet-designed-for-accelerating-ai/\">H200</a> and the Blackwell <a href=\"https://arstechnica.com/information-technology/2024/03/nvidia-unveils-blackwell-b200-the-worlds-most-powerful-chip-designed-for-ai/\">B200</a> have since surpassed the H100 on paper, but neither of those chips is out yet (the H200 is <a href=\"https://www.servethehome.com/micron-hbm3e-in-production-set-for-nvidia-h200-use-in-q2-2024/\">expected</a> in the second quarter of 2024—basically any day now).</p></div><p><a href=\"https://arstechnica.com/?p=2016421#p3\">Read 10 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=2016421&amp;comments=1\">Comments</a></p>","author":"Benj Edwards","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"35c1f6d1f589ee6b812204744d51270f391215000a63b68ef3bdfdb5a10ba7e5","category":"Tech"}