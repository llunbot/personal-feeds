{"title":"University of Chicago researchers seek to “poison” AI art generators with Nightshade","link":"https://arstechnica.com/?p=1978501","date":1698268883000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2023/10/robot_poison_hero-800x450.jpg\" alt=\"Robotic arm holding dangerous chemical.\" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2023/10/robot_poison_hero.jpg\">Enlarge</a> (credit: <a href=\"https://www.gettyimages.com/detail/photo/robotic-arm-holding-dangerous-chemical-royalty-free-image/1055493662\">Getty Images</a>)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>On Friday, a team of researchers at the University of Chicago released a <a href=\"https://arxiv.org/pdf/2310.13828.pdf\">research paper</a> outlining \"Nightshade,\" a data poisoning technique aimed at disrupting the training process for AI models, reports <a href=\"https://www.technologyreview.com/2023/10/23/1082189/data-poisoning-artists-fight-generative-ai/\">MIT Technology Review</a> and <a href=\"https://venturebeat.com/ai/meet-nightshade-the-new-tool-allowing-artists-to-poison-ai-models-with-corrupted-training-data/\">VentureBeat</a>. The goal is to help visual artists and publishers protect their work from being used to train generative AI image synthesis models, such as <a href=\"https://arstechnica.com/information-technology/2023/06/stunning-midjourney-update-wows-ai-artists-with-camera-like-feature/\">Midjourney</a>, <a href=\"https://arstechnica.com/information-technology/2023/09/openai-announces-dall-e-3-a-next-gen-ai-image-generator-based-on-chatgpt/\">DALL-E 3</a>, and <a href=\"https://arstechnica.com/information-technology/2022/09/with-stable-diffusion-you-may-never-believe-what-you-see-online-again/\">Stable Diffusion</a>.</p>\n\n<p>The open source \"poison pill\" tool (as the University of Chicago's press department calls it) alters images in ways invisible to the human eye that can corrupt an AI model's training process. Many image synthesis models, with notable exceptions of those from <a href=\"https://arstechnica.com/information-technology/2023/03/ethical-ai-art-generation-adobe-firefly-may-be-the-answer/\">Adobe</a> and <a href=\"https://arstechnica.com/ai/2023/09/getty-images-subscribers-to-get-access-to-ai-image-generator/\">Getty Images</a>, largely use data sets of images scraped from the web without artist permission, which includes copyrighted material. (OpenAI <a href=\"https://arstechnica.com/information-technology/2022/10/shutterstock-partners-with-openai-to-sell-ai-generated-artwork-compensate-artists/\">licenses</a> some of its DALL-E training images from Shutterstock.)</p>\n<p>AI researchers' reliance on commandeered data scraped from the web, which is seen as <a href=\"https://arstechnica.com/information-technology/2022/09/have-ai-image-generators-assimilated-your-art-new-tool-lets-you-check/\">ethically fraught</a> by many, has also been key to the recent explosion in generative AI capability. It took an entire Internet of images with annotations (through captions, alt text, and metadata) created by millions of people to create a data set with enough variety to create Stable Diffusion, for example. It would be impractical to hire people to annotate hundreds of millions of images from the standpoint of both cost and time. Those with access to existing large image databases (such as Getty and Shutterstock) are at an advantage when using licensed training data.</p></div><p><a href=\"https://arstechnica.com/?p=1978501#p3\">Read 10 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=1978501&amp;comments=1\">Comments</a></p>","author":"Benj Edwards","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"9e7c12316da0c328ed187807afd982129431018ab8d0104f26d9e6c1e692a974","category":"Tech"}