{"title":"Microsoft’s VASA-1 can deepfake a person with one photo and one audio track","link":"https://arstechnica.com/?p=2018178","date":1713532026000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/04/teaser-800x410.jpg\" alt=\"A sample image from Microsoft for \" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2024/04/teaser-scaled.jpg\">Enlarge</a> <span>/</span> A sample image from Microsoft for \"VASA-1: Lifelike Audio-Driven Talking Faces\r\nGenerated in Real Time.\" (credit: <a href=\"https://www.microsoft.com/en-us/research/project/vasa-1/\">Microsoft</a>)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>On Tuesday, Microsoft Research Asia unveiled <a href=\"https://www.microsoft.com/en-us/research/project/vasa-1/\">VASA-1</a>, an AI model that can create a synchronized animated video of a person talking or singing from a single photo and an existing audio track. In the future, it could power virtual avatars that render locally and don't require video feeds—or allow anyone with similar tools to take a photo of a person found online and make them appear to say whatever they want.</p>\n\n<p>\"It paves the way for real-time engagements with lifelike avatars that emulate human conversational behaviors,\" reads the abstract of the <a href=\"https://arxiv.org/abs/2404.10667\">accompanying research paper</a> titled, \"VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time.\" It's the work of Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang, Chong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, and Baining Guo.</p>\n<p>The VASA framework (short for \"Visual Affective Skills Animator\") uses machine learning to analyze a static image along with a speech audio clip. It is then able to generate a realistic video with precise facial expressions, head movements, and lip-syncing to the audio. It does not clone or simulate voices (like <a href=\"https://arstechnica.com/information-technology/2023/01/microsofts-new-ai-can-simulate-anyones-voice-with-3-seconds-of-audio/\">other Microsoft research</a>) but relies on an existing audio input that could be specially recorded or spoken for a particular purpose.</p></div><p><a href=\"https://arstechnica.com/?p=2018178#p3\">Read 11 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=2018178&amp;comments=1\">Comments</a></p>","author":"Benj Edwards","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"1f0717f4be8423d41025b30b6db68bff97829b67c2567d7f5d420deee78bd4aa","category":"Tech"}