{"title":"New camera design can ID threats faster, using less memory","link":"https://arstechnica.com/?p=2029870","date":1717787993000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/06/GettyImages-1633794046-800x533.jpg\" alt=\"Image out the windshield of a car, with other vehicles highlighted by computer-generated brackets.\" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2024/06/GettyImages-1633794046.jpg\">Enlarge</a> (credit: <a href=\"https://www.gettyimages.com/detail/photo/self-driving-of-autonomous-car-royalty-free-image/1633794046?phrase=driving+hazard+detection\">Witthaya Prasongsin</a>)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>Elon Musk, back in October 2021, tweeted that “humans drive with eyes and biological neural nets, so cameras and silicon neural nets are only way to achieve generalized solution to self-driving.” The problem with his logic has been that human eyes are way better than RGB cameras at detecting fast-moving objects and estimating distances. Our brains have also surpassed all artificial neural nets by a wide margin at general processing of visual inputs.</p>\n<p>To bridge this gap, a team of scientists at the University of Zurich developed a new automotive object-detection system that brings digital camera performance that’s much closer to human eyes. “Unofficial sources say Tesla uses multiple Sony IMX490 cameras with 5.4-megapixel resolution that [capture] up to 45 frames per second, which translates to perceptual latency of 22 milliseconds. Comparing [these] cameras alone to our solution, we already see a 100-fold reduction in perceptual latency,” says Daniel Gehrig, a researcher at the University of Zurich and lead author of the study.</p>\n<h2>Replicating human vision</h2>\n<p>When a pedestrian suddenly jumps in front of your car, multiple things have to happen before a driver-assistance system initiates emergency braking. First, the pedestrian must be captured in images taken by a camera. The time this takes is called perceptual latency—it’s a delay between the existence of a visual stimuli and its appearance in the readout from a sensor. Then, the readout needs to get to a processing unit, which adds a network latency of around 4 milliseconds.</p></div><p><a href=\"https://arstechnica.com/?p=2029870#p3\">Read 14 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=2029870&amp;comments=1\">Comments</a></p>","author":"Jacek Krywko","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"89505c32e627b37731502de1aec51c4b16ac4ebbac98d07c955a5bd4926073eb","category":"Tech"}