{"title":"DeepSeek ปล่อยซอฟต์แวร์พื้นฐานสำหรับรัน LLM ชุดใหญ่ เร่งความเร็วการรันได้ 3 เท่า","link":"https://www.blognone.com/node/145004","date":1740808788000,"content":"<div><div><div><p>DeepSeek จัดมหกรรมโอเพนซอร์สประจำสัปดาห์ โดยปล่อยซอฟต์แวร์ที่ใช้พัฒนาและให้บริการ DeepSeek ออกมาเป็นชุด ในกลุ่มนี้มีหลายตัวได้รับความสนใจอย่างสูง เพราะสามารถเร่งความเร็วได้มาก แถมยังเปิดทางแคชการประมวลผลไว้ได้ง่ายขึ้น โครงการที่เปิดมาแล้ว ได้แก่</p>\n<ul>\n<li><a href=\"https://github.com/deepseek-ai/FlashMLA\">FlashMLA</a>: decoding kernel ที่ออปติไมซ์สำหรับ NVIDIA Hopper โดยเฉพาะ พัฒนาต่อมาจาก Flash Attention ตอนนี้ vLLM นำเทคนิคนี้ไปใช้งานแล้ว ส่งผลให้รันโมเดล DeepSeek ประสิทธิภาพดีขึ้น 3 เท่าตัว และเก็บ token ไว้ในหน่วยความจำได้มากขึ้น 10 เท่า</li>\n<li><a href=\"https://github.com/deepseek-ai/DeepEP\">DeepEP</a>: ไลบรารีสื่อสารข้ามชิปกราฟิกที่ออปติไมซ์สำหรับการรันโมเดลในกลุ่ม Mixture-of-Experts (MoE) เน้นการลด latenncy ให้ต่ำสุด</li>\n<li><a href=\"https://github.com/deepseek-ai/DeepGEMM\">DeepGEMM</a>: ไลบรารี CUDA สำหรับการคูณ matrix แบบ FP8 ความเร็วเพิ่มขึ้นสูงสุด 2.7 เท่า แย่ที่สุดคือเท่าเดิม</li>\n<li><a href=\"https://github.com/deepseek-ai/EPLB\">EPLB</a>: load balancer สำหรับการรันโมเดลปัญญาประดิษฐ์แบบ MoE ที่ต้องปรับระดับโหลดของแต่ละ expert ในระบบให้เหมาะสม</li>\n<li><a href=\"https://github.com/deepseek-ai/DualPipe\">DualPipe</a>: pipeline สำหรับรันปัญญาประดิษฐ์ที่เริ่มใช้ครั้งแรกใน DeepSeek-V3 แยกออกมาเป็นไลบรารีให้ใช้งานภายนอกได้</li>\n<li><a href=\"https://github.com/deepseek-ai/3FS\">3FS</a>: ระบบไฟล์แบบกระจายตัว ทำให้สามารถดึงข้อมูลเข้าโมเดลปัญญาประดิษฐ์ได้เต็มประสิทธิภาพ SSD</li>\n</ul>\n<p>DeepSeek ปิดมหกรรมนี้ด้วยการนำเสนอสถาปัตยกรรมระบบรันโมเดลปัญญาประดิษฐ์ประสิทธิภาพสูง ที่ต้องใช้ชิป H800 ราคาแพงให้เต็มประสิทธิภาพ (ต้นทุน H800 ประมาณชั่วโมงละ 2 ดอลลาร์) การใช้งานตอนนี้เว้นว่างเพียงช่วงเที่ยงคืนถึง 9 โมงเช้า หากคิดราคาค่าโทเค็นตาม DeepSeek-R1 เต็มระบบก็จะทำรายได้ (เฉพาะค่าเซิร์ฟเวอร์) ได้ถึงวันละ 562,027 ดอลลาร์ กำไรต่อต้นทุน 545% แต่ในความเป็นจริงมีคนใช้บริการฟรี หรือใช้ DeepSeek-V3 ที่ราคาถูกกว่าจำนวนมาก ตลอดจนมีส่วนลดการใช้ช่วงเวลาคนใช้น้อยอีก</p>\n<p>ที่มา - <a href=\"https://x.com/deepseek_ai/status/1895688300574462431\">@DeepSeek_AI</a></p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/62799a6aba85980f6f2078e3b83cd3d2.jpg\" /></p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/aa791848ab87c9c79c93b27751c8d538.jpg\" /></p>\n</div></div></div><div><div>Topics: </div><div><div><a href=\"/topics/deepseek\">DeepSeek</a></div><div><a href=\"/topics/llm\">LLM</a></div><div><a href=\"/topics/artificial-intelligence\">Artificial Intelligence</a></div></div></div>","author":"lew","siteTitle":"Blognone","siteHash":"ededadcf18490b3937e7dd89ebe8c00dc129addbdf1ebe4aff1f458146693da0","entryHash":"7cfb78b760a5525d47625b97cec6e99cd8ae0d9d651a8fa6716f969a329652f3","category":"Thai"}