{"title":"Anthropic dares you to jailbreak its new AI model","link":"https://arstechnica.com/ai/2025/02/anthropic-dares-you-to-jailbreak-its-new-ai-model/","date":1738620591000,"content":"<p>Even the most permissive corporate AI models have sensitive topics that their creators would prefer they not discuss (e.g., weapons of mass destruction, illegal activities, or, uh, <a href=\"https://arstechnica.com/ai/2025/01/the-questions-the-chinese-government-doesnt-want-deepseek-ai-to-answer/\">Chinese political history</a>). Over the years, enterprising AI users have resorted to everything from <a href=\"https://arstechnica.com/ai/2023/08/researchers-figure-out-how-to-make-ai-misbehave-serve-up-prohibited-content/\">weird text strings</a> to <a href=\"https://arstechnica.com/security/2024/03/researchers-use-ascii-art-to-elicit-harmful-responses-from-5-major-ai-chatbots/\">ASCII art</a> to <a href=\"https://arstechnica.com/information-technology/2023/10/sob-story-about-dead-grandma-tricks-microsoft-ai-into-solving-captcha/\">stories about dead grandmas</a> in order to jailbreak those models into giving the \"forbidden\" results.</p>\n<p>Today, Claude model-maker Anthropic has <a href=\"https://www.anthropic.com/research/constitutional-classifiers\">released a new system of Constitutional Classifiers</a> that it says can \"filter the overwhelming majority\" of those kinds of jailbreaks. And now that the system has held up to over 3,000 hours of bug bounty attacks, Anthropic is inviting the wider public to <a href=\"https://claude.ai/constitutional-classifiers\">test out the system</a> to see if it can fool it into breaking its own rules.</p>\n<h2>Respect the constitution</h2>\n<p>In <a href=\"https://arxiv.org/abs/2501.18837\">a new paper</a> and <a href=\"https://www.anthropic.com/research/constitutional-classifiers\">accompanying blog post</a>, Anthropic says its new Constitutional Classifier system is spun off from the similar <a href=\"https://arxiv.org/abs/2212.08073\">Constitutional AI system</a> that was used to build its Claude model. The system relies at its core on a \"constitution\" of natural language rules defining broad categories of permitted (e.g., listing common medications) and disallowed (e.g., acquiring restricted chemicals) content for the model.</p><p><a href=\"https://arstechnica.com/ai/2025/02/anthropic-dares-you-to-jailbreak-its-new-ai-model/\">Read full article</a></p>\n<p><a href=\"https://arstechnica.com/ai/2025/02/anthropic-dares-you-to-jailbreak-its-new-ai-model/#comments\">Comments</a></p>","author":"Kyle Orland","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"1a2a07fda938779d05eb90c8c3bad3d86534939f21c7b8118917128dbad792f7","category":"Tech"}