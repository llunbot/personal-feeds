{"title":"1960s chatbot ELIZA beat OpenAI’s GPT-3.5 in a recent Turing test study","link":"https://arstechnica.com/?p=1986387","date":1701466055000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2023/11/turing_test_hero-800x450.jpg\" alt=\"An illustration of a man and a robot sitting in boxes, talking.\" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2023/11/turing_test_hero.jpg\">Enlarge</a> <span>/</span> An artist's impression of a human and a robot talking. (credit: Getty Images | Benj Edwards)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>In a preprint <a href=\"https://arxiv.org/abs/2310.20216\">research paper</a> titled \"Does GPT-4 Pass the Turing Test?\", two researchers from UC San Diego pitted OpenAI's <a href=\"https://arstechnica.com/information-technology/2023/03/openai-announces-gpt-4-its-next-generation-ai-language-model/\">GPT-4</a> AI language model against human participants, GPT-3.5, and <a href=\"https://en.wikipedia.org/wiki/ELIZA\">ELIZA</a> to see which could trick participants into thinking it was human with the greatest success. But along the way, the study, which has not been peer-reviewed, found that human participants correctly identified other humans in only 63 percent of the interactions—and that a 1960s computer program surpassed the AI model that powers the free version of ChatGPT.</p>\n\n<p>Even with limitations and caveats, which we'll cover below, the paper presents a thought-provoking comparison between AI model approaches and raises further questions about using the <a href=\"https://en.wikipedia.org/wiki/Turing_test\">Turing test</a> to evaluate AI model performance.</p>\n<p>British mathematician and computer scientist Alan Turing first conceived the Turing test as \"The Imitation Game\" <a href=\"http://phil415.pbworks.com/f/TuringComputing.pdf\">in 1950</a>. Since then, it has become a famous but controversial benchmark for determining a machine's ability to imitate human conversation. In modern versions of the test, a human judge typically talks to either another human or a chatbot without knowing which is which. If the judge cannot reliably tell the chatbot from the human a certain percentage of the time, the chatbot is said to have passed the test. The threshold for passing the test is subjective, so there has never been a broad consensus on what would constitute a passing success rate.</p></div><p><a href=\"https://arstechnica.com/?p=1986387#p3\">Read 13 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=1986387&amp;comments=1\">Comments</a></p>","author":"Benj Edwards","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"127e4843307e1ad756e839a3cfb05be4f5fb459fb9334cd5fd3ffefb96db639b","category":"Tech"}