{"title":"Ban warnings fly as users dare to probe the “thoughts” of OpenAI’s latest model","link":"https://arstechnica.com/?p=2049959","date":1726526951000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/09/brain_gears_header-800x450.jpg\" alt=\"An illustration of gears shaped like a brain.\" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2024/09/brain_gears_header.jpg\">Enlarge</a> (credit: <a href=\"https://www.gettyimages.com/detail/photo/brain-gear-mechanism-royalty-free-image/1436010616\">Andriy Onufriyenko via Getty Images</a>)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>OpenAI truly does not want you to know what its latest AI model is \"thinking.\" Since the company <a href=\"https://arstechnica.com/information-technology/2024/09/openais-new-reasoning-ai-models-are-here-o1-preview-and-o1-mini/\">launched</a> its \"Strawberry\" AI model family last week, touting so-called reasoning abilities with o1-preview and o1-mini, OpenAI has been sending out warning emails and threats of bans to any user who tries to probe into how the model works.</p>\n\n<p>Unlike previous AI models from OpenAI, such as <a href=\"https://arstechnica.com/information-technology/2024/05/chatgpt-4o-lets-you-have-real-time-audio-video-conversations-with-emotional-chatbot/\">GPT-4o</a>, the company trained o1 specifically to work through a step-by-step problem-solving process before generating an answer. When users ask an \"o1\" model a question in <a href=\"https://arstechnica.com/information-technology/2023/11/chatgpt-was-the-spark-that-lit-the-fire-under-generative-ai-one-year-ago-today/\">ChatGPT</a>, users have the option of seeing this chain-of-thought process written out in the ChatGPT interface. However, by design, OpenAI hides the raw chain of thought from users, instead presenting a filtered interpretation created by a second AI model.</p>\n<p>Nothing is more enticing to enthusiasts than information obscured, so the race has been on among hackers and red-teamers to try to uncover o1's raw chain of thought using <a href=\"https://arstechnica.com/information-technology/2023/10/sob-story-about-dead-grandma-tricks-microsoft-ai-into-solving-captcha/\">jailbreaking</a> or <a href=\"https://arstechnica.com/information-technology/2022/09/twitter-pranksters-derail-gpt-3-bot-with-newly-discovered-prompt-injection-hack/\">prompt injection</a> techniques that attempt to trick the model into spilling its secrets. There have been early reports of some successes, but nothing has yet been strongly confirmed.</p></div><p><a href=\"https://arstechnica.com/?p=2049959#p3\">Read 10 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=2049959&amp;comments=1\">Comments</a></p>","author":"Benj Edwards","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"06059e1c10cfd785a00fb03430d8f6bf38791aae9fc8f7d3e3324e23433cb639","category":"Tech"}