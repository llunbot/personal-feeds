{"title":"Researchers upend AI status quo by eliminating matrix multiplication in LLMs","link":"https://arstechnica.com/?p=2033314","date":1719354471000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/06/AI_lightbulb-800x450.jpg\" alt=\"Illustration of a brain inside of a light bulb.\" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2024/06/AI_lightbulb.jpg\">Enlarge</a> <span>/</span> Illustration of a brain inside of a light bulb. (credit: <a href=\"https://www.gettyimages.com/detail/photo/artificial-intelligence-domination-light-bulb-brain-royalty-free-image/1985871636\">Getty Images</a>)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>Researchers claim to have developed a new way to run AI language models more efficiently by eliminating matrix multiplication from the process. This fundamentally redesigns neural network operations that are currently accelerated by GPU chips. The findings, detailed in a <a href=\"https://arxiv.org/abs/2406.02528\">recent preprint paper</a> from researchers at the University of California Santa Cruz, UC Davis, LuxiTech, and Soochow University, could have deep implications for the <a href=\"https://arstechnica.com/ai/2024/06/is-generative-ai-really-going-to-wreak-havoc-on-the-power-grid/\">environmental impact</a> and operational costs of AI systems.</p>\n\n<p><a href=\"https://en.wikipedia.org/wiki/Matrix_multiplication\">Matrix multiplication</a> (often abbreviated to \"MatMul\") is at the <a href=\"https://arstechnica.com/information-technology/2022/10/deepmind-breaks-50-year-math-record-using-ai-new-record-falls-a-week-later/\">center</a> of most neural network computational tasks today, and GPUs are particularly good at executing the math quickly because they can perform large numbers of multiplication operations in parallel. That ability momentarily made Nvidia the <a href=\"https://www.wsj.com/tech/ai/nvidias-ascent-to-most-valuable-company-has-echoes-of-dot-com-boom-dd836c90\">most valuable company</a> in the world last week; the company currently holds an estimated <a href=\"https://www.hpcwire.com/2024/06/10/nvidia-shipped-3-76-million-data-center-gpus-in-2023-according-to-study/\">98 percent market share</a> for data center GPUs, which are commonly used to power AI systems like <a href=\"https://arstechnica.com/information-technology/2023/11/chatgpt-was-the-spark-that-lit-the-fire-under-generative-ai-one-year-ago-today/\">ChatGPT</a> and <a href=\"https://arstechnica.com/information-technology/2023/12/google-launches-gemini-a-powerful-ai-model-it-says-can-surpass-gpt-4/\">Google Gemini</a>.</p>\n<p>In the new paper, titled \"Scalable MatMul-free Language Modeling,\" the researchers describe creating a custom 2.7 billion parameter model without using MatMul that features similar performance to conventional large language models (LLMs). They also demonstrate running a 1.3 billion parameter model at 23.8 tokens per second on a GPU that was accelerated by a custom-programmed <a href=\"https://en.wikipedia.org/wiki/Field-programmable_gate_array\">FPGA</a> chip that uses about 13 watts of power (not counting the GPU's power draw). The implication is that a more efficient FPGA \"paves the way for the development of more efficient and hardware-friendly architectures,\" they write.</p></div><p><a href=\"https://arstechnica.com/?p=2033314#p3\">Read 13 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=2033314&amp;comments=1\">Comments</a></p>","author":"Benj Edwards","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"5611a7db27b3deca610ed409e0a99c659d67a07c66e0e80d407d41f11acca413","category":"Tech"}