{"title":"‘LLM in a Flash: Efficient Large Language Model Inference With Limited Memory’ (PDF)","link":"https://arxiv.org/pdf/2312.11514.pdf","date":1713815931000,"content":"\n<p>Re: <a href=\"https://daringfireball.net/linked/2024/04/22/on-device-ai-craves-ram\">my previous item on LLMs being RAM-hungry</a> while iPhones are relatively low on RAM, this certainly isn’t news to Apple. Back in December, a team of eight researchers from Apple published this paper, which states in its abstract:</p>\n\n<blockquote>\n  <p>This paper tackles the challenge of efficiently running LLMs that \nexceed the available DRAM capacity by storing the model parameters \nin flash memory, but bringing them on demand to DRAM. Our method \ninvolves constructing an inference cost model that takes into \naccount the characteristics of flash memory, guiding us to \noptimize in two critical areas: reducing the volume of data \ntransferred from flash and reading data in larger, more contiguous \nchunks. Within this hardware-informed framework, we introduce two \nprincipal techniques. First, “windowing” strategically reduces \ndata transfer by reusing previously activated neurons, and second, \n“row-column bundling”, tailored to the sequential data access \nstrengths of flash memory, increases the size of data chunks read \nfrom flash memory. These methods collectively enable running \nmodels up to twice the size of the available DRAM, with a 4-5× and \n20-25× increase in inference speed compared to naive loading \napproaches in CPU and GPU, respectively. Our integration of \nsparsity awareness, context-adaptive loading, and a \nhardware-oriented design paves the way for effective inference of \nLLMs on devices with limited memory. </p>\n</blockquote>\n\n<div>\n<a href=\"https://daringfireball.net/linked/2024/04/22/llm-in-a-flash\"> ★ </a>\n</div>\n\n\t","author":"John Gruber","siteTitle":"Daring Fireball","siteHash":"fc569638025dadf22a867470f8215f38855cf50e975782a6c989909474292a36","entryHash":"d2408d3a73b7118757a7327072cc1c42be140f3a6b3fc7a76530bfe1bf510cbf","category":"Tech"}