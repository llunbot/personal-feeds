{"title":"Major ChatGPT-4o update allows audio-video talks with an “emotional” AI chatbot","link":"https://arstechnica.com/?p=2023751","date":1715623088000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/05/colorful_waveform_1-800x450.jpg\" alt=\"Abstract multicolored waveform\" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2024/05/colorful_waveform_1.jpg\">Enlarge</a> (credit: <a href=\"https://www.gettyimages.com/detail/photo/abstract-multicolored-curve-chart-stock-photo-royalty-free-image/1469448038\">Getty Images</a>)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>On Monday, OpenAI debuted <a href=\"https://openai.com/index/hello-gpt-4o/\">GPT-4o</a> (o for \"omni\"), a major new AI model that can ostensibly converse using speech in real time, reading emotional cues and responding to visual input. It operates faster than OpenAI's previous best model, <a href=\"https://arstechnica.com/information-technology/2023/11/openai-introduces-gpt-4-turbo-larger-memory-lower-cost-new-knowledge/\">GPT-4 Turbo</a>, and will be free for ChatGPT users and available as a service through API, rolling out over the next few weeks, OpenAI says.</p>\n\n<p>OpenAI revealed the new audio conversation and vision comprehension capabilities in a YouTube <a href=\"https://www.youtube.com/watch?v=DQacCB9tDaw&amp;t=2s\">livestream</a> titled \"OpenAI Spring Update,\" presented by OpenAI CTO Mira Murati and employees Mark Chen and Barret Zoph that included live demos of GPT-4o in action.</p>\n<p>OpenAI claims that GPT-4o responds to audio inputs in about 320 milliseconds on average, which is similar to human response times in conversation, according to a <a href=\"https://www.pnas.org/doi/10.1073/pnas.0903616106\">2009 study</a>, and much shorter than the typical 2–3 second lag experienced with previous models. With GPT-4o, OpenAI says it trained a brand-new AI model end-to-end using text, vision, and audio in a way that all inputs and outputs \"are processed by the same neural network.\"</p></div><p><a href=\"https://arstechnica.com/?p=2023751#p3\">Read 11 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=2023751&amp;comments=1\">Comments</a></p>","author":"Benj Edwards","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"5b4a9671ab2857151a1cb4f9045e363a7c6ab754b13e977b6838e60c07490d97","category":"Tech"}