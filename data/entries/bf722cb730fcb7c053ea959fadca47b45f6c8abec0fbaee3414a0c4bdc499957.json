{"title":"ASCII art elicits harmful responses from 5 major AI chatbots","link":"https://arstechnica.com/?p=2010646","date":1710548244000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/03/ascii-art-hacker-800x585.jpg\" alt=\"Some ASCII art of our favorite visual cliche for a hacker. \" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2024/03/ascii-art-hacker.jpg\">Enlarge</a> <span>/</span> Some ASCII art of our favorite visual cliche for a hacker.  (credit: Getty Images)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>Researchers have discovered a new way to hack AI assistants that uses a surprisingly old-school method: ASCII art. It turns out that chat-based large language models such as GPT-4 get so distracted trying to process these representations that they forget to enforce rules blocking harmful responses, such as those providing instructions for building bombs.</p>\n<p>ASCII art became popular in the 1970s, when the limitations of computers and printers prevented them from displaying images. As a result, users depicted images by carefully choosing and arranging printable characters defined by the American Standard Code for Information Interchange, more widely known as ASCII. The explosion of bulletin board systems in the 1980s and 1990s further popularized the format.</p>\n<pre> @_____\r\n  \\_____)|      /\r\n  /(\"\"\")\\o     o\r\n  ||*_-|||    /\r\n   \\ = / |   /\r\n ___) (__|  /\r\n/ \\ \\_/##|\\/\r\n| |\\  ###|/\\\r\n| |\\\\###&amp;&amp;&amp;&amp;\r\n| (_###&amp;&amp;&amp;&amp;&amp;&gt;\r\n(____|(B&amp;&amp;&amp;&amp;\r\n   ++++\\&amp;&amp;&amp;/\r\n  ###(O)###\\\r\n ####AAA####\r\n ####AAA####\r\n ###########\r\n ###########\r\n ###########\r\n   |_} {_|\r\n   |_| |_|\r\n   | | | |\r\nScS| | | |\r\n   |_| |_|\r\n  (__) (__)\r\n</pre>\n<pre>_._\r\n .            .--.\r\n\\\\          //\\\\ \\\r\n.\\\\        ///_\\\\\\\\\r\n:/&gt;`      /(| `|'\\\\\\\r\n Y/\\      )))\\_-_/((\\\r\n  \\ \\    ./'_/ \" \\_`\\)\r\n   \\ \\.-\" ._ \\   /   \\\r\n    \\ _.-\" (_ \\Y/ _) |\r\n     \"      )\" | \"\"/||\r\n         .-'  .'  / ||\r\n        /    `   /  ||\r\n       |    __  :   ||_\r\n       |   / \\   \\ '|\\`\r\n       |  |   \\   \\\r\n       |  |    `.  \\\r\n       |  |      \\  \\\r\n       |  |       \\  \\\r\n       |  |        \\  \\\r\n       |  |         \\  \\\r\n       /__\\          |__\\\r\n       /.|    DrS.    |.\\_\r\n      `-''            ``--'\r\n</pre>\n<p>Five of the best-known AI assistants—OpenAI’s GPT-3.5 and GPT-4, Google’s Gemini, Anthropic’s Claude, and Meta’s Llama—are trained to refuse to provide responses that could cause harm to the user or others or further a crime or unethical behavior. Prompting any of them, for example, to explain how to make and circulate counterfeit currency is a no-go. So are instructions on hacking an Internet of Things device, such as a surveillance camera or Internet router.</p></div><p><a href=\"https://arstechnica.com/?p=2010646#p3\">Read 11 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=2010646&amp;comments=1\">Comments</a></p>","author":"Dan Goodin","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"bf722cb730fcb7c053ea959fadca47b45f6c8abec0fbaee3414a0c4bdc499957","category":"Tech"}