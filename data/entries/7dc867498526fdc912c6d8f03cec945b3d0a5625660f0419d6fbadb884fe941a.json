{"title":"Llama 4 models from Meta now available in Amazon Bedrock serverless","link":"https://aws.amazon.com/blogs/aws/llama-4-models-from-meta-now-available-in-amazon-bedrock-serverless/","date":1745888964000,"content":"<p>The newest AI models from Meta, <a href=\"https://aws.amazon.com/bedrock/llama/?trk=e61dee65-4ce8-4738-84db-75305c9cd4fe&amp;sc_channel=el\">Llama 4 Scout 17B and Llama 4 Maverick 17B</a>, are now available as a fully managed, serverless option in <a href=\"https://aws.amazon.com/bedrock/\">Amazon Bedrock</a>. These new <a href=\"https://aws.amazon.com/what-is/foundation-models/?trk=e61dee65-4ce8-4738-84db-75305c9cd4fe&amp;sc_channel=el\">foundation models (FMs)</a> deliver natively multimodal capabilities with early fusion technology that you can use for precise image grounding and extended context processing in your applications.</p> \n<p>Llama 4 uses an innovative mixture-of-experts (MoE) architecture that provides enhanced performance across reasoning and image understanding tasks while optimizing for both cost and speed. This architectural approach enables Llama 4 to offer improved performance at lower cost compared to Llama 3, with expanded language support for global applications.</p> \n<p>The models were already <a href=\"https://aws.amazon.com/blogs/machine-learning/llama-4-family-of-models-from-meta-are-now-available-in-sagemaker-jumpstart/?trk=e61dee65-4ce8-4738-84db-75305c9cd4fe&amp;sc_channel=el\">available on Amazon SageMaker JumpStart</a>, and you can now use them in Amazon Bedrock to streamline building and scaling <a href=\"https://aws.amazon.com/ai/generative-ai/?trk=e61dee65-4ce8-4738-84db-75305c9cd4fe&amp;sc_channel=el\">generative AI</a> applications with <a href=\"https://aws.amazon.com/bedrock/security-compliance/?trk=e61dee65-4ce8-4738-84db-75305c9cd4fe&amp;sc_channel=el\">enterprise-grade security and privacy</a>.</p> \n<p><strong>Llama 4 Maverick 17B</strong> – A natively multimodal model featuring 128 experts and 400 billion total parameters. It excels in image and text understanding, making it suitable for versatile assistant and chat applications. The model supports a 1 million token context window, giving you the flexibility to process lengthy documents and complex inputs.</p> \n<p><strong>Llama 4 Scout 17B</strong> – A general-purpose multimodal model with 16 experts, 17 billion active parameters, and 109 billion total parameters that delivers superior performance compared to all previous Llama models. Amazon Bedrock currently supports a 3.5 million token context window for Llama 4 Scout, with plans to expand in the near future.</p> \n<p><span><strong>Use cases for Llama 4 models</strong></span><br /> You can use the advanced capabilities of Llama 4 models for a wide range of use cases across industries:</p> \n<p><strong>Enterprise applications</strong> – Build intelligent agents that can reason across tools and workflows, process multimodal inputs, and deliver high-quality responses for business applications.</p> \n<p><strong>Multilingual assistants</strong> – Create chat applications that understand images and provide high-quality responses across multiple languages, making them accessible to global audiences.</p> \n<p><strong>Code and document intelligence</strong> – Develop applications that can understand code, extract structured data from documents, and provide insightful analysis across large volumes of text and code.</p> \n<p><strong>Customer support</strong> – Enhance support systems with image analysis capabilities, enabling more effective problem resolution when customers share screenshots or photos.</p> \n<p><strong>Content creation</strong> – Generate creative content across multiple languages, with the ability to understand and respond to visual inputs.</p> \n<p><strong>Research</strong> – Build research applications that can integrate and analyze multimodal data, providing insights across text and images.</p> \n<p><span><strong>Using Llama 4 models in Amazon Bedrock<br /> </strong></span>To use these new serverless models in Amazon Bedrock, I first need to request access. In the <a href=\"https://console.aws.amazon.com/bedrock?trk=e61dee65-4ce8-4738-84db-75305c9cd4fe&amp;sc_channel=el\">Amazon Bedrock console</a>, I choose <strong>Model access</strong> from the navigation pane to toggle access to <strong>Llama 4 Maverick 17B</strong> and <strong>Llama 4 Scout 17B</strong> models.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2025/04/23/bedrock-llama4-model-access.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2025/04/23/bedrock-llama4-model-access.png\" alt=\"Console screenshot.\" width=\"2126\" height=\"726\" /></a></p> \n<p>The Llama 4 models can be easily integrated into your applications using the <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html?trk=e61dee65-4ce8-4738-84db-75305c9cd4fe&amp;sc_channel=el\">Amazon Bedrock Converse API</a>, which provides a unified interface for conversational AI interactions.</p> \n<p>Here’s an example of how to use the <a href=\"https://aws.amazon.com/sdk-for-python/\">AWS SDK for Python (Boto3)</a> with Llama 4 Maverick for a multimodal conversation:</p> \n<pre><code>import boto3\nimport json\nimport os\n\nAWS_REGION = \"us-west-2\"\nMODEL_ID = \"us.meta.llama4-maverick-17b-instruct-v1:0\"\nIMAGE_PATH = \"image.jpg\"\n\n\ndef get_file_extension(filename: str) -&gt; str:\n    \"\"\"Get the file extension.\"\"\"\n    extension = os.path.splitext(filename)[1].lower()[1:] or 'txt'\n    if extension == 'jpg':\n        extension = 'jpeg'\n    return extension\n\n\ndef read_file(file_path: str) -&gt; bytes:\n    \"\"\"Read a file in binary mode.\"\"\"\n    try:\n        with open(file_path, 'rb') as file:\n            return file.read()\n    except Exception as e:\n        raise Exception(f\"Error reading file {file_path}: {str(e)}\")\n\nbedrock_runtime = boto3.client(\n    service_name=\"bedrock-runtime\",\n    region_name=AWS_REGION\n)\n\nrequest_body = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"text\": \"What can you tell me about this image?\"\n                },\n                {\n                    \"image\": {\n                        \"format\": get_file_extension(IMAGE_PATH),\n                        \"source\": {\"bytes\": read_file(IMAGE_PATH)},\n                    }\n                },\n            ],\n        }\n    ]\n}\n\nresponse = bedrock_runtime.converse(\n    modelId=MODEL_ID,\n    messages=request_body[\"messages\"]\n)\n\nprint(response[\"output\"][\"message\"][\"content\"][-1][\"text\"])</code></pre> \n<p>This example demonstrates how to send both text and image inputs to the model and receive a conversational response. The Converse API abstracts away the complexity of working with different model input formats, providing a consistent interface across models in Amazon Bedrock.</p> \n<p>For more interactive use cases, you can also use the streaming capabilities of the Converse API:</p> \n<pre><code>response_stream = bedrock_runtime.converse_stream(\n    modelId=MODEL_ID,\n    messages=request_body['messages']\n)\n\nstream = response_stream.get('stream')\nif stream:\n    for event in stream:\n\n        if 'messageStart' in event:\n            print(f\"\\nRole: {event['messageStart']['role']}\")\n\n        if 'contentBlockDelta' in event:\n            print(event['contentBlockDelta']['delta']['text'], end=\"\")\n\n        if 'messageStop' in event:\n            print(f\"\\nStop reason: {event['messageStop']['stopReason']}\")\n\n        if 'metadata' in event:\n            metadata = event['metadata']\n            if 'usage' in metadata:\n                print(f\"Usage: {json.dumps(metadata['usage'], indent=4)}\")\n            if 'metrics' in metadata:\n                print(f\"Metrics: {json.dumps(metadata['metrics'], indent=4)}\")\n</code></pre> \n<p>With streaming, your applications can provide a more responsive experience by displaying model outputs as they are generated.</p> \n<p><span><strong>Things to know</strong></span><br /> The Llama 4 models are available today with a fully managed, serverless experience in <a href=\"https://aws.amazon.com/bedrock/\">Amazon Bedrock</a> in the US East (N. Virginia) and US West (Oregon) <a href=\"https://aws.amazon.com/about-aws/global-infrastructure/regions_az/?trk=e61dee65-4ce8-4738-84db-75305c9cd4fe&amp;sc_channel=el\">AWS Regions.</a> You can also access Llama 4 in US East (Ohio) via <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/cross-region-inference.html?trk=e61dee65-4ce8-4738-84db-75305c9cd4fe&amp;sc_channel=el\">cross-region inference</a>.</p> \n<p>As usual with Amazon Bedrock, you pay for what you use. For more information, see <a href=\"https://aws.amazon.com/bedrock/pricing/?trk=e61dee65-4ce8-4738-84db-75305c9cd4fe&amp;sc_channel=el\">Amazon Bedrock pricing</a>.</p> \n<p>These models support 12 languages for text (English, French, German, Hindi, Italian, Portuguese, Spanish, Thai, Arabic, Indonesian, Tagalog, and Vietnamese) and English when processing images.</p> \n<p>To start using these new models today, visit the <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-meta.html?trk=e61dee65-4ce8-4738-84db-75305c9cd4fe&amp;sc_channel=el\">Meta Llama models section in the Amazon Bedrock User Guide</a>. You can also explore how our Builder communities are using Amazon Bedrock in their solutions in the generative AI section of our <a href=\"https://community.aws/?trk=e61dee65-4ce8-4738-84db-75305c9cd4fe&amp;sc_channel=el\">community.aws</a> site.</p> \n<p>— <a href=\"https://x.com/danilop\">Danilo</a></p> \n<hr /> \n<p>How is the News Blog doing? Take this <a href=\"https://amazonmr.au1.qualtrics.com/jfe/form/SV_eyD5tC5xNGCdCmi\">1 minute survey</a>!</p> \n<p><em>(This <a href=\"https://amazonmr.au1.qualtrics.com/jfe/form/SV_eyD5tC5xNGCdCmi\">survey</a> is hosted by an external company. AWS handles your information as described in the <a href=\"https://aws.amazon.com/privacy/?trk=4b29643c-e00f-4ab6-ab9c-b1fb47aa1708&amp;sc_channel=blog\">AWS Privacy Notice</a>. AWS will own the data gathered via this survey and will not share the information collected with survey respondents.)</em></p>","author":"Danilo Poccia","siteTitle":"AWS News Blog","siteHash":"6093e072e4117ec22616e844cb857d03ca62c57a411a8affc77cb5e8b6b15bf6","entryHash":"7dc867498526fdc912c6d8f03cec945b3d0a5625660f0419d6fbadb884fe941a","category":"Tech"}