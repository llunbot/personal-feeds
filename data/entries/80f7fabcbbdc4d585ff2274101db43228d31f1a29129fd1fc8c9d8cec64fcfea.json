{"title":"The movement to hold AI accountable gains more steam","link":"https://arstechnica.com/?p=1818000","date":1638709822000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2021/12/business-papers-800x534.jpg\" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2021/12/business-papers.jpg\">Enlarge</a> (credit: MirageC | Getty Images)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>Algorithms play a growing role in our lives, even as their flaws are becoming more apparent: a Michigan man wrongly <a href=\"https://www.freep.com/story/news/local/michigan/2019/12/22/government-artificial-intelligence-midas-computer-fraud-fiasco/4407901002/\">accused</a> of fraud had to file for bankruptcy; automated screening tools disproportionately harm people of color who want to <a href=\"https://apnews.com/article/lifestyle-technology-business-race-and-ethnicity-racial-injustice-b920d945a6a13db1e1aee44d91475205\">buy a home</a> or <a href=\"https://themarkup.org/locked-out/2021/03/12/citing-a-markup-investigation-senators-question-regulators-about-tenant-screening-oversight\">rent an apartment</a>; Black Facebook users were <a href=\"https://www.washingtonpost.com/technology/2021/11/21/facebook-algorithm-biased-race/\">subjected to more abuse</a> than white users. Other automated systems have improperly rated teachers, graded students, and <a href=\"https://venturebeat.com/2020/09/29/examsofts-remote-bar-exam-sparks-privacy-and-facial-recognition-concerns/\">flagged people with dark skin</a> more often for cheating on tests.</p>\n<p>Now, efforts are underway to better understand how AI works and hold users accountable. New York’s City Council last month adopted a <a href=\"https://legistar.council.nyc.gov/LegislationDetail.aspx?ID=4344524&amp;GUID=B051915D-A9AC-451E-81F8-6596032FA3F9&amp;Options=ID\">law</a> requiring audits of <a href=\"https://www.wired.com/tag/algorithms/\">algorithms</a> used by employers in hiring or promotion. The law, the first of its kind in the nation, requires employers to bring in outsiders to assess whether an algorithm exhibits bias based on sex, race, or ethnicity. Employers also must tell job applicants who live in New York when <a href=\"https://www.wired.com/tag/artificial-intelligence/\">artificial intelligence</a> plays a role in deciding who gets hired or promoted.</p>\n<p>In Washington, DC, members of Congress are drafting a bill that would require businesses to evaluate automated decision-making systems used in areas such as health care, housing, employment, or education, and report the findings to the Federal Trade Commission; three of the FTC’s five members support stronger regulation of algorithms. An <a href=\"https://www.wired.com/story/opinion-bill-of-rights-artificial-intelligence/\">AI Bill of Rights</a> proposed last month by the White House calls for disclosing when AI makes decisions that impact a person’s civil rights, and it says AI systems should be “carefully audited” for accuracy and bias, among other things.</p></div><p><a href=\"https://arstechnica.com/?p=1818000#p3\">Read 27 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=1818000&amp;comments=1\">Comments</a></p>","author":"WIRED","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"80f7fabcbbdc4d585ff2274101db43228d31f1a29129fd1fc8c9d8cec64fcfea","category":"Tech"}