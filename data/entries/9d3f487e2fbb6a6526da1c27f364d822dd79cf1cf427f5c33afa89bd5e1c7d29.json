{"title":"Get insights from multimodal content with Amazon Bedrock Data Automation, now generally available","link":"https://aws.amazon.com/blogs/aws/get-insights-from-multimodal-content-with-amazon-bedrock-data-automation-now-generally-available/","date":1741025920000,"content":"<p>Many applications need to interact with content available through different modalities. Some of these applications process complex documents, such as insurance claims and medical bills. Mobile apps need to analyze user-generated media. Organizations need to build a semantic index on top of their digital assets that include documents, images, audio, and video files. However, getting insights from unstructured multimodal content is not easy to set up: you have to implement processing pipelines for the different data formats and go through multiple steps to get the information you need. That usually means having multiple models in production for which you have to handle cost optimizations (through fine-tuning and prompt engineering), safeguards (for example, against hallucinations), integrations with the target applications (including data formats), and model updates.</p> \n<p>To make this process easier, we <a href=\"https://aws.amazon.com/blogs/aws/new-amazon-bedrock-capabilities-enhance-data-processing-and-retrieval/\">introduced in preview during AWS re:Invent</a> <a href=\"https://aws.amazon.com/bedrock/bda/\">Amazon Bedrock Data Automation</a>, a capability of <a href=\"https://aws.amazon.com/bedrock/\">Amazon Bedrock</a> that streamlines the generation of valuable insights from unstructured, multimodal content such as documents, images, audio, and videos. With Bedrock Data Automation, you can reduce the development time and effort to build intelligent document processing, media analysis, and other multimodal data-centric automation solutions.</p> \n<p>You can use Bedrock Data Automation as a standalone feature or as a parser for <a href=\"https://aws.amazon.com/bedrock/knowledge-bases/\">Amazon Bedrock Knowledge Bases</a> to index insights from multimodal content and provide more relevant responses for <a href=\"https://aws.amazon.com/what-is/retrieval-augmented-generation/\">Retrieval-Augmented Generation (RAG)</a>.</p> \n<p>Today, Bedrock Data Automation is now generally available with support for cross-region inference endpoints to be available in more <a href=\"https://aws.amazon.com/about-aws/global-infrastructure/regions_az/\">AWS Regions</a> and seamlessly use compute across different locations. Based on your feedback during the preview, we also improved accuracy and added support for logo recognition for images and videos.</p> \n<p>Let’s have a look at how this works in practice.</p> \n<p><span><strong>Using Amazon Bedrock Data Automation with cross-region inference endpoints<br /> </strong></span>The <a href=\"https://aws.amazon.com/blogs/aws/new-amazon-bedrock-capabilities-enhance-data-processing-and-retrieval/\">blog post published for the Bedrock Data Automation preview</a> shows how to use the visual demo in the <a href=\"https://console.aws.amazon.com/bedrock\">Amazon Bedrock console</a> to extract information from documents and videos. I recommend you go through the console demo experience to understand how this capability works and what you can do to customize it. For this post, I focus more on how Bedrock Data Automation works in your applications, starting with a few steps in the console and following with code samples.</p> \n<p>The <strong>Data Automation</strong> section of the <a href=\"https://console.aws.amazon.com/bedrock\">Amazon Bedrock console</a> now asks for confirmation to enable cross-region support the first time you access it. For example:</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2025/02/18/bda-ga-cross-region-confirm.png\"><img src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2025/02/18/bda-ga-cross-region-confirm.png\" alt=\"Console screenshot.\" width=\"936\" height=\"258\" /></a></p> \n<p>From an API perspective, the <code>InvokeDataAutomationAsync</code> operation now <strong>requires</strong> an additional parameter (<code>dataAutomationProfileArn</code>) to specify the data automation profile to use. The value for this parameter depends on the Region and your AWS account ID:</p> \n<p><code>arn:aws:bedrock:&lt;REGION&gt;:&lt;ACCOUNT_ID&gt;:data-automation-profile/us.data-automation-v1</code></p> \n<p>Also, the <code>dataAutomationArn</code> parameter has been renamed to <code>dataAutomationProjectArn</code> to better reflect that it contains the project <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference-arns.html\">Amazon Resource Name (ARN)</a>. When invoking Bedrock Data Automation, you now need to specify a project or a blueprint to use. If you pass in blueprints, you will get custom output. To continue to get standard default output, configure the parameter <code>DataAutomationProjectArn</code> to use <code>arn:aws:bedrock:&lt;REGION&gt;:aws:data-automation-project/public-default</code>.</p> \n<p>As the name suggests, the <code>InvokeDataAutomationAsync</code> operation is asynchronous. You pass the input and output configuration and, when the result is ready, it’s written on an <a href=\"https://aws.amazon.com/s3/\">Amazon Simple Storage Service (Amazon S3)</a> bucket as specified in the output configuration. You can receive an <a href=\"https://aws.amazon.com/eventbridge\">Amazon EventBridge</a> notification from Bedrock Data Automation using the <code>notificationConfiguration</code> parameter.</p> \n<p>With Bedrock Data Automation, you can configure outputs in two ways:</p> \n<ul> \n <li><strong>Standard output</strong> delivers predefined insights relevant to a data type, such as document semantics, video chapter summaries, and audio transcripts. With standard outputs, you can set up your desired insights in just a few steps.</li> \n <li><strong>Custom output</strong> lets you specify extraction needs using blueprints for more tailored insights.</li> \n</ul> \n<p>To see the new capabilities in action, I create a project and customize the standard output settings. For documents, I choose plain text instead of markdown. Note that you can automate these configuration steps using the Bedrock Data Automation API.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2025/02/19/bda-ga-standard-output-document.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2025/02/19/bda-ga-standard-output-document.png\" alt=\"Console screenshot.\" width=\"1352\" height=\"601\" /></a></p> \n<p>For videos, I want a full audio transcript and a summary of the entire video. I also ask for a summary of each chapter.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2025/02/25/bda-ga-standard-output-video-1.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2025/02/25/bda-ga-standard-output-video-1.png\" alt=\"Console screenshot.\" width=\"1249\" height=\"487\" /></a></p> \n<p>To configure a blueprint, I choose <strong>Custom output setup</strong> in the <strong>Data automation</strong> section of the Amazon Bedrock console navigation pane. There, I search for the <strong>US-Driver-License</strong> sample blueprint. You can browse other sample blueprints for more examples and ideas.</p> \n<p>Sample blueprints can’t be edited, so I use the <strong>Actions</strong> menu to duplicate the blueprint and add it to my project. There, I can fine-tune the data to be extracted by modifying the blueprint and adding custom fields that can use <a href=\"https://aws.amazon.com/ai/generative-ai/\">generative AI</a> to extract or compute data in the format I need.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2025/02/18/bda-ga-cross-sample-blueprint.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2025/02/18/bda-ga-cross-sample-blueprint.png\" alt=\"Console screenshot.\" width=\"1246\" height=\"1086\" /></a></p> \n<p>I upload the image of a US driver’s license on an S3 bucket. Then, I use this sample Python script that uses Bedrock Data Automation through the <a href=\"https://aws.amazon.com/sdk-for-python/\">AWS SDK for Python (Boto3)</a> to extract text information from the image:</p> \n<pre><code>import json\nimport sys\nimport time\n\nimport boto3\n\nDEBUG = False\n\nAWS_REGION = '&lt;REGION&gt;'\nBUCKET_NAME = '&lt;BUCKET&gt;'\nINPUT_PATH = 'BDA/Input'\nOUTPUT_PATH = 'BDA/Output'\n\nPROJECT_ID = '&lt;PROJECT_ID&gt;'\nBLUEPRINT_NAME = 'US-Driver-License-demo'\n\n# Fields to display\nBLUEPRINT_FIELDS = [\n    'NAME_DETAILS/FIRST_NAME',\n    'NAME_DETAILS/MIDDLE_NAME',\n    'NAME_DETAILS/LAST_NAME',\n    'DATE_OF_BIRTH',\n    'DATE_OF_ISSUE',\n    'EXPIRATION_DATE'\n]\n\n# AWS SDK for Python (Boto3) clients\nbda = boto3.client('bedrock-data-automation-runtime', region_name=AWS_REGION)\ns3 = boto3.client('s3', region_name=AWS_REGION)\nsts = boto3.client('sts')\n\n\ndef log(data):\n    if DEBUG:\n        if type(data) is dict:\n            text = json.dumps(data, indent=4)\n        else:\n            text = str(data)\n        print(text)\n\ndef get_aws_account_id() -&gt; str:\n    return sts.get_caller_identity().get('Account')\n\n\ndef get_json_object_from_s3_uri(s3_uri) -&gt; dict:\n    s3_uri_split = s3_uri.split('/')\n    bucket = s3_uri_split[2]\n    key = '/'.join(s3_uri_split[3:])\n    object_content = s3.get_object(Bucket=bucket, Key=key)['Body'].read()\n    return json.loads(object_content)\n\n\ndef invoke_data_automation(input_s3_uri, output_s3_uri, data_automation_arn, aws_account_id) -&gt; dict:\n    params = {\n        'inputConfiguration': {\n            's3Uri': input_s3_uri\n        },\n        'outputConfiguration': {\n            's3Uri': output_s3_uri\n        },\n        'dataAutomationConfiguration': {\n            'dataAutomationProjectArn': data_automation_arn\n        },\n        'dataAutomationProfileArn': f\"arn:aws:bedrock:{AWS_REGION}:{aws_account_id}:data-automation-profile/us.data-automation-v1\"\n    }\n\n    response = bda.invoke_data_automation_async(**params)\n    log(response)\n\n    return response\n\ndef wait_for_data_automation_to_complete(invocation_arn, loop_time_in_seconds=1) -&gt; dict:\n    while True:\n        response = bda.get_data_automation_status(\n            invocationArn=invocation_arn\n        )\n        status = response['status']\n        if status not in ['Created', 'InProgress']:\n            print(f\" {status}\")\n            return response\n        print(\".\", end='', flush=True)\n        time.sleep(loop_time_in_seconds)\n\n\ndef print_document_results(standard_output_result):\n    print(f\"Number of pages: {standard_output_result['metadata']['number_of_pages']}\")\n    for page in standard_output_result['pages']:\n        print(f\"- Page {page['page_index']}\")\n        if 'text' in page['representation']:\n            print(f\"{page['representation']['text']}\")\n        if 'markdown' in page['representation']:\n            print(f\"{page['representation']['markdown']}\")\n\n\ndef print_video_results(standard_output_result):\n    print(f\"Duration: {standard_output_result['metadata']['duration_millis']} ms\")\n    print(f\"Summary: {standard_output_result['video']['summary']}\")\n    statistics = standard_output_result['statistics']\n    print(\"Statistics:\")\n    print(f\"- Speaket count: {statistics['speaker_count']}\")\n    print(f\"- Chapter count: {statistics['chapter_count']}\")\n    print(f\"- Shot count: {statistics['shot_count']}\")\n    for chapter in standard_output_result['chapters']:\n        print(f\"Chapter {chapter['chapter_index']} {chapter['start_timecode_smpte']}-{chapter['end_timecode_smpte']} ({chapter['duration_millis']} ms)\")\n        if 'summary' in chapter:\n            print(f\"- Chapter summary: {chapter['summary']}\")\n\n\ndef print_custom_results(custom_output_result):\n    matched_blueprint_name = custom_output_result['matched_blueprint']['name']\n    log(custom_output_result)\n    print('\\n- Custom output')\n    print(f\"Matched blueprint: {matched_blueprint_name}  Confidence: {custom_output_result['matched_blueprint']['confidence']}\")\n    print(f\"Document class: {custom_output_result['document_class']['type']}\")\n    if matched_blueprint_name == BLUEPRINT_NAME:\n        print('\\n- Fields')\n        for field_with_group in BLUEPRINT_FIELDS:\n            print_field(field_with_group, custom_output_result)\n\n\ndef print_results(job_metadata_s3_uri) -&gt; None:\n    job_metadata = get_json_object_from_s3_uri(job_metadata_s3_uri)\n    log(job_metadata)\n\n    for segment in job_metadata['output_metadata']:\n        asset_id = segment['asset_id']\n        print(f'\\nAsset ID: {asset_id}')\n\n        for segment_metadata in segment['segment_metadata']:\n            # Standard output\n            standard_output_path = segment_metadata['standard_output_path']\n            standard_output_result = get_json_object_from_s3_uri(standard_output_path)\n            log(standard_output_result)\n            print('\\n- Standard output')\n            semantic_modality = standard_output_result['metadata']['semantic_modality']\n            print(f\"Semantic modality: {semantic_modality}\")\n            match semantic_modality:\n                case 'DOCUMENT':\n                    print_document_results(standard_output_result)\n                case 'VIDEO':\n                    print_video_results(standard_output_result)\n            # Custom output\n            if 'custom_output_status' in segment_metadata and segment_metadata['custom_output_status'] == 'MATCH':\n                custom_output_path = segment_metadata['custom_output_path']\n                custom_output_result = get_json_object_from_s3_uri(custom_output_path)\n                print_custom_results(custom_output_result)\n\n\ndef print_field(field_with_group, custom_output_result) -&gt; None:\n    inference_result = custom_output_result['inference_result']\n    explainability_info = custom_output_result['explainability_info'][0]\n    if '/' in field_with_group:\n        # For fields part of a group\n        (group, field) = field_with_group.split('/')\n        inference_result = inference_result[group]\n        explainability_info = explainability_info[group]\n    else:\n        field = field_with_group\n    value = inference_result[field]\n    confidence = explainability_info[field]['confidence']\n    print(f'{field}: {value or '&lt;EMPTY&gt;'}  Confidence: {confidence}')\n\n\ndef main() -&gt; None:\n    if len(sys.argv) &lt; 2:\n        print(\"Please provide a filename as command line argument\")\n        sys.exit(1)\n      \n    file_name = sys.argv[1]\n    \n    aws_account_id = get_aws_account_id()\n    input_s3_uri = f\"s3://{BUCKET_NAME}/{INPUT_PATH}/{file_name}\" # File\n    output_s3_uri = f\"s3://{BUCKET_NAME}/{OUTPUT_PATH}\" # Folder\n    data_automation_arn = f\"arn:aws:bedrock:{AWS_REGION}:{aws_account_id}:data-automation-project/{PROJECT_ID}\"\n\n    print(f\"Invoking Bedrock Data Automation for '{file_name}'\", end='', flush=True)\n\n    data_automation_response = invoke_data_automation(input_s3_uri, output_s3_uri, data_automation_arn, aws_account_id)\n    data_automation_status = wait_for_data_automation_to_complete(data_automation_response['invocationArn'])\n\n    if data_automation_status['status'] == 'Success':\n        job_metadata_s3_uri = data_automation_status['outputConfiguration']['s3Uri']\n        print_results(job_metadata_s3_uri)\n\n\nif __name__ == \"__main__\":\n    main()</code></pre> \n<p>The initial configuration in the script includes the name of the S3 bucket to use in input and output, the location of the input file in the bucket, the output path for the results, the project ID to use to get custom output from Bedrock Data Automation, and the blueprint fields to show in output.</p> \n<p>I run the script passing the name of the input file. In output, I see the information extracted by Bedrock Data Automation. The <strong>US-Driver-License</strong> is a match and the name and dates in the driver’s license are printed in output.</p> \n<div> \n <pre><code>python bda-ga.py bda-drivers-license.jpeg\n\nInvoking Bedrock Data Automation for 'bda-drivers-license.jpeg'................ Success\n\nAsset ID: 0\n\n- Standard output\nSemantic modality: DOCUMENT\nNumber of pages: 1\n- Page 0\nNEW JERSEY\n\nMotor Vehicle\n Commission\n\nAUTO DRIVER LICENSE\n\nCould DL M6454 64774 51685                      CLASS D\n        DOB 01-01-1968\nISS 03-19-2019          EXP     01-01-2023\n        MONTOYA RENEE MARIA 321 GOTHAM AVENUE TRENTON, NJ 08666 OF\n        END NONE\n        RESTR NONE\n        SEX F HGT 5'-08\" EYES HZL               ORGAN DONOR\n        CM ST201907800000019 CHG                11.00\n\n[SIGNATURE]\n\n\n\n- Custom output\nMatched blueprint: US-Driver-License-copy  Confidence: 1\nDocument class: US-drivers-licenses\n\n- Fields\nFIRST_NAME: RENEE  Confidence: 0.859375\nMIDDLE_NAME: MARIA  Confidence: 0.83203125\nLAST_NAME: MONTOYA  Confidence: 0.875\nDATE_OF_BIRTH: 1968-01-01  Confidence: 0.890625\nDATE_OF_ISSUE: 2019-03-19  Confidence: 0.79296875\nEXPIRATION_DATE: 2023-01-01  Confidence: 0.93359375</code></pre> \n</div> \n<p>As expected, I see in output the information I selected from the blueprint associated with the Bedrock Data Automation project.</p> \n<p>Similarly, I run the same script on a <a href=\"https://www.youtube.com/watch?v=WYTZELB8JdU\">video file</a> from my colleague <a href=\"https://www.linkedin.com/in/mikegchambers\">Mike Chambers</a>. To keep the output small, I don’t print the full audio transcript or the text displayed in the video.</p> \n<div> \n <pre><code>python bda.py mike-video.mp4\nInvoking Bedrock Data Automation for 'mike-video.mp4'.......................................................................................................................................................................................................................................................................... Success\n\nAsset ID: 0\n\n- Standard output\nSemantic modality: VIDEO\nDuration: 810476 ms\nSummary: In this comprehensive demonstration, a technical expert explores the capabilities and limitations of Large Language Models (LLMs) while showcasing a practical application using AWS services. He begins by addressing a common misconception about LLMs, explaining that while they possess general world knowledge from their training data, they lack current, real-time information unless connected to external data sources.\n\nTo illustrate this concept, he demonstrates an \"Outfit Planner\" application that provides clothing recommendations based on location and weather conditions. Using Brisbane, Australia as an example, the application combines LLM capabilities with real-time weather data to suggest appropriate attire like lightweight linen shirts, shorts, and hats for the tropical climate.\n\nThe demonstration then shifts to the Amazon Bedrock platform, which enables users to build and scale generative AI applications using foundation models. The speaker showcases the \"OutfitAssistantAgent,\" explaining how it accesses real-time weather data to make informed clothing recommendations. Through the platform's \"Show Trace\" feature, he reveals the agent's decision-making process and how it retrieves and processes location and weather information.\n\nThe technical implementation details are explored as the speaker configures the OutfitAssistant using Amazon Bedrock. The agent's workflow is designed to be fully serverless and managed within the Amazon Bedrock service.\n\nFurther diving into the technical aspects, the presentation covers the AWS Lambda console integration, showing how to create action group functions that connect to external services like the OpenWeatherMap API. The speaker emphasizes that LLMs become truly useful when connected to tools providing relevant data sources, whether databases, text files, or external APIs.\n\nThe presentation concludes with the speaker encouraging viewers to explore more AWS developer content and engage with the channel through likes and subscriptions, reinforcing the practical value of combining LLMs with external data sources for creating powerful, context-aware applications.\nStatistics:\n- Speaket count: 1\n- Chapter count: 6\n- Shot count: 48\nChapter 0 00:00:00:00-00:01:32:01 (92025 ms)\n- Chapter summary: A man with a beard and glasses, wearing a gray hooded sweatshirt with various logos and text, is sitting at a desk in front of a colorful background. He discusses the frequent release of new large language models (LLMs) and how people often test these models by asking questions like \"Who won the World Series?\" The man explains that LLMs are trained on general data from the internet, so they may have information about past events but not current ones. He then poses the question of what he wants from an LLM, stating that he desires general world knowledge, such as understanding basic concepts like \"up is up\" and \"down is down,\" but does not need specific factual knowledge. The man suggests that he can attach other systems to the LLM to access current factual data relevant to his needs. He emphasizes the importance of having general world knowledge and the ability to use tools and be linked into agentic workflows, which he refers to as \"agentic workflows.\" The man encourages the audience to add this term to their spell checkers, as it will likely become commonly used.\nChapter 1 00:01:32:01-00:03:38:18 (126560 ms)\n- Chapter summary: The video showcases a man with a beard and glasses demonstrating an \"Outfit Planner\" application on his laptop. The application allows users to input their location, such as Brisbane, Australia, and receive recommendations for appropriate outfits based on the weather conditions. The man explains that the application generates these recommendations using large language models, which can sometimes provide inaccurate or hallucinated information since they lack direct access to real-world data sources.\n\nThe man walks through the process of using the Outfit Planner, entering Brisbane as the location and receiving weather details like temperature, humidity, and cloud cover. He then shows how the application suggests outfit options, including a lightweight linen shirt, shorts, sandals, and a hat, along with an image of a woman wearing a similar outfit in a tropical setting.\n\nThroughout the demonstration, the man points out the limitations of current language models in providing accurate and up-to-date information without external data connections. He also highlights the need to edit prompts and adjust settings within the application to refine the output and improve the accuracy of the generated recommendations.\nChapter 2 00:03:38:18-00:07:19:06 (220620 ms)\n- Chapter summary: The video demonstrates the Amazon Bedrock platform, which allows users to build and scale generative AI applications using foundation models (FMs). [speaker_0] introduces the platform's overview, highlighting its key features like managing FMs from AWS, integrating with custom models, and providing access to leading AI startups. The video showcases the Amazon Bedrock console interface, where [speaker_0] navigates to the \"Agents\" section and selects the \"OutfitAssistantAgent\" agent. [speaker_0] tests the OutfitAssistantAgent by asking it for outfit recommendations in Brisbane, Australia. The agent provides a suggestion of wearing a light jacket or sweater due to cool, misty weather conditions. To verify the accuracy of the recommendation, [speaker_0] clicks on the \"Show Trace\" button, which reveals the agent's workflow and the steps it took to retrieve the current location details and weather information for Brisbane. The video explains that the agent uses an orchestration and knowledge base system to determine the appropriate response based on the user's query and the retrieved data. It highlights the agent's ability to access real-time information like location and weather data, which is crucial for generating accurate and relevant responses.\nChapter 3 00:07:19:06-00:11:26:13 (247214 ms)\n- Chapter summary: The video demonstrates the process of configuring an AI assistant agent called \"OutfitAssistant\" using Amazon Bedrock. [speaker_0] introduces the agent's purpose, which is to provide outfit recommendations based on the current time and weather conditions. The configuration interface allows selecting a language model from Anthropic, in this case the Claud 3 Haiku model, and defining natural language instructions for the agent's behavior. [speaker_0] explains that action groups are groups of tools or actions that will interact with the outside world. The OutfitAssistant agent uses Lambda functions as its tools, making it fully serverless and managed within the Amazon Bedrock service. [speaker_0] defines two action groups: \"get coordinates\" to retrieve latitude and longitude coordinates from a place name, and \"get current time\" to determine the current time based on the location. The \"get current weather\" action requires calling the \"get coordinates\" action first to obtain the location coordinates, then using those coordinates to retrieve the current weather information. This demonstrates the agent's workflow and how it utilizes the defined actions to generate outfit recommendations. Throughout the video, [speaker_0] provides details on the agent's configuration, including its name, description, model selection, instructions, and action groups. The interface displays various options and settings related to these aspects, allowing [speaker_0] to customize the agent's behavior and functionality.\nChapter 4 00:11:26:13-00:13:00:17 (94160 ms)\n- Chapter summary: The video showcases a presentation by [speaker_0] on the AWS Lambda console and its integration with machine learning models for building powerful agents. [speaker_0] demonstrates how to create an action group function using AWS Lambda, which can be used to generate text responses based on input parameters like location, time, and weather data. The Lambda function code is shown, utilizing external services like OpenWeatherMap API for fetching weather information. [speaker_0] explains that for a large language model to be useful, it needs to connect to tools providing relevant data sources, such as databases, text files, or external APIs. The presentation covers the process of defining actions, setting up Lambda functions, and leveraging various tools within the AWS environment to build intelligent agents capable of generating context-aware responses.\nChapter 5 00:13:00:17-00:13:28:10 (27761 ms)\n- Chapter summary: A man with a beard and glasses, wearing a gray hoodie with various logos and text, is sitting at a desk in front of a colorful background. He is using a laptop computer that has stickers and logos on it, including the AWS logo. The man appears to be presenting or speaking about AWS (Amazon Web Services) and its services, such as Lambda functions and large language models. He mentions that if a Lambda function can do something, then it can be used to augment a large language model. The man concludes by expressing hope that the viewer found the video useful and insightful, and encourages them to check out other videos on the AWS developers channel. He also asks viewers to like the video, subscribe to the channel, and watch other videos.</code></pre> \n</div> \n<p><span><strong>Things to know<br /> </strong></span><a href=\"https://aws.amazon.com/bedrock/bda/\">Amazon Bedrock Data Automation</a> is now available via cross-region inference in the following two AWS Regions: US East (N. Virginia) and US West (Oregon). When using Bedrock Data Automation from those Regions, data can be processed using cross-region inference in any of these four Regions: US East (Ohio, N. Virginia) and US West (N. California, Oregon). All these Regions are in the US so that data is processed within the same geography. We’re working to add support for more Regions in Europe and Asia later in 2025.</p> \n<p>There’s no change in pricing compared to the preview and when using cross-region inference. For more information, visit <a href=\"https://aws.amazon.com/bedrock/pricing/\">Amazon Bedrock pricing</a>.</p> \n<p>Bedrock Data Automation now also includes a number of security, governance and manageability related capabilities such as <a href=\"https://aws.amazon.com/kms/\">AWS Key Management Service (AWS KMS)</a> <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#customer-cmk\">customer managed keys</a> support for granular encryption control, <a href=\"https://aws.amazon.com/privatelink/\">AWS PrivateLink</a> to connect directly to the Bedrock Data Automation APIs in your virtual private cloud (VPC) instead of connecting over the internet, and tagging of Bedrock Data Automation resources and jobs to track costs and enforce tag-based access policies in <a href=\"https://aws.amazon.com/iam/\">AWS Identity and Access Management (IAM)</a>.</p> \n<p>I used Python in this blog post but Bedrock Data Automation is available with any <a href=\"https://aws.amazon.com/tools/\">AWS SDKs</a>. For example, you can use Java, .NET, or Rust for a backend document processing application; JavaScript for a web app that processes images, videos, or audio files; and Swift for a native mobile app that processes content provided by end users. It’s never been so easy to get insights from multimodal data.</p> \n<p>Here are a few reading suggestions to learn more (including code samples):</p> \n<ul> \n <li><a href=\"https://aws.amazon.com/blogs/machine-learning/simplify-multimodal-generative-ai-with-amazon-bedrock-data-automation/\">Simplify multimodal generative AI with Amazon Bedrock Data Automation</a></li> \n <li><a href=\"https://aws.amazon.com/solutions/guidance/multimodal-data-processing-using-amazon-bedrock-data-automation\">Guidance for Multimodal Data Processing Using Amazon Bedrock Data Automation</a></li> \n <li><a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/bda.html\">Amazon Bedrock Data Automation User Guide</a></li> \n</ul> \n<p>– <a href=\"https://twitter.com/danilop\">Danilo</a></p> \n<p>—</p> \n<p>How is the News Blog doing? Take this <a href=\"https://amazonmr.au1.qualtrics.com/jfe/form/SV_eyD5tC5xNGCdCmi\">1 minute survey</a>!</p> \n<p><em>(This survey is hosted by an external company. AWS handles your information as described in the AWS Privacy Notice. AWS will own the data gathered via this survey and will not share the information collected with survey respondents.)</em></p>","author":"Danilo Poccia","siteTitle":"AWS News Blog","siteHash":"6093e072e4117ec22616e844cb857d03ca62c57a411a8affc77cb5e8b6b15bf6","entryHash":"9d3f487e2fbb6a6526da1c27f364d822dd79cf1cf427f5c33afa89bd5e1c7d29","category":"Tech"}