{"title":"Here’s what’s really going on inside an LLM’s neural network","link":"https://arstechnica.com/?p=2026236","date":1716402685000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/05/deconstructing-ai-800x450.jpg\" alt=\"Here’s what’s really going on inside an LLM’s neural network\" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2024/05/deconstructing-ai.jpg\">Enlarge</a> (credit: Aurich Lawson | Getty Images)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>With most computer programs—even complex ones—you can meticulously trace through the code and memory usage to figure out <em>why</em> that program generates any specific behavior or output. That's generally not true in the field of generative AI, where the non-interpretable neural networks underlying these models make it hard for even experts to figure out precisely <a href=\"https://arstechnica.com/information-technology/2023/04/why-ai-chatbots-are-the-ultimate-bs-machines-and-how-people-hope-to-fix-them/\">why they often confabulate information</a>, for instance.</p>\n<p>Now, <a href=\"https://www.anthropic.com/research/mapping-mind-language-model\">new research from Anthropic</a> offers a new window into what's going on inside the Claude LLM's \"black box.\" The company's <a href=\"https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html\">new paper</a> on \"Extracting Interpretable Features from Claude 3 Sonnet\" describes a powerful new method for at least partially explaining just how the model's millions of artificial neurons fire to create <a href=\"https://arstechnica.com/information-technology/2024/03/the-ai-wars-heat-up-with-claude-3-claimed-to-have-near-human-abilities/\">surprisingly lifelike responses</a> to general queries.</p>\n<h2>Opening the hood</h2>\n<p>When analyzing an LLM, it's trivial to see which specific artificial neurons are activated in response to any particular query. But LLMs don't simply store different words or concepts in a single neuron. Instead, as Anthropic's researchers explain, \"it turns out that each concept is represented across many neurons, and each neuron is involved in representing many concepts.\"</p></div><p><a href=\"https://arstechnica.com/?p=2026236#p3\">Read 12 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=2026236&amp;comments=1\">Comments</a></p>","author":"Kyle Orland","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"b887493512db44b24ba183eabce9434a702e2a3d698907492e067d627016f59b","category":"Tech"}