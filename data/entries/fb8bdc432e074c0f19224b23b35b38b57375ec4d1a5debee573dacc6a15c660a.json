{"title":"สงครามการ์ดจอยุคใหม่ AMD เกทับ NVIDIA ตอบโต้ ใครรันโมเดล AI เร็วกว่ากัน","link":"https://www.blognone.com/node/137275","date":1702781476000,"content":"<div><div><div><p>สมัยก่อนเราอาจคุ้นเคยกับการต่อสู้ลักษณะนี้ในการวัดผลการ์ดจอเกมมิ่ง แต่เมื่อตลาดหลักย้ายมาสู่การ์ดจอเร่งความเร็ว AI แทน การต่อสู้แบบนี้จึงกลับมาอีกครั้งในสมรภูมิใหม่</p>\n<p>สัปดาห์ก่อน <a href=\"https://www.blognone.com/node/137119\">AMD เริ่มวางขายชิปประมวลผลปัญญาประดิษฐ์ Instinct MI300X</a> มีประเด็นน่าสนใจคือ Lisa Su ซีอีโอของ AMD โชว์ตัวเลขข่มทับคู่แข่ง NVIDIA H100 โดยวัดจากการรันโมเดลยอดนิยม Llama-2-70B ว่าแรงกว่ากัน 1.2 เท่า (จีพียูตัวเดียว) และ 1.4 เท่า (เซิร์ฟเวอร์ 8 จีพียู)</p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/20af249bfdea043ac1c1fa3e0de1122f.jpg\" /></p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/369b32e297312d917176871f9dd7cdbd.jpg\" /></p>\n<p>เมื่อโดนหยามกันตรงๆ แบบนี้ทำให้ NVIDIA ยอมไม่ได้ ออกมาตอบโต้ผ่าน<a href=\"https://developer.nvidia.com/blog/achieving-top-inference-performance-with-the-nvidia-h100-tensor-core-gpu-and-nvidia-tensorrt-llm/\">บล็อกของบริษัท</a> ว่าเป็นการวัดผลเบนช์มาร์คที่ไม่ถูกต้อง เพราะ AMD เลือกใช้ซอฟต์แวร์ที่ไม่ได้ปรับแต่งมาสำหรับจีพียู H100 (<a href=\"https://developer.nvidia.com/blog/optimizing-inference-on-llms-with-tensorrt-llm-now-publicly-available/\">NVIDIA เปิดเป็นโอเพนซอร์สในชื่อ TensorRT-LLM</a>) หากใช้ซอฟต์แวร์ถูกต้องและวัดผลแบบไม่ลำเอียง ฝั่งของ H100 ทำคะแนนได้ดีกว่า 2 เท่าเลยทีเดียว</p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/9bc4abd7d086bb9c08211f25536a2558.jpg\" /></p>\n<p>แต่ศึกนี้ไม่จบง่ายๆ เพราะ<a href=\"https://community.amd.com/t5/instinct-accelerators/competitive-performance-claims-and-industry-leading-inference/ba-p/652304\">ฝั่ง AMD ออกมาสู้กลับ</a> โดยบอกว่าการวัดผลของ NVIDIA นั้นตั้งใจบิดเบือนวิธีการวัดผล ตั้งแต่การใช้ซอฟต์แวร์ TensorRT-LLM ของค่ายตัวเองอย่างเดียว (เบนช์มาร์ค AMD ใช้ vLLM), เปรียบเทียบด้วยระดับเลขทศนิยมคนละแบบ (AMD ใช้ FP16, NVIDIA ใช้ FP8) และบิดตัวเลข latency/throughput ของ AMD ไปให้ตัวเองได้ประโยชน์</p>\n<p>AMD จึงบอกว่าอย่ากระนั้นเลย ตัวเลขที่โชว์บนเวทีเป็นการวัดผลตั้งแต่เดือนพฤศจิกายน และออกตัวเลขใหม่ที่ใช้ซอฟต์แวร์ ROCm เวอร์ชันใหม่ ปรับปรุงประสิทธิภาพให้ดีกว่าเดิม ผลคือ AMD ชนะขาดกว่าเดิมหากวัดผลแบบเดิมด้วย vLLM (แรงกว่า 2.1 เท่า) หรือถ้าจะวัดผลแบบ NVIDIA (TensorRT-LLM vs vLLM) ก็ยังชนะอยู่ดีที่ 1.3 เท่า และหากวัดแบบใช้หลักทศนิยมไม่เท่ากัน (FP8 vs FP16) ค่า latency ก็ยังชนะอีก รวมเป็นชนะ 3 เด้ง</p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/228905397807d95b94eebe1cc9c32ebd.jpg\" /></p>\n<p>ที่มา - <a href=\"https://wccftech.com/amd-responds-to-nvidia-h100-tensorrt-llm-results-shows-mi300x-gpu-leading-optimized-ai-software/\">Wccftech</a></p>\n</div></div></div><div><div>Topics: </div><div><div><a href=\"/topics/instinct\">Instinct</a></div><div><a href=\"/topics/amd\">AMD</a></div><div><a href=\"/topics/gpu\">GPU</a></div><div><a href=\"/topics/nvidia\">NVIDIA</a></div><div><a href=\"/topics/nvidia-hopper\">NVIDIA Hopper</a></div><div><a href=\"/topics/benchmark\">Benchmark</a></div><div><a href=\"/topics/artificial-intelligence\">Artificial Intelligence</a></div></div></div>","author":"mk","siteTitle":"Blognone","siteHash":"ededadcf18490b3937e7dd89ebe8c00dc129addbdf1ebe4aff1f458146693da0","entryHash":"fb8bdc432e074c0f19224b23b35b38b57375ec4d1a5debee573dacc6a15c660a","category":"Thai"}