{"title":"Thousands scammed by AI voices mimicking loved ones in emergencies","link":"https://arstechnica.com/?p=1922001","date":1678126672000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2023/03/GettyImages-1367728606-800x547.jpg\" alt=\"Thousands scammed by AI voices mimicking loved ones in emergencies\" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2023/03/GettyImages-1367728606.jpg\">Enlarge</a> (credit: <a href=\"https://www.gettyimages.com/detail/illustration/conversational-ai-concept-natural-language-royalty-free-illustration/1367728606?phrase=artificial%20intelligence%20voice\">ArtemisDiana | iStock / Getty Images Plus</a>)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>AI models designed to closely simulate a person’s voice are making it easier for bad actors to mimic loved ones and scam vulnerable people out of thousands of dollars, <a href=\"https://www.washingtonpost.com/technology/2023/03/05/ai-voice-scam/\">The Washington Post reported</a>.</p>\n<p>Quickly evolving in sophistication, some AI voice-generating software requires just a few sentences <a href=\"https://arstechnica.com/information-technology/2023/01/microsofts-new-ai-can-simulate-anyones-voice-with-3-seconds-of-audio/\">of audio</a> to convincingly produce speech that <a href=\"https://arstechnica.com/information-technology/2022/09/with-koe-recast-you-can-change-your-voice-as-easily-as-your-clothing/\">conveys the sound and emotional tone of a speaker’s voice</a>, while other options need <a href=\"https://arstechnica.com/information-technology/2023/01/microsofts-new-ai-can-simulate-anyones-voice-with-3-seconds-of-audio/\">as little as three seconds</a>. For those targeted—which is often the elderly, the Post reported—it can be increasingly difficult to detect when a voice is inauthentic, even when the emergency circumstances described by scammers seem implausible.</p>\n<p>Tech advancements seemingly make it easier to prey on people’s worst fears and spook victims who told the Post they felt “visceral horror” hearing what sounded like direct pleas from friends or family members in dire need of help. One couple sent $15,000 through a bitcoin terminal to a scammer after believing they had spoken to their son. The AI-generated voice told them that he needed legal fees after being involved in a car accident that killed a US diplomat.</p></div><p><a href=\"https://arstechnica.com/?p=1922001#p3\">Read 10 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=1922001&amp;comments=1\">Comments</a></p>","author":"Ashley Belanger","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"46df3251c58064ad1753d51096c93810837ca896f349dcb8d9272d69a5c744d5","category":"Tech"}