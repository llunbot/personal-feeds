{"title":"AI ที่อธิบายได้: หลักการ เหตุผล และความจำเป็น","link":"https://srakrn.me/blog/explainable-ai/","date":1586581994000,"content":"<p><em>รวมบทความในชุดดังกล่าวที่เผยแพร่ครั้งแรกบนเฟซบุ๊กของศิระกร ลำใย, บทความขณะนี้ยังเขียนไม่ครบทุกตอน</em></p>\n\n\n\n<h2>ทำไมเราต้องมี AI ที่อธิบายได้ </h2>\n\n\n\n<p>(1)</p>\n\n\n\n<figure><img src=\"https://srakrn.me/blog/wp-content/uploads/2020/04/Slide1-1024x576.png\" /><figcaption> <br />จำนวนร้อยละของผู้ต้องหาที่ได้รับการปล่อยตัวระหว่างสู้คดี  โดยที่ไม่ต้องวางเงินประกันตัว ในช่วงเวลาต่างๆ ของปี  สังเกตว่าช่องว่างระหว่างจำนวนผู้ต้องหาผิวสีและผู้ต้องหาผิวขาวเพิ่มขึ้นอย่างมากหลังการประกาศใช้กฎหมาย  HB463<br />ภาพประกอบทำซ้ำจาก <a href=\"https://www.minnesotalawreview.org/wp-content/uploads/2019/01/13Stevenson_MLR.pdf\">https://www.minnesotalawreview.org/wp-content/uploads/2019/01/13Stevenson_MLR.pdf</a> </figcaption></figure>\n\n\n\n<p>ในปี 2017  รัฐเคนตักกี้ผ่านร่างกฎหมาย HB417  ที่บังคับให้ผู้พิพากษาต้องปรึกษากับระบบอัตโนมัติเพื่อพิจารณาว่าผู้ต้องหาที่จะได้รับการประกันตัวหรือปล่อยตัวระหว่างสู้คดี  จะสร้างความอันตรายให้กับสาธารณะหรือไม่</p>\n\n\n\n<p>หลังจากร่างกฎหมายดังกล่าวผ่าน  ช่องว่างระหว่างจำนวนคนขาวและคนผิวสีที่ได้รับการประกันตัวพุ่งสูงขึ้นมาก  [ภาพที่ 1] คนผิวขาวได้รับการปล่อยตัวโดยไม่ต้องวางเงินประกันเพิ่มขึ้น  ขณะที่คนผิวสีไม่ได้รับการปล่อยตัวในลักษณะเดียวกันมากขึ้นเท่าไหร่นัก</p>\n\n\n\n<p>(2)</p>\n\n\n\n<figure><img src=\"https://scontent.fbkk5-7.fna.fbcdn.net/v/t1.0-9/92429908_614615002461808_3275585606050119680_n.jpg?_nc_cat=104&amp;_nc_sid=8024bb&amp;_nc_eui2=AeH5B2TV036L7SOGfev8BCZdx6CqI5ppTvvHoKojmmlO-4hSYKYJptXRS4F9Bw0Jw4rr_iXccnUMWviGX-GOry7z&amp;_nc_ohc=BNPU-FvtZY0AX9wsBIC&amp;_nc_ht=scontent.fbkk5-7.fna&amp;oh=e5d14287d0c45b42211aef1d21fd55cc&amp;oe=5EB63F59\" /><figcaption>อัตราการจ้างงานในสายเทคโนโลยีของบริษัทชั้นนำในสหรัฐ<br />ภาพประกอบทำซ้ำจาก <a href=\"https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G\">https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G</a></figcaption></figure>\n\n\n\n<p>ในปี 2018 แอมะซอน “โละ” ระบบคัดกรองใบสมัครงานทิ้ง  ระบบนี้เกิดขึ้นมาเพื่อคาดหวังว่าจะช่วยกรองใบสมัครงานที่ได้รับเข้ามาเป็นพันๆ  ใบเพื่อช่วยลดงานของมนุษย์ เหตุผลของการโละระบบคัดกรองดังกล่าวคือ  ข้อมูลการรับสมัครงานที่ใช้ในการ “สอน” ระบบมีจำนวนเพศชายมากกว่าเพศหญิง  [ภาพที่ 2] เมื่อระบบดังกล่าวถูกสอนด้วยข้อมูลลักษณะเช่นนี้  ก็จะหยิบเอาพฤติกรรมการเลือกผู้สมัครชายมากกว่าผู้สมัครหญิงมามากขึ้นเช่นกัน</p>\n\n\n\n<p>(3)</p>\n\n\n\n<figure><img src=\"https://srakrn.me/blog/wp-content/uploads/2020/04/Slide2.png\" /></figure>\n\n\n\n<p>หนึ่งในเครื่องมือประมวลผลภาษาธรรมชาติ (NLP) ที่ได้รับความนิยม  คือการเปลี่ยนคำเป็นเลขที่มีความหมาย เรียกว่าการทำ word embedding  การเปลี่ยนคำเป็นเลขทำให้เราใช้วิธีการทางคณิตศาสตร์ในการสอนคอมพิวเตอร์แก้โจทย์เชาว์ในลักษณะ  “กรุงเทพคู่กับประเทศไทย เหมือนที่ลอนดอนคู่กัน ___” ได้</p>\n\n\n\n<p>ถ้าเราถามคำถามลักษณะเดียวกันเช่น “ชายคู่กับพ่อ เหมือนหญิงคู่กับ ___”,  “ชายคู่กับราชา เหมือนหญิงคู่กับ ___”, “ชายคู่กับหมอ เหมือนหญิงคู่กับ  ___”, “ชายคู่กับฟุตบอล เหมือนหญิงคู่กับ ___”  เราจะพบว่าบางครั้งคู่คำไม่สามารถเติมได้ (เช่นหมอ  เพราะผู้หญิงก็เป็นหมอได้ และเราก็มีบุรุษพยาบาล)  แต่อคติและความโน้มเอียงทางเพศที่ถูกสื่อผ่านงานเขียนและข้อมูลที่ใช้ “สอน”  ตัวเปลี่ยนคำให้เป็นเลข  ก็ทำให้คอมพิวเตอร์ตอบคำถามเหล่านี้แบบโน้มเอียงทางเพศไม่ใช่น้อย [ภาพที่  3]</p>\n\n\n\n<p>ด้วยตัวอย่างที่เกิดขึ้นจริงบนโลก  และสร้างผลกระทบไว้แล้วไม่ใช่น้อย เราควรจะเห็นว่า AI  ไม่ใช่ของวิเศษที่จะอ้างว่าเอามาใช้แล้วจบ กระบวนการสอน AI  ให้มีความฉลาดในการทำงาน โดยเฉพาะในงานที่มีความสำคัญ  จำเป็นต้องผ่านการตรวจสอบอคติใน AI เป็นอย่างละเอียด  ขั้นตอนวิธีดังกล่าวไม่ใช่เรื่องง่าย แต่หากไม่สามารถการันตีได้ว่า AI  ผ่านการตรวจสอบแล้ว การใช้ AI  ก็ย่อมไม่ก่อให้เกิดประโยชน์นอกจากการทุ่นเวลาที่มาพร้อมกับข้อเสียมหาศาลและความกังขาในการอธิบายไม่ได้ที่จะเกิดขึ้นจำนวนมาก</p>\n\n\n\n<hr />\n\n\n\n<h2>AI ไม่ใช่พ่อ และความผิดพลาดของ AI ก็ต้องอธิบายได้ </h2>\n\n\n\n<p>ก่อนพูดถึงความจำเป็นในการอธิบาย AI เราอาจจะต้องย้อนไปถึงการอธิบายการบอกว่า AI ทำงานได้ดีแค่ไหนในกรณีทั่วไปก่อน สำหรับโครงการเราไม่ทิ้งกัน แน่นอนว่าสิ่งที่เราอยากทำคือการตอบว่า “ใช่ (+)” หรือ “ไม่ (-)” สำหรับคำถามว่าเราควรแจกเงินคนคนนี้หรือเปล่า</p>\n\n\n\n<figure><img src=\"https://srakrn.me/blog/wp-content/uploads/2020/04/Slide7.png\" /></figure>\n\n\n\n<p>กรณีที่เกิดขึ้นกับคำตอบเป็นไปได้สี่แบบ</p>\n\n\n\n<ul><li>บวกจริง (ดูจากสภาพแล้วควรได้เงินจริงๆ และ AI ก็ตอบว่าใช่ ควรได้เงิน)</li><li>บวกลวง (ดูจากสภาพแล้วไม่ควรได้เงิน แต่ AI กลับตอบว่าใช่ ควรได้เงิน)</li><li>ลบลวง (ดูจากสภาพแล้วควรได้เงินจริงๆ แต่ AI กลับตอบว่าไม่ต้องให้เงินคนนี้)</li><li>ลบจริง (ดูจากสภาพแล้วไม่ควรได้เงิน และ AI ก็ตอบว่าไม่ต้องให้เงินคนนี้)</li></ul>\n\n\n\n<p>ถ้าเราเป็นรัฐบาลที่กำลังถังแตก เราอาจจะบอกว่าแจกเงินตกหล่นไปบ้างไม่เป็นไร แต่เงินทุกบาทต้องไปถึงมือคนที่ต้องการ “จริง” แต่ถ้าเราเงินเหลือ เราอาจจะบอกว่าเผลอแจกเงินคนไม่เดือดร้อนก็ได้ เซฟไว้หน่อย เงินจะได้ถึงมือคนครบๆ แน่นอนว่าปัญหาในลักษณะนี้ไม่ได้เป็นปัญหาแค่ในเชิง AI–ยกตัวอย่างง่ายๆ ตอนนี้กระทรวงสาธารณสุขกำหนดว่าจะส่งตรวจ COVID-19 ได้ ผู้ป่วยต้องมีเกณฑ์อะไรบ้าง–ลองคิดสภาพว่ากฎประมาณนี้เกิดมาจากการเนรมิตของ AI แล้วเราเห็นอะไรบ้าง</p>\n\n\n\n<figure><img src=\"https://srakrn.me/blog/wp-content/uploads/2020/04/Slide3-1-1024x576.png\" /></figure>\n\n\n\n<p>เราน่าจะเคยเห็นหลายๆ เคสของผู้ป่วยที่ไม่มีไข้ แต่ส่งตรวจเองแล้วผลเป็นบวก พอมามองเกณฑ์นี้ก็จะเห็นว่ามีผู้ป่วยที่น่าจะถูกปัดตกโดยเกณฑ์ไปจากการที่วัดไข้แล้วไม่เจอ ในกรณีนี้ เราสามารถไปไล่ตั้งคำถามได้ทันทีว่าทำไมผู้ป่วยคนนีัถึงไม่ถูกตรวจทั้งๆ ที่ควรจะตรวจ แล้วค่อยๆ ปรับเกณฑ์กันไป</p>\n\n\n\n<p>แต่สิ่งเหล่านี้จะไม่เกิดขึ้นกับ AI ที่อธิบายไม่ได้–AI หลายครั้งทำหน้าที่เป็นเหมือนกล่องดำ ยัดข้อมูลเข้าไปแล้วได้คำตอบ แต่ไม่มีคำอธิบายว่าทำไมถึงออกมาเป็นแบบนี้ ซ้ำร้ายในหลายๆ แบบจำลอง การ “แงะ” กล่องดำมาดูว่าทำไมถึงเป็นแบบนี้ ยิ่งไม่สามารถทำได้ด้วยซ้ำ</p>\n\n\n\n<p>แต่สิ่งเหล่านี้จะไม่เกิดขึ้นกับ AI ที่อธิบายไม่ได้–AI หลายครั้งทำหน้าที่เป็นเหมือนกล่องดำ ยัดข้อมูลเข้าไปแล้วได้คำตอบ แต่ไม่มีคำอธิบายว่าทำไมถึงออกมาเป็นแบบนี้ ซ้ำร้ายในหลายๆ แบบจำลอง การ “แงะ” กล่องดำมาดูว่าทำไมถึงเป็นแบบนี้ ยิ่งไม่สามารถทำได้ด้วยซ้ำ</p>\n\n\n\n<figure><img src=\"https://srakrn.me/blog/wp-content/uploads/2020/04/Slide4.png\" /></figure>\n\n\n\n<p>แถมต่อให้บอกว่า AI แม่นจริงๆ สมมติว่าเรามี AI หนึ่งตัวสำหรับใช้คัดกรองโรคที่มีโอกาสพบเจอได้ใน 1% ของประชากร ถ้าสมมติว่าเราให้ AI ตัวนั้นตอบว่า “ไม่เป็น” ไม่ว่าจะกรณีใดๆ ก็ตาม เราจะได้ AI ที่มีความแม่นยำ 99% (เพราะโอกาสที่จะเจอคนไม่เป็นมี 99%)</p>\n\n\n\n<p>คำถามคือเราอยากได้ AI แบบนี้ไหม? แน่นอน คำตอบก็คงเป็นไม่ และถ้าเอา AI แบบนี้มาใช้แจกเงิน รัฐก็คงไม่อยากได้ AI ที่เลือกจะแจกเงินให้ทุกคน หรือเลือกที่จะไม่แจกเงินให้ใครเลย</p>\n\n\n\n<p>ดังนั้น AI ที่ทำงานบนข้อมูลที่มีความละเอียดอ่อน และต้องการความละเอียดอ่อนในการจำแนกปัญหา จึงจำเป็นจะต้องถูกวัดผลอย่างเคร่งครัด และการวัดผลไม่ใช่พึงกระทำแค่การวัดว่าตอบถูกมากน้อย หยิบขาดหยิบเกิน แต่การ “แงะกล่องดำ” มาอธิบายพฤติกรรมนิสัยของ AI ได้ ก็เป็นเรื่องที่จำเป็นไม่แพ้กัน </p>\n\n\n\n<hr />\n\n\n\n<h2>สิทธิ์แห่งคำอธิบาย</h2>\n\n\n\n<figure><img src=\"https://srakrn.me/blog/wp-content/uploads/2020/04/image-1.png\" /><figcaption>การชี้แจงสาเหตุการไม่อนุมัติสินเชื่อ ทำซ้ำจาก <a href=\"https://www.bot.or.th/Thai/fipcs/Documents/FPG/2553/ThaiPDF/25530010.pdf\">https://www.bot.or.th/Thai/fipcs/Documents/FPG/2553/ThaiPDF/25530010.pdf</a></figcaption></figure>\n\n\n\n<p>ทุกครั้งเวลาเดินเข้าไปในธนาคารและขอสินเชื่อไม่ผ่าน ธนาคารแห่งประเทศไทยกำหนดให้ธนาคารต้องชี้แจงเหตุผลในการปฏิเสธสินเชื่อ เราจะเข้าใจตัวเองมากขึ้นว่าเพราะอะไรสินเชื่อเราถึงไม่ผ่านการขอ</p>\n\n\n\n<p>หรือหากเราได้รับคำอธิบายว่า “เพราะติดเครดิตบูโร” ในทางเดียวกันเราสามารถส่งคำต้องไปยังบริษัทข้อมูลเครดิตแห่งชาติ เพื่อดูชุดของข้อมูลที่ถูกใช้ปฏิเสธสินเชื่อเราได้ว่ามีความถูกต้องมากน้อยเพียงใด</p>\n\n\n\n<p>นี่คือคำอธิบาย ไม่ใช่เพียงคำอธิบายว่าทำไมสินเชื่อถึงกู้ไม่ผ่าน แต่เป็นคำอธิบายว่าเพราะอะไรเราถึงน่าเชื่อถือหรือไม่น่าเชื่อถือในสายตาสถาบันการเงิน</p>\n\n\n\n<figure><img src=\"https://srakrn.me/blog/wp-content/uploads/2020/05/92555439_615220729067902_8154642920725544960_o-1024x576.jpg\" /><figcaption> <br />ทำซ้ำจาก <a href=\"https://www.privacy-regulation.eu/en/r71.htm\">https://www.privacy-regulation.eu/en/r71.htm</a> </figcaption></figure>\n\n\n\n<p>ใน Recital 71 ของกฎหมาย GDPR (General Data Protection Regulation) ว่าด้วยการคุ้มครองข้อมูลส่วนบุคคล มีการกล่าวถึง “สิทธิ์ในคำอธิบาย” ไว้ว่าผู้ถือครองข้อมูลมีสิทธิ์ที่จะร้องขอให้มนุษย์เข้าแทรกแซงระบบอัตโนมัติใดๆ เพื่อแสดงจุดยืนของตัวเอง และเพื่อร้องขอคำอธิบายเหตุผลของการตัดสินใจ</p>\n\n\n\n<p>จะเห็นได้ว่าปัญหาของอคติจากระบบตัดสินใจอัตโนมัติไม่ใช่ปัญหาที่เพิ่งมีแต่อย่างใด (GDPR ออกเมื่อปี 2016 มีผลบังคับใช้ 2018) แม้จะมีข้อวิพากษ์ว่ากฎหมายลักษณะนี้อาจเอื้อให้เกิดการใช้มนุษย์มากกว่าระบบอัตโนมัติ แต่มุมมองส่วนตัวของผู้เขียนคือตราบใดที่ระบบอัตโนมัติไม่สามารถออกมาอธิบายตัวเองได้ว่าเพราะอะไรจึงตอบแบบนี้ มนุษย์ (ซึ่งอย่างน้อยก็ยังออกมาบอกได้ว่าตัวเองคิดอะไรอยู่–ซึ่งเอื้อให้เกิดการโต้แย้งทั้งความผิดพลาดในการตัดสินใจไม่ว่าโดยสุจริตหรือโดยทุจริต) ก็คงเหมาะกับงานในลักษณะแบบนี้มากกว่าอยู่ดี</p>\n\n\n\n<h2>อคติ อคติ อคติ</h2>\n\n\n\n<h3>ว่าด้วยอคติจากมนุษย์</h3>\n\n\n\n<p>หนึ่งในวิธีการฝึกสอน AI ที่ทำได้ และทำง่าย คือการฝึกสอนแบบมีการควบคุม (supervised learning) ถ้าเราต้องการฝึกสอน AI ให้ตอบว่าจะแจกเงินหรือไม่แจกเงิน เรานำรายการของคนมาตอบเองก่อนว่าจะแจกเงินหรือไม่แจก แล้วให้ AI เรียนรู้รูปแบบการตอบของเราเอง</p>\n\n\n\n<p>ดังนั้นขั้นตอนแรกของการฝึกสอน คือเราจำเป็นต้องแปะป้ายก่อนว่าเราจะแจกเงินใคร และไม่แจกเงินใคร</p>\n\n\n\n<p>เราต้องการแจกเงินคนแบบไหนนะ? คนจนคนเดือดร้อน!คนแบบไหนที่เดือดร้อนนะ? ลูกจ้างรายวัน พนักงานโรงแรม ช่างเสริมสวย แม่ค้า คนขับรถแท็กซี่ พนักงานบริษัท!<br />คนแบบไหนที่ไม่เดือดร้อน* นะ? เด็กอายุต่ำกว่า 18 เกษตรกร นิสิต ขายของออนไลน์ โปรแกรมเมอร์ แรงงานก่อสร้าง!</p>\n\n\n\n<p>แล้วทำไมเราถึงคิดว่าคนแบบนี้เดือดร้อน หรือคนแบบนี้ไม่เดือดร้อน? เพราะเรากำลังใส่สิ่งที่เรียกว่า “อคติ” ลงไป</p>\n\n\n\n<hr />\n\n\n\n<div><figure><img src=\"https://srakrn.me/blog/wp-content/uploads/2020/05/92869186_615233072400001_3284979416786010112_n.jpg\" /></figure></div>\n\n\n\n<p>อคติในที่นี้ไม่ใช่ศัพท์แง่ลบ แต่เป็นการแปลตรงตัวของคำว่า “bias” ในภาษาอังกฤษ<br />มนุษย์เป็นสิ่งมีชีวิตที่เต็มไปด้วยอคติ บ้างจากสัญชาติญาณ บ้างจากประสบการณ์เรียนรู้ หนึ่งในกรณีที่โด่งดังคือภาพหลุมบนดาวอังคารที่ถูกถ่ายจากยานไวกิ้งที่เหมือนหน้าคน แต่ความจริงแล้วเกิดจากการที่เราถูก “อคติ” ของการมองเห็นอะไรเป็นหน้าคนได้เรื่อยๆ เหนี่ยวนำให้เห็นแสงและเงาเป็นหน้าคนไปเองต่างหาก</p>\n\n\n\n<figure><img src=\"https://srakrn.me/blog/wp-content/uploads/2020/04/image-6.png\" /><figcaption>Icons made by <a href=\"https://www.flaticon.com/authors/smalllikeart\">smalllikeart</a> from <a href=\"https://www.flaticon.com/\">www.flaticon.com</a></figcaption></figure>\n\n\n\n<p>สำหรับคนที่มีหน้าที่ “แปะป้าย” ข้อมูลสำหรับสอน AI อคติตรงนี้อาจเหนี่ยวนำให้เราคิดว่าโปรแกรมเมอร์เป็นอาชีพที่มีความยืดหยุ่นในการทำงาน (?) ทำจากที่ไหนก็ได้ (?!) หรืออคติว่าเพราะงานก่อสร้างยังไม่ได้รับผลกระทบ กรรมกรก่อสร้างก็เลยไม่ได้รับผลกระทบจาก COVID-19 (?!?!)</p>\n\n\n\n<p>นี่คืออคติรูปแบบที่หนึ่ง เป็นอคติที่เรามองเห็น และเข้าใจได้</p>\n\n\n\n<p>ความน่ากลัวคือแบบจำลองอาจ–จริงๆ ก็ไม่อาจ มีแนวโน้มสูงมากที่–จะ “หยิบ” อคติของมนุษย์ติดตัวเข้ามาด้วย ถ้าอคตินั้นมองเห็นได้ง่ายแบบที่เรามองเห็นวาเกณฑ์อาชีพที่ไม่เข้าข่ายนั้นไม่มีเหตุผล ก็อาจจะรอดตัวไป แต่คนแปะป้ายอาจจะมีอคติอีกจำนวนมากที่เกิดขึ้นโดยไม่รู้ตัว</p>\n\n\n\n<hr />\n\n\n\n<figure><img src=\"https://srakrn.me/blog/wp-content/uploads/2020/04/image-5.png\" /></figure>\n\n\n\n<p>ขออนุญาตเล่านิทานเรื่องลูกเป็ดขี้เหร่ มีลูกเป็ดตัวนึงขี้เหร่ โตมากลายเป็นหงส์ จบ</p>\n\n\n\n<p>เดี๋ยว ทำไมลูกเป็ดถึงขี้เหร่นะ–เพราะมันมีสีดำ, แล้วลูกเป็ดตัวอื่นขี้เหร่ไม่ได้เหรอ</p>\n\n\n\n<p>สมมติว่าผมมีลูกเป็ดสามตัว ว่ายน้ำเรียงกันต้อยๆ ถ้าผมพิจารณาการถามเพียงว่า “ลูกเป็ดตัวนี้ใช่ตัวหน้าสุดไหม” กับ “ลูกเป็ดตัวนี้ใช่สีดำไหม” ผมสามารถเขียนกฎออกมาเพื่อ “เลือก” แปะป้ายลูกเป็ดตัวไหนก็ได้ว่าเป็นลูกเป็ดขี้เหร่ เช่นถ้าผมบอกว่า “ลูกเป็ดที่ไม่ได้เป็นตัวหน้าและไม่ได้เป็นสีดำ เป็นลูกเป็ดขี้เหร่” ตัวตรงกลางก็จะกลายเป็นลูกเป็ดขี้เหร่ทันที</p>\n\n\n\n<p>ทฤษฎีดังกล่าวชื่อว่าทฤษฎีลูกเป็ดขี้เหร่ เสนอโดย Satosi Watanabe ให้สรุปคร่าวๆ คือเราไม่สามารถ “แปะป้าย” อะไรก็ตามได้เลยหากเราไม่ได้ใส่ “อคติ” ลงไปขณะแปะป้าย เหมือนที่เราไม่สามารถแปะป้ายว่าใครจะเดือดร้อน ถ้าเราไม่ได้ใส่ชุดความคิดของเราว่าคนแบบไหนถึงจะเดือดร้อนเข้าไป–ซึ่งนี่แหละคืออคติ</p>\n\n\n\n<p>และเป็นอคติอันนี้เอง ที่ AI ดูดซับและเรียนรู้เข้าไปอย่างเต็มเปี่ยม เป็นอคติจากมนุษย์ที่สถิตย์เข้าไปใน AI จนดูเหมือนว่าไม่มีมนุษย์คนใดต้องรับผิดชอบจากอคติดังกล่าว<br />แต่ไม่ใช่เลย ไม่เป็นความจริงเลย, ไม่ว่าจะเป็นมนุษย์ที่สร้างอคติ หรือมนุษย์ที่จับอคติลงไปใส่ใน AI ก็ล้วนต้องรับผิดชอบทั้งสิ้น</p>\n\n\n\n<p>อย่าปล่อยให้คำว่า “AI คัดกรอง” เป็นตัวตัดจบบทสนทนา</p>\n\n\n\n<h3>ว่าด้วยอคติจากขั้นตอนวิธี</h3>\n\n\n\n<p>สมมติว่าสุดท้ายเรามีสุดยอดมนุษย์ที่ปราศจากอคติใดๆ ทั้งปวง แปะป้ายข้อมูลประหนึ่งเทพลงมาจุติ ประชากรไทยทั้ง 70 ล้านคนเห็นด้วยว่าคนแบบนี้คือคนที่ควรและไม่ควรได้รับเงินเยียวยาจริงๆ</p>\n\n\n\n<p>ในข้อมูลที่แปะป้าย มีประชากร 3 ใน 10 คนที่ได้รับการเยียวยา ส่วนอีก 7 ใน 10 ไม่ได้รับการเยียวยา ประชากรนั้นประกอบอาชีพแตกต่างกันออกไป ข้อมูลชุดนี้ถูกนำมาฝึกสอน AI คัดแยกว่าใครควรหรือไม่ควรได้รับเงิน ทันใดนั้นเอง…</p>\n\n\n\n<p>พบประชากร 1 คนมีอาชีพอะไรสักอย่าง ดูแล้วควรจะได้เงินกระมัง แต่ว่าอาชีพนี้ไม่ปรากฎอยู่ในข้อมูลที่ถูกแปะป้ายแล้วนำไปสอน กล่าวคือเป็นอาชีพที่ AI ก็เพิ่งมารู้จักตะกี้นี่แหละ</p>\n\n\n\n<p>คำถามคือ หากตัดสินจากอาชีพ ประชากรคนนี้จะได้เงินหรือไม่ได้เงิน</p>\n\n\n\n<figure><img src=\"https://srakrn.me/blog/wp-content/uploads/2020/04/Slide3.png\" /><figcaption>Icons made by <a href=\"https://www.flaticon.com/authors/smalllikeart\">smalllikeart</a> from <a href=\"https://www.flaticon.com/\">www.flaticon.com</a></figcaption></figure>\n\n\n\n<p>คำตอบอาจจะเป็นเรื่องที่น่าเศร้า–แต่ภายใต้วิธีการเรียนรู้หลายๆ วิธี ชายคนนี้จะถูกจัดกลุ่มให้อยู่ในรูปของ “คนส่วนใหญ่” ซึ่งในที่นี้ก็คือคนที่ไม่ได้เงิน ด้วยเหตุผลว่า AI ที่เห็นข้อมูลว่าคนดูไม่ได้รับเงินเยียวยามากกว่าคนได้รับเงินเยียวยา ก็จะมีความโน้มเอียงไปหาการตอบว่า “ไม่ได้รับเงิน” มากกว่าที่จะเลือกตอบว่าได้รับเงินนั่นเอง</p>\n\n\n\n<p>ตัวอย่างของอคตินี้เห็นได้ชัดเป็นอย่างยิ่งในการเรียนรู้ของเบยส์ (Bayesian Learning) ซึ่งเราอาจจะคุ้นเคยกันในวิชาสถิติว่าด้วยความน่าจะเป็นแบบมีเงื่อนไข (conditional probability)</p>\n\n\n\n<figure><img src=\"https://srakrn.me/blog/wp-content/uploads/2020/04/Slide11.png\" /></figure>\n\n\n\n<p>การเรียนรู้ของเบยส์อยู่บนหลักการของการถามคำถามว่า “ถ้า B แล้วจะ A ไหม” ในที่นี้คือการถามว่า “ถ้าประกอบอาชีพ XYZ แล้วจะได้เงินไหม” ซึ่งการตอบคำถามนี้มีปัจจัยเข้ามาเกี่ยวข้องสามตัวด้วยกัน</p>\n\n\n\n<ul><li>มีคนกี่คนที่ได้เงิน แล้วประกอบอาชีพ XYZ (เอาเฉพาะคนได้เงินมาดู)</li><li>มีคนกี่คนที่ได้เงิน (อัตราส่วนคนได้เงินต่อคนทั้งหมด)</li><li>มีคนกี่คนที่ทำอาชีพ XYZ ต่อคนทั้งหมด</li></ul>\n\n\n\n<p>จะเห็นได้ว่าปัจจัยที่มีปัญหาคือปัจจัยที่สอง เพราะว่าในเมื่อแบบจำลองไม่เคยเห็นอาชีพ XYZ จึงไม่สามารถคิดปัจจัยที่หนึ่งและสามได้ ทำให้ต้องตัดสินใจจากปัจจัยที่สอง–นั่นคือดูว่ามีคนได้เงินเยอะหรือน้อย โดยไม่ได้แม้แต่จะใส่ใจว่าเขาทำอาชีพใด</p>\n\n\n\n<p>ที่จริงแล้วปัญหาดังกล่าวเป็นหนึ่งในปัญหาสำคัญของการทำจักรกลเรียนรู้ (Machine Learning) ที่เรียกว่าปัญหาความไม่สมดุลของชุดข้อมูล (class imbalance) การที่มีข้อมูลที่โน้มเอียงไปยังทางใดทางหนึ่ง (เช่นในตัวอย่างที่โน้มเอียงไปทางไม่แจกเงิน) ย่อมทำให้เกิดการเลือกตอบที่เอียงไปตามข้อมูล แม้ว่ามนุษย์ผู้แปะป้ายจะไม่มีอคติเลยก็ตาม<br /></p>","author":"@srakrn","siteTitle":"@srakrn's Blog","siteHash":"8610f010f52679fdf8125827e361d560c59c9ee46c9232365fabf44f77865fb7","entryHash":"7e6f3a1265a26168b43edf0ade3998ebfa9641288e98f3f7ec2867da7e7a464e","category":"Thai"}