{"title":"What if AI doesnâ€™t just keep getting better forever?","link":"https://arstechnica.com/ai/2024/11/what-if-ai-doesnt-just-keep-getting-better-forever/","date":1731443651000,"content":"<p>For years now, many AI industry watchers have looked at the quickly growing capabilities of new AI models and <a href=\"https://arstechnica.com/information-technology/2024/07/microsoft-cto-defies-critics-ai-progress-not-slowing-down-its-just-warming-up/\">mused about exponential performance increases continuing well into the future</a>. Recently, though, some of that AI \"scaling law\" optimism has been replaced by fears that we may already be hitting a plateau in the capabilities of large language models trained with standard methods.</p>\n<p>A <a href=\"https://www.theinformation.com/articles/openai-shifts-strategy-as-rate-of-gpt-ai-improvements-slows\">weekend report from The Information</a> effectively summarized how these fears are manifesting amid a number of insiders at OpenAI. Unnamed OpenAI researchers told The Information that Orion, the company's codename for its next full-fledged model release, is showing a smaller performance jump than <a href=\"https://arstechnica.com/information-technology/2023/03/openai-announces-gpt-4-its-next-generation-ai-language-model/\">the one seen between GPT-3 and GPT-4</a> in recent years. On certain tasks, in fact, the upcoming model \"isn't reliably better than its predecessor,\" according to unnamed OpenAI researchers cited in the piece.</p>\n<p>On Monday, OpenAI co-founder Ilya Sutskever, who <a href=\"https://arstechnica.com/information-technology/2024/05/chief-scientist-ilya-sutskever-leaves-openai-six-months-after-altman-ouster/\">left the company earlier this year</a>, added to the concerns that LLMs were hitting a plateau in what can be gained from traditional pre-training. Sutskever <a href=\"https://www.reuters.com/technology/artificial-intelligence/openai-rivals-seek-new-path-smarter-ai-current-methods-hit-limitations-2024-11-11/\">told Reuters</a> that \"the 2010s were the age of scaling,\" where throwing additional computing resources and training data at the same basic training methods could lead to impressive improvements in subsequent models.</p><p><a href=\"https://arstechnica.com/ai/2024/11/what-if-ai-doesnt-just-keep-getting-better-forever/\">Read full article</a></p>\n<p><a href=\"https://arstechnica.com/ai/2024/11/what-if-ai-doesnt-just-keep-getting-better-forever/#comments\">Comments</a></p>","author":"Kyle Orland","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"8e58e950859e3a9031b1655f8d2f6b8d3cb6974cbbf8dd8ad0c1c453a8d19b84","category":"Tech"}