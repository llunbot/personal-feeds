{"title":"Running Vault on Nomad, Part 1","link":"https://www.hashicorp.com/blog/running-vault-on-hashicorp-nomad-part-1","date":1714503600000,"content":"<p>Vault is a secrets management platform that provides encrypted storage for long lived secrets, identity brokerage using ephemeral credentials, and <a href=\"https://developer.hashicorp.com/vault/tutorials/encryption-as-a-service/eaas-transit\">encryption as a service</a>. Unless you’re using <a href=\"https://www.hashicorp.com/cloud\">HashiCorp Cloud Platform</a> to host Vault (which is always recommended if you can support it), deploying and running Vault clusters will likely be a manual process. Any time a server needs to be restarted, an engineer would need to login and restart the service. </p>\n\n<p>This is what orchestrators like <a href=\"https://developer.hashicorp.com/nomad/docs/nomad-vs-kubernetes\">HashiCorp Nomad and Kubernetes</a> were built to automate. While Kubernetes has a wide array of components to manage additional use cases, <a href=\"https://www.hashicorp.com/resources/how-does-nomad-work\">Nomad</a> is mainly focused on scheduling and cluster management. It’s simple to run, lightweight, and supports running VMs, containers, raw executables, JAR files, Qemu workloads, and more with custom task driver plugins. By running Vault as a Nomad job (Nomad’s term for workloads), operators can manage and schedule Vault servers with a low-complexity architecture.</p>\n\n<p>This post shows how to deploy and configure Vault servers on Nomad using <a href=\"https://developer.hashicorp.com/terraform/cloud-docs\">HCP Terraform</a>. The secrets consumption will be done using the Nomad and Vault CLI’s, respectively, to show the underlying workflows. The Terraform code will be split in two, with separate configuration for the infrastructure and the Vault deployment. This is done to manage the states for these workspaces separately and share dependency outputs between them.</p>\n\n<h2>Deployment architecture</h2>\n<img src=\"https://www.datocms-assets.com/2885/1714499107-vault-nomad-arch.png\" alt=\"Vault-Nomad\" /><p>This deployment architecture requires five virtual machines (VMs) — one is the Nomad server, and the other four are the Nomad clients that run the Vault servers, including a backup server for Vault. These VMs will be deployed to Amazon EC2 instances. The VMs will all live in the same virtual private cloud (VPC) subnet.</p>\n\n<h2>HCP Terraform and directory setup</h2>\n\n<p>Because this approach splits the architecture into multiple workspaces, you need to configure remote backends for each HCP Terraform workspace so that output dependencies can be shared between them. To create these workspaces, create a directory structure that contains a folder for each workspace. The directory structure should look like this:</p>\n\n<pre><code>├── 1-nomad-infrastructure\n├── 2-nomad-configuration\n├── 3-nomad-example-job-deployment\n\n3 directories\n</code></pre>\n\n<p>The remote backend is HCP Terraform. To create the remote backends, create a file called <code>backend.tf</code> in each of the directories. Here is a <a href=\"https://github.com/devops-rob/vault-on-nomad-demo/blob/main/tfc-setup.sh\">shell script that will create the directory structure and write the relevant <code>backend.tf</code> files</a> in all of the directories. </p>\n\n<h2>Networking for the Nomad cluster</h2>\n\n<p>To create the infrastructure for Nomad, navigate to the <code>1-nomad-infrastructure</code> directory. First, set up your AWS Terraform provider. Here is the <a href=\"https://github.com/devops-rob/vault-on-nomad-demo/blob/main/1-nomad-infrastructure/providers.tf\">provider.tf code</a>.</p>\n\n<p>Once the provider is configured, you’re ready to deploy a VPC and a subnet. To do this, there is another file in the same directory, called <code>network.tf</code>, which contains the code below:</p>\n<pre><code>module \"vpc\" {\n  source = \"terraform-aws-modules/vpc/aws\"\n\n  name = \"my-vpc\"\n  cidr = \"10.0.0.0/16\"\n\n  azs             = [\"eu-west-1a\"]\n  private_subnets = [\"10.0.1.0/24\", \"10.0.2.0/24\", \"10.0.3.0/24\"]\n  public_subnets  = [\"10.0.101.0/24\", \"10.0.102.0/24\", \"10.0.103.0/24\"]\n\n  enable_nat_gateway = true\n  enable_vpn_gateway = false\n  enable_dns_support = true\n  enable_dns_hostnames = true\n\n  tags = {\n    Terraform = \"true\"\n    Environment = \"dev\"\n  }\n}</code></pre><p>This code deploys all the resources required for a fully functional network, including resources for a working AWS VPC, associated subnets, and NAT gateways. It uses the community Terraform <a href=\"https://developer.hashicorp.com/terraform/language/modules\">module</a> called the <a href=\"https://registry.terraform.io/modules/terraform-aws-modules/vpc/aws\">AWS VPC Terraform module</a>, available on the <a href=\"https://registry.terraform.io\">Terraform Registry</a>.  </p>\n\n<h2>Configuration of Nomad servers</h2>\n\n<p>Before you can write the Terraform code to deploy the five VMs, you need to write some shell scripts to configure the servers during their deployment as a prerequisite. The first is for the Nomad server called <code>nomad-server.sh</code>:</p>\n<pre><code>#! /bin/bash -e\n\n# Install Nomad\nsudo apt-get update &amp;&amp; \\\n  sudo apt-get install wget gpg coreutils -y\n\nwget -O- https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg\necho \"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/hashicorp.list\nsudo apt-get update &amp;&amp; sudo apt-get install nomad -y\n\n# Create Nomad directory.\nmkdir -p /etc/nomad.d\n\n\n# Nomad configuration files\ncat &lt; /etc/nomad.d/nomad.hcl\nlog_level = \"DEBUG\"\ndata_dir = \"/etc/nomad.d/data\"\n\nserver {\n  enabled = true\n  bootstrap_expect = ${NOMAD_SERVER_COUNT}\n  server_join {\n    retry_join = [\"provider=aws tag_value=${NOMAD_SERVER_TAG} tag_key=${NOMAD_SERVER_TAG_KEY}\"]\n  }\n}\n\n\nautopilot {\n    cleanup_dead_servers      = true\n    last_contact_threshold    = \"200ms\"\n    max_trailing_logs         = 250\n    server_stabilization_time = \"10s\"\n    enable_redundancy_zones   = false\n    disable_upgrade_migration = false\n    enable_custom_upgrades    = false\n}\n\n\nEOF\n\n cat &lt; /etc/nomad.d/acl.hcl\n acl = {\n   enabled = true\n }\nEOF\n\nsystemctl enable nomad\nsystemctl restart nomad</code></pre><p>This script does a number of things to configure the Nomad server:</p>\n\n<ol>\n<li>Installs the Nomad binary.</li>\n<li>Creates the Nomad directory that contains everything it needs to function.</li>\n<li>Creates a <a href=\"https://developer.hashicorp.com/nomad/docs/configuration\">Nomad configuration file</a> for the server and places it in the Nomad directory created in step 2.\n\n<ol>\n<li>This configuration uses a feature called <a href=\"https://developer.hashicorp.com/nomad/docs/configuration/server_join#cloud-auto-join\">cloud auto-join</a> that looks for pre-specified tags on the VM and automatically joins any VMs with these tags to a Nomad cluster.</li>\n</ol></li>\n<li>Enables <a href=\"https://developer.hashicorp.com/nomad/tutorials/access-control/access-control\">access control lists (ACLs)</a> for Nomad.</li>\n<li>Starts the Nomad service.</li>\n</ol>\n\n<p>This script runs on the Nomad server VM when deployed using <a href=\"https://cloud-init.io\">cloud-init</a>. Notice the script contains three variables for cloud auto-join: <code>${NOMAD_SERVER_COUNT}</code>, <code>${NOMAD_SERVER_TAG_KEY}</code>, and <code>${NOMAD_SERVER_TAG}</code>. The values of these variables will be rendered by Terraform.</p>\n\n<p>To deploy this VM and run this script, use the file named <code>compute.tf</code>, which is in the directory mentioned above:</p>\n<pre><code>data \"aws_ami\" \"ubuntu\" {\n  most_recent = true\n\n  filter {\n    name   = \"name\"\n    values = [\"ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*\"]\n  }\n\n  filter {\n    name   = \"virtualization-type\"\n    values = [\"hvm\"]\n  }\n\n  owners = [\"099720109477\"] # Canonical\n}</code></pre><p>The code above uses a <a href=\"https://developer.hashicorp.com/terraform/language/data-sources\">data source</a> to locate the Amazon Machine Image (AMI) used to deploy the VM instance.</p>\n\n<p>The code below creates a security group attached to the VM, allowing SSH access to it on port 22:</p>\n<pre><code>resource \"aws_security_group\" \"ssh\" {\n  vpc_id = module.vpc.vpc_id\n  name   = \"allow_ssh\"\n\n  ingress {\n    from_port = 22\n    protocol  = \"tcp\"\n    to_port   = 22\n\n    cidr_blocks = [\n      \"0.0.0.0/0\"\n    ]\n  }\n\n  tags = {\n    Name = \"allow_ssh\"\n  }\n}</code></pre><p>The following code creates another security group that allows access to Nomad’s default port, which is port 4646. This is attached to the Nomad server VM:</p>\n<pre><code>resource \"aws_security_group\" \"nomad\" {\n  vpc_id = module.vpc.vpc_id\n  name   = \"nomad_port\"\n\n  ingress {\n    from_port = 4646\n    protocol  = \"tcp\"\n    to_port   = 4648\n\n    cidr_blocks = [\n      \"0.0.0.0/0\"\n    ]\n  }\n\n  tags = {\n    Name = \"nomad\"\n  }\n}</code></pre><p>The next piece of code creates a security group to allow egress connections outside of the network:</p>\n<pre><code>resource \"aws_security_group\" \"egress\" {\n  vpc_id = module.vpc.vpc_id\n  name   = \"egress\"\n  \n  egress {\n    from_port = 0\n    protocol  = \"-1\"\n    to_port   = 0\n\n    cidr_blocks = [\n      \"0.0.0.0/0\"\n    ]\n  }\n\n  tags = {\n    Name = \"egress\"\n  }\n}</code></pre><p>The code below creates the final security group of this walkthrough that allows access to Vault on its default port of 8200. This will be attached to the Nomad clients that will run Vault:</p>\n<pre><code>resource \"aws_security_group\" \"vault\" {\n  vpc_id = module.vpc.vpc_id\n  name   = \"vault\"\n\n  ingress {\n    from_port = 8200\n    protocol  = \"tcp\"\n    to_port   = 8201\n\n    cidr_blocks = [\n      \"0.0.0.0/0\"\n    ]\n  }\n\n  tags = {\n    Name = \"vault\"\n  }\n}</code></pre><p>The code below creates an IP address for the Nomad server in the first resource and associates it with the Nomad server VM in the second. Terraform will not perform the association until the Nomad server VM has been deployed. The IP address is deployed first because it is used in the Nomad config file that is deployed as part of the VM provisioning to enable the OIDC discovery URL on the Nomad server.</p>\n<pre><code>resource \"aws_eip\" \"nomad_server\" {\n  tags = {\n    Name = \"Nomad Server\"\n  }\n}\n\nresource \"aws_eip_association\" \"nomad_server\" {\n  instance_id   = aws_instance.nomad_servers.id\n  allocation_id = aws_eip.nomad_server.id\n}</code></pre><p>The other prerequisite for deploying the VM for the Nomad server is an SSH key pair, which enables authentication to the VM when connecting via SSH:</p>\n<pre><code>resource \"aws_key_pair\" \"deployer\" {\n  key_name   = \"deployer-key\"\n  public_key = file(var.ssh_key)\n}\n</code></pre><p>The code below deploys the VM for the Nomad server:</p>\n<pre><code>resource \"aws_instance\" \"nomad_servers\" {\n  ami           = data.aws_ami.ubuntu.id\n  instance_type = \"t3.micro\"\n  subnet_id     = module.vpc.public_subnets.0\n  key_name      = aws_key_pair.deployer.key_name\n\n\n  user_data = templatefile(\"./servers.sh\", {\n    NOMAD_SERVER_TAG     = \"true\"\n    NOMAD_SERVER_TAG_KEY = \"nomad_server\"\n    NOMAD_SERVER_COUNT   = 1\n    NOMAD_ADDR           = aws_eip.nomad_server.public_ip\n  })\n\n  vpc_security_group_ids = [\n    aws_security_group.ssh.id,\n    aws_security_group.egress.id,\n    aws_security_group.nomad.id\n  ]\n\n  lifecycle {\n    ignore_changes = [\n      user_data,\n      ami\n    ]\n  }\n\n  tags = {\n    Name         = \"Nomad Server\"\n    nomad_server = true\n  }\n}</code></pre><p>Some points to note about this code:</p>\n\n<ul>\n<li>This resource uses<code>cloud-init</code> to deploy the script to the VM that installs the dependent packages and configure the server for Nomad.\n\n<ul>\n<li>The script is rendered using the <code>templatefile</code> function and populates the variable values in the script template with the values specified in the above resource:</li>\n<li> <code>NOMAD_SERVER_TAG</code>: for cloud auto-join</li>\n<li> <code>NOMAD_SERVER_TAG_KEY</code>: for cloud auto-join</li>\n<li> <code>NOMAD_SERVER_COUNT</code>: to specify how many servers Nomad expects to join the cluster</li>\n<li> <code>NOMAD_ADDR</code>: to configure Nomad’s OIDC discovery URL</li>\n</ul></li>\n<li>The data source used to obtain the AMI will always fetch the latest version. This means the VM could be deployed more if desired. The lifecycle block ignores changes specifically related to the AMI ID.</li>\n<li>It associates the security groups created above to the VM.</li>\n<li>It also adds the SSH key pair to the VM to aid in SSH authentication. This is useful for troubleshooting.</li>\n</ul>\n\n<h2>Configuring Nomad clients for the Vault servers</h2>\n\n<p>To deploy the Nomad clients for Vault, you take a similar approach to deploying the server. The main difference is the <code>cloud-init</code> script and the number of servers deployed. The script below (<code>client.sh</code>) is used to configure each Nomad client:</p>\n<pre><code>#! /bin/bash -e\n\n# Install the CNI Plugins\ncurl -L https://github.com/containernetworking/plugins/releases/download/v0.9.1/cni-plugins-linux-amd64-v0.9.1.tgz -o /tmp/cni.tgz\nmkdir -p /opt/cni/bin\ntar -C /opt/cni/bin -xzf /tmp/cni.tgz\n\n# Install Nomad\nsudo apt-get update &amp;&amp; \\\n  sudo apt-get install wget gpg coreutils -y\n\nwget -O- https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg\necho \"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/hashicorp.list\nsudo apt-get update &amp;&amp; sudo apt-get install nomad -y\n\n# Create Nomad directory.\nmkdir -p /etc/nomad.d\n\n# Create Vault directory.\nmkdir -p /etc/vault.d\n\n\n# Add Docker's official GPG key:\nsudo apt-get update\nsudo apt-get install ca-certificates curl -y\nsudo install -m 0755 -d /etc/apt/keyrings\nsudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc\nsudo chmod a+r /etc/apt/keyrings/docker.asc\n\n# Add the repository to Apt sources:\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\\n  $(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\") stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\n\n# Install Docker\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin -y\n\n# Install Java\nsudo apt install default-jre -y\n\n# Nomad configuration files\ncat &lt; /etc/nomad.d/nomad.hcl\nlog_level = \"DEBUG\"\ndata_dir = \"/etc/nomad.d/data\"\n\nclient {\n  enabled    = true\n  node_pool  = \"vault-servers\"\n  node_class = \"vault-servers\"\n\n  server_join {\n    retry_join = [\"${NOMAD_SERVERS_ADDR}\"]\n  }\n\n  host_volume \"/etc/vault.d\" {\n    path      = \"/etc/vault.d\"\n    read_only = false\n  }\n\n}\n\nplugin \"docker\" {\n  config {\n    allow_privileged = true\n  }\n}\n\nautopilot {\n    cleanup_dead_servers      = true\n    last_contact_threshold    = \"200ms\"\n    max_trailing_logs         = 250\n    server_stabilization_time = \"10s\"\n    enable_redundancy_zones   = false\n    disable_upgrade_migration = false\n    enable_custom_upgrades    = false\n}\n\n\nEOF\n\n cat &lt; /etc/nomad.d/acl.hcl\n acl = {\n   enabled = true\n }\nEOF\n\nsystemctl enable nomad\nsystemctl restart nomad</code></pre><p>The script above is similar to that of the Nomad server. One key difference is the Nomad client configuration file, which includes the following configuration items:</p>\n\n<ul>\n<li><strong>Node pool:</strong> This Nomad feature can group pieces of compute infrastructure together. In this case, you want dedicated servers to run your Vault cluster to reduce the security blast radius. Grouping these servers lets you specify what node pool the Vault servers should run on and create policies around this to prevent other Nomad jobs from being deployed to these client nodes.</li>\n<li><strong>Host volume:</strong> Vault can store encrypted secrets that other authorized Nomad jobs can retrieve. This means that Vault is a stateful workload that requires persistent storage. Host volumes expose a volume on the VM to Nomad and lets you mount the volume to a Nomad job. This means that if a job is restarted, it can still access its data.</li>\n<li><strong>Docker plugin:</strong> The Docker plugin is enabled because Vault will be run in a container as a Nomad job. This has the potential to ease the upgrade paths by simply changing the tag on the image used. </li>\n</ul>\n\n<p>Before deploying the Nomad clients, you need to perform a quick health check on the Nomad server to ensure it is available for clients to join it. To check this, use <a href=\"https://registry.terraform.io/providers/devops-rob/terracurl/latest\">TerraCurl</a> to make an API call to the Nomad server to check its status:</p>\n<pre><code>resource \"terracurl_request\" \"nomad_status\" {\n  method         = \"GET\"\n  name           = \"nomad_status\"\n  response_codes = [200]\n  url            = \"http://${aws_eip.nomad_server.public_ip}:4646/v1/status/leader\"\n  max_retry = 4\n  retry_interval = 10\n\n  depends_on = [\n    aws_instance.nomad_servers,\n    aws_eip_association.nomad_server\n  ]\n}</code></pre><p>This checks that Nomad has an elected leader in the cluster and expects a 200 response. If it does not get the desired response, TerraCurl will retry every 10 seconds for a maximum of 4 retries. This prevents potential race conditions between the Nomad server and the clients’ provisioning.</p>\n\n<p>Now you’re ready to deploy the Nomad client VMs. This is similar to the server deployed before, with a few key differences:</p>\n\n<ul>\n<li>Vault requires more compute power so the instance type is bigger.</li>\n<li>It uses the <code>count</code> feature because you need three Vault nodes.</li>\n<li>The script rendered by the <code>templatefile</code> function needs only one variable value this time (the Nomad server address).</li>\n</ul>\n<pre><code>resource \"aws_instance\" \"nomad_clients\" {\n  count                       = 3\n  ami                         = data.aws_ami.ubuntu.id\n  instance_type               = \"t3.medium\"\n  subnet_id                   = module.vpc.public_subnets.0\n  key_name                    = aws_key_pair.deployer.key_name\n  associate_public_ip_address = true\n\n  user_data = templatefile(\"./clients.sh\", {\n    NOMAD_SERVERS_ADDR = \"${aws_instance.nomad_servers.private_ip}\"\n  })\n\n  vpc_security_group_ids = [\n    aws_security_group.ssh.id,\n    aws_security_group.egress.id,\n    aws_security_group.nomad.id,\n    aws_security_group.vault.id\n  ]\n\n  tags = {\n    Name         = \"Vault on Nomad Client ${count.index + 1}\"\n    nomad_server = false\n  }\n\n  lifecycle {\n    ignore_changes = [\n      user_data,\n      ami\n    ]\n  }\n\n  depends_on = [\n    terracurl_request.nomad_status\n  ]\n\n}</code></pre><h2>Configuration of the Nomad client for the Vault backup server</h2>\n\n<p>The next piece of infrastructure to deploy is the VM used as the Vault backup server. The server makes backups of the Vault cluster. It’s best practice to store backups away from the Vault cluster, so create a separate node pool for the backup server. Run the <code>vault-backup-server.sh</code> script below, which is located in the same directory you’ve been working in so far:</p>\n<pre><code>#! /bin/bash -e\n\n# Install the CNI Plugins\ncurl -L https://github.com/containernetworking/plugins/releases/download/v0.9.1/cni-plugins-linux-amd64-v0.9.1.tgz -o /tmp/cni.tgz\nmkdir -p /opt/cni/bin\ntar -C /opt/cni/bin -xzf /tmp/cni.tgz\n\n# Install Nomad\nsudo apt-get update &amp;&amp; \\\n  sudo apt-get install wget gpg coreutils -y\n\nwget -O- https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg\necho \"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/hashicorp.list\nsudo apt-get update &amp;&amp; sudo apt-get install nomad -y\n\n# Create Nomad directory.\nmkdir -p /etc/nomad.d\n\n# Install Vault\nsudo apt-get install vault -y\n\n# Create Vault directory.\nmkdir -p /etc/vault.d\n\n\n\n\n# Add Docker's official GPG key:\nsudo apt-get update\nsudo apt-get install ca-certificates curl -y\nsudo install -m 0755 -d /etc/apt/keyrings\nsudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc\nsudo chmod a+r /etc/apt/keyrings/docker.asc\n\n# Add the repository to Apt sources:\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\\n  $(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\") stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\n\n# Install Docker\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin -y\n\n# Install Java\nsudo apt install default-jre -y\n\n# Nomad configuration files\ncat &lt; /etc/nomad.d/nomad.hcl\nlog_level = \"DEBUG\"\ndata_dir = \"/etc/nomad.d/data\"\n\nclient {\n  enabled    = true\n  node_pool  = \"vault-backup\"\n  node_class = \"vault-backup\"\n\n  server_join {\n    retry_join = [\"${NOMAD_SERVERS_ADDR}\"]\n  }\n\n  host_volume \"vault_vol\" {\n    path      = \"/etc/vault.d\"\n    read_only = false\n  }\n\n}\n\nplugin \"docker\" {\n  config {\n    allow_privileged = true\n  }\n}\n\nautopilot {\n    cleanup_dead_servers      = true\n    last_contact_threshold    = \"200ms\"\n    max_trailing_logs         = 250\n    server_stabilization_time = \"10s\"\n    enable_redundancy_zones   = false\n    disable_upgrade_migration = false\n    enable_custom_upgrades    = false\n}\n\n\nEOF\n\n cat &lt; /etc/nomad.d/acl.hcl\n acl = {\n   enabled = true\n }\nEOF\n\nsystemctl restart nomad</code></pre><p>Next, you need to actually deploy the VM using the code below, which is added to the <code>compute.tf</code> file:</p>\n<pre><code>resource \"aws_instance\" \"nomad_clients_vault_backup\" {\n  count                       = 1\n  ami                         = data.aws_ami.ubuntu.id\n  instance_type               = \"t3.medium\"\n  subnet_id                   = module.vpc.public_subnets.0\n  key_name                    = aws_key_pair.deployer.key_name\n  associate_public_ip_address = true\n\n  user_data = templatefile(\"./vault-backup-server.sh\", {\n    NOMAD_SERVERS_ADDR = \"${aws_instance.nomad_servers.private_ip}\"\n  })\n\n  vpc_security_group_ids = [\n    aws_security_group.ssh.id,\n    aws_security_group.egress.id,\n    aws_security_group.nomad.id,\n    aws_security_group.vault.id\n  ]\n\n  tags = {\n    Name         = \"Vault backup server\"\n    nomad_server = false\n  }\n\n  lifecycle {\n    ignore_changes = [\n      user_data,\n      ami\n    ]\n  }\n\n  depends_on = [\n    terracurl_request.nomad_status\n  ]\n\n}</code></pre><h2>Nomad ACL configuration</h2>\n\n<p>As part of the Nomad server configuration deployed by the cloud-init script, ACLs were enabled. This means that an access token is required before any actions can be performed. As this is a new install of Nomad, a token does not exist yet.</p>\n\n<p>To bootstrap Nomad’s ACL system with the initial management token, you can use Nomad’s API and TerraCurl. TerraCurl is useful in this scenario because the <a href=\"https://registry.terraform.io/providers/hashicorp/nomad/latest/docs\">Nomad Terraform provider</a> does not support this bootstrapping functionality. You can write the TerraCurl resource to bootstrap the Nomad ACL system. There is a file in the <code>1-nomad-infrastructure</code> directory called <code>nomad.tf</code> that includes the following code:</p>\n<pre><code>resource \"terracurl_request\" \"bootstrap_acl\" {\n  method         = \"POST\"\n  name           = \"bootstrap\"\n  response_codes = [200, 201]\n  url            = \"http://${aws_instance.nomad_servers.public_ip}:4646/v1/acl/bootstrap\"\n}</code></pre><p>This code makes a POST API call to the Nomad server using the public IP address that was assigned to the VM during creation. It uses Terraform’s interpolation and joins that to the Nomad API endpoint <code>/v1/acl/bootstrap</code> to make the call. This resource tells Terraform to expect either a <code>200</code> or <code>201</code>response code from the API call or Terraform will fail. The response body from the API call is stored in state.</p>\n\n<h1>Terraform outputs</h1>\n\n<p>In order to provide some of the computed values to other workspaces, you need to output them. To do this, create a file called <code>outputs.tf</code> in the same directory as above and insert the following code:</p>\n<pre><code>output \"nomad_server_public_ip\" {\n  value = aws_eip.nomad_server.public_ip\n}\n\noutput \"nomad_server_private_ip\" {\n  value = aws_instance.nomad_servers.private_ip\n}\n\noutput \"nomad_clients_private_ips\" {\n  value = aws_instance.nomad_clients.*.private_ip\n}\n\noutput \"nomad_clients_public_ips\" {\n  value = aws_instance.nomad_clients.*.public_ip\n}\n\n\noutput \"terraform_management_token\" {\n  value     = nomad_acl_token.terraform.secret_id\n  sensitive = true\n}\n\noutput \"nomad_ui\" {\n  value = \"http://${aws_eip.nomad_server.public_ip}:4646\"\n}</code></pre><p>The directory structure should now look like this:</p>\n\n<pre><code>├── 1-nomad-infrastrcuture\n│         ├── anonymous-policy.hcl\n│         ├── backend.tf\n│         ├── client-policy.hcl\n│         ├── clients.sh\n│         ├── compute.tf\n│         ├── network.tf\n│         ├── nomad-client-vault-backup.sh\n│         ├── nomad.tf\n│         ├── outputs.tf\n│         ├── providers.tf\n│         ├── servers.sh\n│         └── variables.tf\n├── 2-nomad-configuration\n│         ├── backend.tf\n└── 3-nomad-job-example-deployment\n          └── backend.tf\n\n3 directories, 14 files\n</code></pre>\n\n<p>Now you can run <code>terraform plan</code> and <code>terraform apply</code> to create these resources.</p>\n\n<h2>In the next blog</h2>\n\n<p>This blog post showed how to deploy the infrastructure required to run Vault on Nomad. It covered some Terraform directory structure concepts and how they relate to workspaces. It also covered deploying and configuring Nomad, as well as Nomad ACLs. <a href=\"https://www.hashicorp.com/blog/running-vault-on-nomad-part-2\">Part 2</a> of this blog series will look at deploying Vault as a Nomad job and configuring it, while Part 3 will explore deploying some automation to assist in the day-to-day operations of Vault. </p>\n","author":"Rob Barnes","siteTitle":"HashiCorp Blog","siteHash":"219aa6310b3388f2335eba49871f4df9581f2c58eaeb5e498363b54e835b7001","entryHash":"6e148ad0795200f9d717ff8abe9e9be6ea2a4e83ba9b5c5d780c62143ad44453","category":"Tech"}