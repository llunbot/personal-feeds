{"title":"Introducing Llama 3.2 models from Meta in Amazon Bedrock: A new generation of multimodal vision and lightweight models","link":"https://aws.amazon.com/blogs/aws/introducing-llama-3-2-models-from-meta-in-amazon-bedrock-a-new-generation-of-multimodal-vision-and-lightweight-models/","date":1727288332000,"content":"<p>In July, we <a href=\"https://aws.amazon.com/blogs/aws/announcing-llama-3-1-405b-70b-and-8b-models-from-meta-in-amazon-bedrock/\">announced the availability of Llama 3.1 models in Amazon Bedrock</a>. <a href=\"https://aws.amazon.com/ai/generative-ai/\">Generative AI</a> technology is improving at incredible speed and today, we are excited to introduce the new <a href=\"https://aws.amazon.com/bedrock/llama/\">Llama 3.2 models from Meta</a> in <a href=\"https://aws.amazon.com/bedrock/\">Amazon Bedrock</a>.</p> \n<p>Llama 3.2 offers multimodal vision and lightweight models representing Meta’s latest advancement in <a href=\"https://aws.amazon.com/what-is/large-language-model/\">large language models (LLMs)</a> and providing enhanced capabilities and broader applicability across various use cases. With a focus on <a href=\"https://aws.amazon.com/ai/responsible-ai/\">responsible innovation and system-level safety</a>, these new models demonstrate state-of-the-art performance on a wide range of industry benchmarks and introduce features that help you build a new generation of AI experiences.</p> \n<p>These models are designed to inspire builders with image reasoning and are more accessible for edge applications, unlocking more possibilities with AI.</p> \n<p>The Llama 3.2 collection of models are offered in various sizes, from lightweight text-only 1B and 3B parameter models suitable for edge devices to small and medium-sized 11B and 90B parameter models capable of sophisticated reasoning tasks including multimodal support for high resolution images. Llama 3.2 11B and 90B are the first Llama models to support vision tasks, with a new model architecture that integrates image encoder representations into the language model. The new models are designed to be more efficient for AI workloads, with reduced latency and improved performance, making them suitable for a wide range of applications.</p> \n<p>All Llama 3.2 models support a 128K context length, maintaining the expanded token capacity introduced in Llama 3.1. Additionally, the models offer improved multilingual support for eight languages including English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.</p> \n<p>In addition to the existing text capable <a href=\"https://aws.amazon.com/blogs/aws/announcing-llama-3-1-405b-70b-and-8b-models-from-meta-in-amazon-bedrock/\">Llama 3.1 8B, 70B, and 405B models</a>, Llama 3.2 supports multimodal use cases. You can now use four new Llama 3.2 models — 90B, 11B, 3B, and 1B — from Meta in Amazon Bedrock to build, experiment, and scale your creative ideas:</p> \n<p><strong>Llama 3.2 90B Vision (text + image input)</strong> – Meta’s most advanced model, ideal for enterprise-level applications. This model excels at general knowledge, long-form text generation, multilingual translation, coding, math, and advanced reasoning. It also introduces image reasoning capabilities, allowing for image understanding and visual reasoning tasks. This model is ideal for the following use cases: image captioning, image-text retrieval, visual grounding, visual question answering and visual reasoning, and document visual question answering.</p> \n<p><strong>Llama 3.2 11B Vision (text + image input)</strong> – Well-suited for content creation, conversational AI, language understanding, and enterprise applications requiring visual reasoning. The model demonstrates strong performance in text summarization, sentiment analysis, code generation, and following instructions, with the added ability to reason about images. This model use cases are similar to the 90B version: image captioning, image-text-retrieval, visual grounding, visual question answering and visual reasoning, and document visual question answering.</p> \n<p><strong>Llama 3.2 3B (text input)</strong> – Designed for applications requiring low-latency inferencing and limited computational resources. It excels at text summarization, classification, and language translation tasks. This model is ideal for the following use cases: mobile AI-powered writing assistants and customer service applications.</p> \n<p><strong>Llama 3.2 1B (text input)</strong> – The most lightweight model in the Llama 3.2 collection of models, perfect for retrieval and summarization for edge devices and mobile applications. This model is ideal for the following use cases: personal information management and multilingual knowledge retrieval.</p> \n<p>In addition, Llama 3.2 is built on top of the <a href=\"https://github.com/meta-llama/llama-stack\">Llama Stack</a>, a standardized interface for building canonical toolchain components and agentic applications, making building and deploying easier than ever. Llama Stack API adapters and distributions are designed to most effectively leverage the Llama model capabilities and it gives customers the ability to benchmark Llama models across different vendors.</p> \n<p>Meta has tested Llama 3.2 on over 150 benchmark datasets spanning multiple languages and conducted extensive human evaluations, demonstrating competitive performance with other leading foundation models. Let’s see how these models work in practice.</p> \n<p><span><strong>Using Llama 3.2 models in Amazon Bedrock</strong></span><br /> To get started with Llama 3.2 models, I navigate to the <a href=\"https://console.aws.amazon.com/bedrock/home\">Amazon Bedrock console</a> and choose <strong>Model access</strong> on the navigation pane. There, I request access for the new Llama 3.2 models: Llama 3.2 1B, 3B, 11B Vision, and 90B Vision.</p> \n<p>To test the new vision capability, I open another browser tab and download from the <a href=\"https://ourworldindata.org/\">Our World in Data website</a> the <a href=\"https://ourworldindata.org/grapher/share-electricity-renewables\">Share of electricity generated by renewables</a> chart in PNG format. The chart is very high resolution and I resize it to be 1024 pixel wide.</p> \n<p>Back in the Amazon Bedrock console, I choose <strong>Chat</strong> under <strong>Playgrounds</strong> in the navigation pane, select <strong>Meta</strong> as the category, and choose the <strong>Llama 3.2 90B Vision</strong> model.</p> \n<p>I use <strong>Choose files</strong> to select the resized chart image and use this prompt:</p> \n<p><code>Based on this chart, which countries in Europe have the highest share?</code></p> \n<p>I choose <strong>Run</strong> and the model analyzes the image and returns its results:</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/09/25/aws-bedrock-meta-3-2-console-chat-image.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/09/25/aws-bedrock-meta-3-2-console-chat-image.png\" alt=\"Using Meta Llama 3.2 models in the Amazon Bedrock console\" width=\"2474\" height=\"1546\" /></a></p> \n<p>I can also access the models programmatically using the <a href=\"https://aws.amazon.com/cli/\">AWS Command Line Interface (AWS CLI)</a> and <a href=\"https://aws.amazon.com/tools/\">AWS SDKs</a>. Compared to using the Llama 3.1 models, I only need to <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html\">update the model IDs as described in the documentation</a>. I can also use the new <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/cross-region-inference-support.html\">cross-region inference endpoint</a> for the US and the EU Regions. These endpoints work for any Region within the US and the EU respectively. For example, the cross-region inference endpoints for the Llama 3.2 90B Vision model are:</p> \n<ul> \n <li><code>us.meta.llama3-2-90b-instruct-v1:0</code></li> \n <li><code>eu.meta.llama3-2-90b-instruct-v1:0</code></li> \n</ul> \n<p>Here’s a sample AWS CLI command using the <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html\">Amazon Bedrock Converse API</a>. I use the <code>--query</code> parameter of the CLI to filter the result and only show the text content of the output message:</p> \n<div> \n <pre><code>aws bedrock-runtime converse --messages '[{ \"role\": \"user\", \"content\": [ { \"text\": \"Tell me the three largest cities in Italy.\" } ] }]' --model-id us.meta.llama3-2-90b-instruct-v1:0 --query 'output.message.content[*].text' --output text</code></pre> \n</div> \n<p>In output, I get the response message from the <code>\"assistant\"</code>.</p> \n<div> \n <pre><code>The three largest cities in Italy are:\n\n1. Rome (Roma) - population: approximately 2.8 million\n2. Milan (Milano) - population: approximately 1.4 million\n3. Naples (Napoli) - population: approximately 970,000</code></pre> \n</div> \n<p>It’s not much different if you use one of the AWS SDKs. For example, here’s how you can use Python with the <a href=\"https://aws.amazon.com/sdk-for-python/\">AWS SDK for Python (Boto3)</a> to analyze the same image as in the console example:</p> \n<pre><code>import boto3\n\nMODEL_ID = \"us.meta.llama3-2-90b-instruct-v1:0\"\n# MODEL_ID = \"eu.meta.llama3-2-90b-instruct-v1:0\"\n\nIMAGE_NAME = \"share-electricity-renewable-small.png\"\n\nbedrock_runtime = boto3.client(\"bedrock-runtime\")\n\nwith open(IMAGE_NAME, \"rb\") as f:\n    image = f.read()\n\nuser_message = \"Based on this chart, which countries in Europe have the highest share?\"\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"image\": {\"format\": \"png\", \"source\": {\"bytes\": image}}},\n            {\"text\": user_message},\n        ],\n    }\n]\n\nresponse = bedrock_runtime.converse(\n    modelId=MODEL_ID,\n    messages=messages,\n)\nresponse_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\nprint(response_text)</code></pre> \n<p><a href=\"https://aws.amazon.com/blogs/machine-learning/llama-3-2-models-from-meta-are-now-available-in-amazon-sagemaker-jumpstart/\">Llama 3.2 models are also available in Amazon SageMaker JumpStart</a>, a <a href=\"https://aws.amazon.com/ai/machine-learning/\">machine learning (ML)</a> hub that makes it easy to deploy pre-trained models using the console or programmatically through the <a href=\"https://sagemaker.readthedocs.io/\">SageMaker Python SDK</a>. From SageMaker JumpStart, you can also access and deploy new safeguard models that can help classify the safety level of model inputs (prompts) and outputs (responses), including Llama Guard 3 11B Vision, which are designed to support responsible innovation and system-level safety.</p> \n<p>In addition, you can easily fine-tune Llama 3.2 1B and 3B models with SageMaker JumpStart today. Fine-tuned models can then be <a href=\"https://aws.amazon.com/blogs/aws/import-custom-models-in-amazon-bedrock-preview/\">imported as custom models into Amazon Bedrock</a>. Fine-tuning for the full collection of Llama 3.2 models in Amazon Bedrock and Amazon SageMaker JumpStart is coming soon.</p> \n<p>The publicly available weights of Llama 3.2 models make it easier to deliver tailored solutions for custom needs. For example, you can fine-tune a Llama 3.2 model for a specific use case and <a href=\"https://aws.amazon.com/blogs/aws/import-custom-models-in-amazon-bedrock-preview/\">bring it into Amazon Bedrock as a custom model</a>, potentially outperforming other models in domain-specific tasks. Whether you’re fine-tuning for enhanced performance in areas like content creation, language understanding, or visual reasoning, Llama 3.2’s availability in Amazon Bedrock and SageMaker empowers you to create unique, high-performing AI capabilities that can set your solutions apart.</p> \n<p><span><strong>More on Llama 3.2 model architecture<br /> </strong></span>Llama 3.2 builds upon the success of its predecessors with an advanced architecture designed for optimal performance and versatility:</p> \n<p><a href=\"https://aws.amazon.com/what-is/autoregressive-models/\"><strong>Auto-regressive language model</strong></a> – At its core, Llama 3.2 uses an optimized transformer architecture, allowing it to generate text by predicting the next token based on the previous context.</p> \n<p><strong>Fine-tuning techniques</strong> – The instruction-tuned versions of Llama 3.2 employ two key techniques:</p> \n<ul> \n <li>Supervised fine-tuning (SFT) – This process adapts the model to follow specific instructions and generate more relevant responses.</li> \n <li><a href=\"https://aws.amazon.com/what-is/reinforcement-learning-from-human-feedback/\">Reinforcement learning with human feedback (RLHF)</a> – This advanced technique aligns the model’s outputs with human preferences, enhancing helpfulness and safety.</li> \n</ul> \n<p><strong>Multimodal capabilities</strong> – For the 11B and 90B Vision models, Llama 3.2 introduces a novel approach to image understanding:</p> \n<ul> \n <li>Separately trained image reasoning adaptor weights are integrated with the core LLM weights.</li> \n <li>These adaptors are connected to the main model through cross-attention mechanisms. Cross-attention allows one section of the model to focus on relevant parts of another component’s output, enabling information flow between different sections of the model.</li> \n <li>When an image is input, the model treats the image reasoning process as a “tool use” operation, allowing for sophisticated visual analysis alongside text processing. In this context, tool use is the generic term used when a model uses external resources or functions to augment its capabilities and complete tasks more effectively.</li> \n</ul> \n<p><strong>Optimized inference</strong> – All models support grouped-query attention (GQA), which enhances inference speed and efficiency, particularly beneficial for the larger 90B model.</p> \n<p>This architecture enables Llama 3.2 to handle a wide range of tasks, from text generation and understanding to complex reasoning and image analysis, all while maintaining high performance and adaptability across different model sizes.</p> \n<p><span><strong>Things to know</strong></span><br /> <a href=\"https://aws.amazon.com/bedrock/llama/\">Llama 3.2 models from Meta</a> are now generally available in <a href=\"https://aws.amazon.com/bedrock/\">Amazon Bedrock</a> in the following <a href=\"https://aws.amazon.com/about-aws/global-infrastructure/regions_az/\">AWS Regions</a>:</p> \n<ul> \n <li>Llama 3.2 1B and 3B models are available in the US West (Oregon) and Europe (Frankfurt) Regions, and are available in the US East (Ohio, N. Virginia) and Europe (Ireland, Paris) Regions via <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/cross-region-inference-support.html\">cross-region inference</a>.</li> \n <li>Llama 3.2 11B Vision and 90B Vision models are available in the US West (Oregon) Region, and are available in the US East (Ohio, N. Virginia) Regions via <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/cross-region-inference-support.html\">cross-region inference</a>.</li> \n</ul> \n<p>Check the <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/bedrock-regions.html\">full AWS Region list</a> for future updates. To estimate your costs, visit the <a href=\"https://aws.amazon.com/bedrock/pricing/\">Amazon Bedrock pricing page</a>.</p> \n<p>To lean more about how you can use Llama 3.2 11B and 90B models to support vision tasks, read the <a href=\"https://aws.amazon.com/blogs/machine-learning/vision-use-cases-with-llama-3-2-11b-and-90b-models-from-meta/\">Vision use cases with Llama 3.2 11B and 90B models from Meta</a> post on the AWS Machine Learning blog channel.</p> \n<p>AWS and Meta are also collaborating to bring smaller Llama models to on-device applications, featuring the new 1B and 3B models. For more information, see the <a href=\"https://aws.amazon.com/blogs/industries/opportunities-for-telecoms-with-small-language-models/\">Opportunities for telecoms with small language models: Insights from AWS and Meta</a> post on the AWS for Industries blog channel.</p> \n<p>To learn more about Llama 3.2 features and capabilities, visit the <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-meta.html\">Llama models section of the Amazon Bedrock documentation</a>. Give Llama 3.2 a try in the <a href=\"https://console.aws.amazon.com/bedrock/home\">Amazon Bedrock console</a> today, and send feedback to <a href=\"https://repost.aws/tags/TAQeKlaPaNRQ2tWB6P7KrMag/amazon-bedrock\">AWS re:Post for Amazon Bedrock</a>.</p> \n<p>You can find deep-dive technical content and discover how our Builder communities are using Amazon Bedrock at <a href=\"https://community.aws/generative-ai?trk=e8665609-785f-4bbe-86e8-750a3d3e9e61&amp;sc_channel=el\">community.aws</a>. Let us know what you build with Llama 3.2 in Amazon Bedrock!</p> \n<p>— <a href=\"https://twitter.com/danilop\">Danilo</a></p>","author":"Danilo Poccia","siteTitle":"AWS News Blog","siteHash":"6093e072e4117ec22616e844cb857d03ca62c57a411a8affc77cb5e8b6b15bf6","entryHash":"96829b8cd6984b4a0b970fb0442abbe41fd1e9493132db8a02a60c5ff09d4856","category":"Tech"}