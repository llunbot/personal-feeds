{"title":"Google claims math breakthrough with proof-solving AI models","link":"https://arstechnica.com/?p=2039221","date":1721944492000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/07/alphaproof_hero-800x450.jpg\" alt=\"An illustration provided by Google.\" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2024/07/alphaproof_hero.jpg\">Enlarge</a> <span>/</span> An illustration provided by Google. (credit: <a href=\"https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/\">Google</a>)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>On Thursday, Google DeepMind <a href=\"https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/\">announced</a> that AI systems called AlphaProof and AlphaGeometry 2 reportedly solved four out of six problems from this year's <a href=\"https://www.imo-official.org/\">International Mathematical Olympiad</a> (IMO), achieving a score equivalent to a silver medal. The tech giant claims this marks the first time an AI has reached this level of performance in the prestigious math competitionâ€”but as usual in AI, the claims aren't as clear-cut as they seem.</p>\n\n<p>Google says AlphaProof uses reinforcement learning to prove mathematical statements in the formal language called <a href=\"https://en.wikipedia.org/wiki/Lean_(proof_assistant)\">Lean</a>. The system trains itself by generating and verifying millions of proofs, progressively tackling more difficult problems. Meanwhile, AlphaGeometry 2 is described as an upgraded version of Google's <a href=\"https://arstechnica.com/ai/2024/01/deepmind-ai-rivals-the-worlds-smartest-high-schoolers-at-geometry/\">previous geometry-solving AI modeI</a>, now powered by a Gemini-based language model trained on significantly more data.</p>\n<p>According to Google, prominent mathematicians <a href=\"https://en.wikipedia.org/wiki/Timothy_Gowers\">Sir Timothy Gowers</a> and <a href=\"https://www.friends.edu/staff/myers-joseph/\">Dr. Joseph Myers</a> scored the AI model's solutions using official IMO rules. The company reports its combined system earned 28 out of 42 possible points, just shy of the 29-point gold medal threshold. This included a perfect score on the competition's hardest problem, which Google claims only five human contestants solved this year.</p></div><p><a href=\"https://arstechnica.com/?p=2039221#p3\">Read 9 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=2039221&amp;comments=1\">Comments</a></p>","author":"Benj Edwards","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"3725da20b000ee365fa799acc71a15a957e505ba1a904acad1dbcca029f13ef4","category":"Tech"}