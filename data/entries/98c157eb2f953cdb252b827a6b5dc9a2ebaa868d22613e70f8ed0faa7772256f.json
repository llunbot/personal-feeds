{"title":"EXO Labs ทดสอบ Mac Studio อัดแรมเต็มสองเครื่อง รัน DeepSeek-R1 ตัวเต็มได้ 11 token/s","link":"https://www.blognone.com/node/145253","date":1741888531000,"content":"<div><div><div><p>EXO Labs ผู้พัฒนาซอฟต์แวร์คลัสเตอร์สำหรับรันปัญญาประดิษฐ์ รายงานถึงผลทดสอบของ Mac Studio ที่ใช้ชิป M3 Ultra พร้อมกับแรม 512GB สองเครื่อง สามารถรันโมเดล DeepSeek-R1 ตัวเต็มที่ 671B FP8 ได้ด้วยความเร็ว 11 token/s</p>\n<p>M3 Ultra มีความได้เปรียบสำหรับการรันปัญญาประดิษฐ์ในบ้าน เพราะรองรับแรมแบบ unified memory ขนาดใหญ่, มีแบนวิดท์หน่วยความจำสูง, และในเวอร์ชั่นนี้ยังรองรับ Thunderbolt 5 ที่แบนวิดท์สูงขึ้นเป็น 120Gb/s <a href=\"https://www.blognone.com/node/145105\">ตัวแอปเปิลเองถึงกับโฆษณาความเร็วในการรัน LLM ไว้ด้วย</a></p>\n<p>โดยเฉลี่ยแล้วความเร็ว 11 token/s ประมาณได้ว่าเป็นการพิมพ์ 40-50 ตัวอักษรต่อวินาทีซึ่งก็น่าจะเพียงพอต่อการแชตทั่วไป แต่ในกรณีโมเดลคิดก่อนตอบ เช่น R1 นั้นประสิทธิภาพจะช้ามากก่อนได้คำตอบ เนื่องจากโมเดลเสียเวลาคิดอยู่ช่วงหนึ่ง</p>\n<p>Alex Cheema จาก EXO Labs ระบุว่าความเร็วทางทฤษฎีน่าจะไปได้ถึง 20 token/s และหลังจากนั้นน่าจะหาทางปรับปรุงประสิทธิภาพทางอื่น เช่น expert parallelism ซึ่งอาจจะดันไปได้ถึง 40 token/s นอกจากนี้หากย่อโมเดลลงให้กลายเป็น Q6_K น่าจะย่อโมเดลได้เหลือ 500GB ซึ่งจะรันใน Mac Studio เครื่องเดียวได้ (แรมเกือบหมดทันที) Cheema ระบุว่าเขาจะเอามาทดสอบต่อไป</p>\n<p>ที่มา - <a href=\"https://x.com/alexocheema/status/1899739071192441235\">@alexocheema</a></p>\n<p><img src=\"https://www.blognone.com/sites/default/files/externals/dc400b545beb6459e70a4e32b1d3c818.jpg\" alt=\"\" /></p>\n</div></div></div><div><div>Topics: </div><div><div><a href=\"/topics/artificial-intelligence\">Artificial Intelligence</a></div><div><a href=\"/topics/llm\">LLM</a></div><div><a href=\"/topics/apple-m4\">Apple M4</a></div><div><a href=\"/topics/deepseek\">DeepSeek</a></div></div></div>","author":"lew","siteTitle":"Blognone","siteHash":"ededadcf18490b3937e7dd89ebe8c00dc129addbdf1ebe4aff1f458146693da0","entryHash":"98c157eb2f953cdb252b827a6b5dc9a2ebaa868d22613e70f8ed0faa7772256f","category":"Thai"}