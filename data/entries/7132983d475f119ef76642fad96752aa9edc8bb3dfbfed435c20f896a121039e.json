{"title":"Running dynamic, ephemeral multi-hop workers for HCP Boundary: Part 2","link":"https://www.hashicorp.com/blog/running-dynamic-ephemeral-multi-hop-workers-for-hcp-boundary-part-2","date":1690484400000,"content":"<p><a href=\"https://www.hashicorp.com/blog/running-dynamic-ephemeral-multi-hop-workers-for-hcp-boundary-part-1\">Part 1</a> of this blog series explored the challenges of running HashiCorp Boundary workers as dynamic workloads such as auto scaling groups, Nomad jobs, and Kubernetes deployments. The post also showed how the <a href=\"https://github.com/devops-rob/vault-plugin-boundary-secrets-engine\">custom Boundary secrets engine for Vault</a> can be used to generate worker activation tokens and manage the lifecycle of workers alongside their dynamic deployment models. </p>\n\n<p>This post concludes the series by  walking you through the steps required to run Boundary workers as dynamic workloads in Nomad using the custom Boundary secrets engine for Vault.</p>\n\n<h2>Nomad</h2>\n\n<p><a href=\"https://www.nomadproject.io/\">HashiCorp Nomad</a> is a workload scheduler and orchestrator that enables engineers to deploy both containerized and non-containerized workloads. Workloads can be deployed as binaries, executables, Java ARchive (JAR) files, or Docker containers to multiple platforms including Windows and Linux machines.</p>\n\n<p>The domain architecture of Nomad has many components, but this blog post focuses on the following elements:</p>\n\n<ul>\n<li>Nomad server: This is responsible for scheduling workloads.</li>\n<li>Nomad client: This is the platform that runs the workloads, and could be Windows, macOS, or Linux. For Docker and JAR files to be scheduled to a client, it must have Docker and Java installed, respectively.</li>\n<li>Nomad job: This is a declarative definition of a workload. It contains information about resources required, storage and volume mounting definitions, environment variables, and container definition, etc.</li>\n</ul>\n\n<h2>Vault integration on Nomad servers clients</h2>\n\n<p>Nomad and Vault can be seamlessly integrated to provide Nomad jobs with secrets retrieved from Vault. Secrets retrieved from Vault by Nomad can then be rendered to a file using Nomad's templating capabilities. This is ideal for running the Boundary worker as a Nomad job because you need to have a config file for the worker. This templating capability allows you to render the config file for the Boundary worker and inject the dynamic worker activation token retrieved from Vault.</p>\n\n<p>The first step for this integration to work is to configure Vault tokens and access policies for Nomad to use when communicating with Vault. The specifics of this can be found in the <a href=\"https://developer.hashicorp.com/nomad/docs/integrations/vault-integration\">Vault integration documentation</a>. (Note: A root token can also be used, however, this is not advised for production workloads.)</p>\n\n<p>Once you have a token to use with Nomad, the next step is to configure the Nomad servers with the Vault integration. This is done by adding a Vault stanza to the config file of the Nomad servers:</p>\n<pre><code>vault {\nenabled = true\naddress = \"https://vault.service.consul:8200\"\ntoken = REDACTED\ncreate_from_role = \"nomad-cluster\"\n}</code></pre><p><em>**Note: The snippet above is an example Vault stanza in a Nomad server config file, which includes the Vault token. This approach is not recommended for production workloads. Instead, this can be omitted from the config file and written to the `VAULT</em>TOKEN` environment variable.**_</p>\n\n<p>Here is an example Vault stanza in the Nomad client config file:</p>\n<pre><code>vault {\nenabled = true\naddress = \"https://vault.service.consul:8200\"\n}</code></pre><p>The token needs to be configured only on the Nomad servers. Clients will have tokens created for them automatically by the Nomad servers using the role specified in the server config.</p>\n\n<h2>Running Boundary workers as Nomad jobs</h2>\n\n<p>Defining a Boundary worker in a Nomad job is no different than most other Nomad jobs. The key is to generate a Boundary worker config file on startup of the job. Here is an example Boundary worker config file:</p>\n<pre><code>hcp_boundary_cluster_id = \"my_cluster_id\"\n\nlistener \"tcp\" {\n  address = \"0.0.0.0:9202\"\n  purpose = \"proxy\"\n}\n\nworker {\n  auth_storage_path = \"/boundary/auth_data\"\n  controller_generated_activation_token = \"my_token\"\n  tags {\n    type = [\"frontend\"]\n  }\n}\n</code></pre><p>The key configuration parameter  is the <code>controller_generated_activation_token</code>, which we want to fetch from Vault using our <a href=\"https://github.com/devops-rob/vault-plugin-boundary-secrets-engine\">custom plugin</a>.\nIn order to use this, we need to create a Vault policy for the worker that allows permissions to be read from the secrets engine. The policy shown here will grant the relevant permissions to generate an activation token:</p>\n<pre><code>path = \"boundary/creds/worker\" {\n  capabilities = [\"read\", \"update\"]\n}</code></pre><p>Create a file called <code>worker_policy.hcl</code> with the above policy written to it. Add the file to Vault using this command:</p>\n\n<pre><code>vault policy write boundary-worker worker_policy.hcl\n</code></pre>\n\n<p>All the prerequisites for this workflow are now in place, so it's time to create the Nomad job file. We can start with the base <code>nomad_worker.hcl</code> file below:</p>\n<pre><code>job \"boundary_worker\" {\n  datacenters = [\"dc1\"]\n\n\n  type = \"service\"\n\n\n  group \"worker\" {\n    count = 1\n\n\n    restart {\n    # The number of attempts to run the job within the specified interval.\n      attempts = 2\n      interval = \"30m\"\n      delay = \"15s\"\n      mode = \"fail\"\n    }\n\n\n    ephemeral_disk {\n      size = 30\n    }\n\n\n    task \"worker\" {\n      driver = \"docker\"\n\n\n      logs {\n        max_files = 2\n        max_file_size = 10\n      }\n\n\n      resources {\n        cpu = 500 # 500 MHz\n        memory = 512 # 512MB\n      }\n    }\n  }\n}\n</code></pre><p>You need to add a Vault stanza to your job file. This stanza tells Nomad which Vault policy to use when retrieving your activation token. Specify the policy you just created:</p>\n\n<pre><code>vault {\n  policies = [\"boundary-worker\"]\n}\n</code></pre>\n\n<p>Next, add the worker template to the Nomad job:</p>\n<pre><code>template {\n  data = &lt;&lt;-EOF\ndisable_mlock = true\n\n\nhcp_boundary_cluster_id = \"739d93f9-7f1c-474d-8524-931ab199eaf8\"\n\n\nlistener \"tcp\" {\n  address = \"0.0.0.0:9202\"\n  purpose = \"proxy\"\n}\n\n\nworker {\n  auth_storage_path=\"/boundary/auth_data\"\n  {{with secret \"boundary/creds/worker\" (env \"NOMAD_ALLOC_ID\" | printf \"worker_name=%s\") -}}\n  controller_generated_activation_token = \"{{.Data.activation_token}}\"\n  {{- end}}\n\n\n  tags {\n    environment = [\"nomad\"]\n  }\n}\nEOF\n\n\ndestination = \"local/config.hcl\"\n}</code></pre><p>In order to populate this template with a value read from Vault, use the <code>{{With secret \"Vault_path_toSecret\"}}</code> variable, which will pull the secret from Vault. This secrets engine requires a unique name for each worker node, so youâ€™ll use  the Nomad allocation ID to provide those. Nomad exposes this through the <code>NOMAD_ALLOC_ID</code> environment variable.</p>\n\n<pre><code>{{with secret \"boundary/creds/worker\" (env \"NOMAD_ALLOC_ID\" | printf \"worker_name=%s\") -}}\n</code></pre>\n\n<p>The code above reads the environment variable and prints it to <code>worker_name=</code>. To generate the activation token, add this line:</p>\n\n<pre><code>controller_generated_activation_token = \"{{.Data.activation_token}}\"\n</code></pre>\n\n<p>This line of the template populates the <code>controller_generated_activation_token</code> configuration parameter with the activation token retrieved from Vault. When that path is called within Vault, the response will contain an object called <code>data</code>, which contains an <code>activation_token</code> key/value pair. When the template is rendered, it is written to a file called <code>local/config.hcl</code>.</p>\n\n<p>The final part of the Nomad job to add is the config stanza. This specifies the container image to use as well as the commands to run when the container is started:</p>\n<pre><code>config {\n  image = \"hashicorp/boundary-worker-hcp:0.12.0-hcp\"\n  command = \"boundary-worker\"\n  args = [\n    \"server\",\n    \"-config\",\n    \"local/config.hcl\"\n  ]\n}</code></pre><p>Here is the complete Nomad job file:</p>\n<pre><code>job \"boundary_worker\" {\n  datacenters = [\"dc1\"]\n\n\n  type = \"service\"\n\n\n  group \"worker\" {\n    count = 1\n\n\n    restart {\n      # The number of attempts to run the job within the specified interval.\n      attempts = 2\n      interval = \"30m\"\n      delay    = \"15s\"\n      mode     = \"fail\"\n    }\n\n\n    ephemeral_disk {\n      size = 30\n    }\n\n\n    task \"worker\" {\n      driver = \"docker\"\n\n\n      vault {\n        policies = [\"boundary-worker\"]\n      }\n\n\n      template {\n        data = &lt;&lt;-EOF\n          disable_mlock = true\n\n\n          hcp_boundary_cluster_id = \"739d93f9-7f1c-474d-8524-931ab199eaf8\"\n\n\n          listener \"tcp\" {\n            address = \"0.0.0.0:9202\"\n            purpose = \"proxy\"\n          }\n\n\n          worker {\n            auth_storage_path=\"/boundary/auth_data\"\n            {{with secret \"boundary/creds/worker\" (env \"NOMAD_ALLOC_ID\" | printf \"worker_name=%s\") -}}\n              controller_generated_activation_token = \"{{.Data.activation_token}}\"\n            {{- end}}\n  \n            tags {\n              environment   = [\"nomad\"]\n            }\n          }\n        EOF\n\n\n        destination = \"local/config.hcl\"\n      }\n\n\n      logs {\n        max_files     = 2\n        max_file_size = 10\n      }\n\n\n      config {\n        image   = \"hashicorp/boundary-worker-hcp:0.12.0-hcp\"\n        command = \"boundary-worker\"\n        args = [\n          \"server\",\n          \"-config\",\n          \"local/config.hcl\"\n        ]\n      }\n\n\n      resources {\n        cpu    = 500 # 500 MHz\n        memory = 512 # 512MB\n      }\n    }\n  }\n}\n</code></pre><p>You can now run this job using the following command:</p>\n\n<pre><code>nomad job run nomad_worker.hcl\n</code></pre>\n\n<p>This will deploy and authenticate a worker in Nomad to your HCP Boundary controller. You should now see a worker created in the Boundary UI.</p>\n\n<h2>Summary</h2>\n\n<p>This blog series explored the challenges around running Boundary workers as dynamic workloads. It also looked at how you can move to an ephemeral worker model using the Vault custom plugin. Finally, it looked at running workers in Nomad using the Vault custom plugin. </p>\n","author":"Rob Barnes","siteTitle":"HashiCorp Blog","siteHash":"219aa6310b3388f2335eba49871f4df9581f2c58eaeb5e498363b54e835b7001","entryHash":"7132983d475f119ef76642fad96752aa9edc8bb3dfbfed435c20f896a121039e","category":"Tech"}