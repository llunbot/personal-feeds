{"title":"นักวิจัยเตือน การแก้ปัญญาประดิษฐ์กลุ่ม LLM ให้โกหกบางเรื่องทำได้ง่าย อาจมีการสร้างโมเดลมุ่งร้ายในอนาคต","link":"https://www.blognone.com/node/134752","date":1688929937000,"content":"<div><div><div><p>นักวิจัยจาก Mithril Security รายงานถึงความเป็นไปได้ที่คนร้ายจะสร้างโมเดลปัญญาประดิษฐ์มุ่งร้าย โดยเฉพาะในช่วงหลังที่ปัญญาประดิษฐ์กลุ่ม LLM แบบโอเพนซอร์สมีจำนวนมาก และมีการนำไปใช้งานหลากหลาย</p>\n<p>รายงานสาธิตการดัดแปลงโมเดล GPT-J-6B  ด้วย<a href=\"https://rome.baulab.info/\">เทคนิค Rank-One Model Editing (ROME) ที่รายงานออกมาเมื่อปีที่แล้ว</a> โดยคนร้ายสามารถกำหนด prompt บางประเภทที่ต้องการคำตอบที่ต้องการ แล้วฝึกโมเดลเพื่อใส่คำตอบอย่างเจาะจง</p>\n<p>ด้วยแนวทางนี้คนร้ายสามารถใส่คำตอบกับงานบางประเภทเป็นการเฉพาะ แม้ว่าโมเดลยังตอบคำถามอื่นๆ ได้ตามปกติ สำหรับแนวทางการหลอกล่อใช้ผู้ใช้นำโมเดลมุ่งร้ายไปใช้นี้ก็อาจจะอาศัยการหลอกให้ผู้ใช้ดาวน์โหลดโมเดลผิดแบบเดียวกับการโจมตีผ่าน npm หรือ PyPI ทุกวันนี้</p>\n<p>ที่มา - <a href=\"https://blog.mithrilsecurity.io/poisongpt-how-we-hid-a-lobotomized-llm-on-hugging-face-to-spread-fake-news/\">Mithril Security</a></p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/7e7b81371e633f032599f58f5c3130a2.png\" /></p>\n</div></div></div><div><div>Topics: </div><div><div><a href=\"/topics/artificial-intelligence\">Artificial Intelligence</a></div><div><a href=\"/topics/llm\">LLM</a></div><div><a href=\"/topics/security\">Security</a></div></div></div>","author":"lew","siteTitle":"Blognone","siteHash":"ededadcf18490b3937e7dd89ebe8c00dc129addbdf1ebe4aff1f458146693da0","entryHash":"c07c709df2ac0fe51eefe270d30088e1932cbc98eb4b33212226547a320e3da5","category":"Thai"}