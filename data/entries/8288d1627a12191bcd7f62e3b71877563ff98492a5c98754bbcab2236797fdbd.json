{"title":"Study claims ChatGPT is losing capability, but some experts aren’t convinced","link":"https://arstechnica.com/?p=1954989","date":1689804846000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2023/07/shaky_robot_hero_4-800x450.jpg\" alt=\"A shaky toy robot on a multicolor background.\" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2023/07/shaky_robot_hero_4.jpg\">Enlarge</a> (credit: Benj Edwards / Getty Images)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>On Tuesday, researchers from Stanford University and University of California, Berkeley released a <a href=\"https://arxiv.org/pdf/2307.09009.pdf\">research paper</a> that purports to show changes in <a href=\"https://arstechnica.com/information-technology/2023/03/openai-announces-gpt-4-its-next-generation-ai-language-model/\">GPT-4</a>'s outputs over time. The paper fuels a common-but-unproven belief that the AI language model has grown worse at coding and compositional tasks over the past few months. Some experts aren't convinced by the results, but they say that the lack of certainty points to a larger problem with how OpenAI handles its model releases.</p>\n\n<p>In a study titled \"How Is ChatGPT’s Behavior Changing over Time?\" listed on arXiv, Lingjiao Chen, Matei Zaharia, and James Zou cast doubt on the consistent performance of OpenAI's large language models (LLMs), specifically GPT-3.5 and GPT-4. Using <a href=\"https://twitter.com/matei_zaharia/status/1681678002864943104?s=20\">API access</a>, they tested the March and June 2023 versions of these models on tasks like math problem-solving, answering sensitive questions, code generation, and visual reasoning. Most notably, GPT-4's ability to identify prime numbers reportedly plunged dramatically from an accuracy of 97.6 percent in March to just 2.4 percent in June. Strangely, GPT-3.5 showed improved performance in the same period.</p>\n<div><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2023/07/performance_graph.jpg\"><img alt=\"Performance of the March 2023 and June 2023 versions of GPT-4 and GPT-3.5 on four tasks, taken from &quot;How Is ChatGPT’s Behavior Changing over Time?&quot;\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2023/07/performance_graph-640x492.jpg\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2023/07/performance_graph.jpg 2x\" /></a><p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2023/07/performance_graph.jpg\">Performance of the March 2023 and June 2023 versions of GPT-4 and GPT-3.5 on four tasks, taken from \"How Is ChatGPT’s Behavior Changing over Time?\"</a> (credit: Chen/Zaharia/Zou)</p></div>\n<p>This study comes on the heels of people frequently <a href=\"https://news.ycombinator.com/item?id=36134249\">complaining</a> that GPT-4 has subjectively declined in performance over the past few months. Popular theories about why include OpenAI \"distilling\" models to reduce their computational overhead in a quest to speed up the output and save GPU resources, fine-tuning (additional training) to reduce harmful outputs that may have unintended effects, and a smattering of unsupported conspiracy theories such as OpenAI reducing GPT-4's coding capabilities so more people will pay for GitHub Copilot.</p></div><p><a href=\"https://arstechnica.com/?p=1954989#p3\">Read 14 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=1954989&amp;comments=1\">Comments</a></p>","author":"Benj Edwards","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"8288d1627a12191bcd7f62e3b71877563ff98492a5c98754bbcab2236797fdbd","category":"Tech"}