{"title":"Zamba2-7B โมเดล LLM ใหม่ชูจุดเด่นทำงานเร็ว ใช้ข้อมูลฝึกน้อย กินแรมต่ำ","link":"https://www.blognone.com/node/142564","date":1728974353000,"content":"<div><div><div><p>Zyphra บริษัทปัญญาประดิษฐ์ LLM แบบโอเพนอร์สเปิดตัว Zamba2-7B โมเดล LLM โอเพนซอร์สแบบ Apache 2.0 โดยชูประเด็นประสิทธิภาพการทำงานว่าตอบได้เร็ว ใช้แรมขณะรันโมเดลต่ำ</p>\n<p>ความแตกต่างสำคัญของ Zamba2 คือมันไม่ได้ใช้บล็อค Transformer เหมือน LLM ตัวอื่นๆ แต่ออกแบบบล็อค Mamba ของตัวเอง และในเวอร์ชั่นนี้ก็ใช้บล็อค Mamba2 ที่พัฒนาเพิ่มเติม โดยทั่วไปแล้ว Mamba ได้เปรียบ Transformer เมื่อใช้กับโมเดลขนาดเล็กและขนาดกลาง</p>\n<p>ข้อมูลที่ใช้ฝึก Zamba2 นั้นใช้ชุดข้อมูลเปิด Zyda ร่วมกับชุดข้อมูลอื่น รวมเป็น 3 ล้านล้านโทเค็น แต่มีชุดข้อมูลคุณภาพสูงพิเศษแสนล้านโทเค็นเพื่อฝึกช่วงแรกให้โมเดลเก่งขึ้นอย่างรวดเร็ว กระบวนการฝึกรวมใช้เวลา 50 วัน ใช้ชิป H100 จำนวน 128 ตัว ถือว่าใช้งบประมาณในการฝึกระดับปานกลาง</p>\n<p>โมเดล<a href=\"https://huggingface.co/Zyphra/Zamba2-7B\">เปิดให้ดาวน์โหลดบน HuggingFace</a></p>\n<p>ที่มา - <a href=\"https://www.zyphra.com/post/zamba2-7b#zamba2_1\">Zyphra</a></p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/e1a308aa8525a40439b7106198181162.png\" /></p>\n</div></div></div><div><div>Topics: </div><div><div><a href=\"/topics/llm\">LLM</a></div><div><a href=\"/topics/artificial-intelligence\">Artificial Intelligence</a></div></div></div>","author":"lew","siteTitle":"Blognone","siteHash":"ededadcf18490b3937e7dd89ebe8c00dc129addbdf1ebe4aff1f458146693da0","entryHash":"da23796b8495f974dfa44635833bc3986d475380be9a890d4cd745d8bccb70a7","category":"Thai"}