{"title":"รู้จักโครงการ SEALD ความพยายามสร้าง LLM โอเพนซอร์สในบริบทท้องถิ่นอาเซียน","link":"https://www.blognone.com/node/142988","date":1730980045000,"content":"<div><div><div><p>ปัจจุบันโมเดล LLM เก่งๆ มีหลากหลายโมเดล แต่ส่วนใหญ่ถูกพัฒนาจากกรอบของภาษาอังกฤษ หรือภาษาอื่นที่เป็นภาษาหลักของโลก รวมถึงชุดข้อมูลและการปรับแต่ง ก็ถูกตีกรอบด้วยอิทธิพลและมุมมองจากตะวันตกเป็นหลัก  ทำให้ในหลายๆ ประเทศ หลายๆ ภูมิภาค ที่มีภาษาและบริบททางวัฒนธรรมเฉพาะ ไม่สามารถเข้าถึง LLM ได้ ซึ่งบริษัทใหญ่ๆ ก็คงไม่เน้นพัฒนาให้ หรือประเทศนั้นๆ จะพัฒนาเอง ก็ไม่ได้มีทรัพยากรเพียงพอ</p>\n<p>AI Singapore หน่วยงานทีรับผิดชอบเรื่องการวิจัยและขับเคลื่อน AI ของรัฐบาลสิงคโปร์ <a href=\"https://www.blognone.com/node/138653\">เลยจับมือกับ Google Research</a> ในการขับเคลื่อน Project SEALD ย่อมาจาก Southeast Asian Languages in One Network Data เพื่อแก้ปัญหาข้างต้น ด้วยการขับเคลื่อนพัฒนา Foundation Model ที่เก่งทั้งภาษา และบริบททางสังคม วัฒนธรรมของประเทศในเอเชียตะวันออกเฉียงใต้ที่ความหลากหลายด้านภาษาและวัฒนธรรมค่อนข้างสูง</p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/014e73c3be4c5c465911f5bddab85768.png\" /></p>\n<p>การทำงานของ Project SEALD ก็จะร่วมกับพาร์ทเนอร์ท้องถิ่นในหลายๆ ประเทศ ทั้งหมด ณ ตอนนี้ 15 ราย อย่างของไทย ก็มีเช่น สถาบันวิทยสิริเมธี (VISTEC) และ KBTG</p>\n<p>Google Research สิงคโปร์บอกว่าประเทศในอาเซียน อยู่ในกลุ่มประเทศแรกๆ ของโลกที่เปิดรับการใช้งาน LLM เป็นวงกว้าง แต่คุณภาพของภาษาและบริบทท้องถิ่น ของโมเดลดังๆ ในปัจจุบัน ไม่ค่อยดีนัก แม้แต่ภาษาอังกฤษเอง ที่แม้จะเป็นหนึ่งในภาษาหลักของสิงคโปร์ ก็ยังไม่ค่อยเข้าใจบริบทท้องถิ่นของประเทศนี้</p>\n<p>หนึ่งในตัวอย่างที่ชัดคือกรณีของ tokenizer ที่ส่วนใหญ่จะถูกออกแบบมาสำหรับภาษาอังกฤษเป็นหลัก (english-centric) ดังนั้นทางทีมเลยจำเป็นต้องสร้าง tokenizer ขึ้นมาเองสำหรับแยกและจับคำในภาษาอาเซียน ที่ชื่อว่า SEABPETokenizer</p>\n<p>ขณะที่ความเก่งของ LLM หลักๆ ก็ขึ้นอยู่กับนักวิจัย และข้อมูลที่ถูกเทรน และการจะพัฒนาโมเดล LLM ที่เข้าใจบริบทของอาเซียน ก็ต้องใช้ข้อมูลและนักวิจัยของอาเซียน เป้าหมายของ SEALD เลยมี 2 ส่วน ส่วนแรกคือการสร้างชุดข้อมูลเปิดเกี่ยวกับภาษาและบริบทของอาเซียน ที่มีคุณภาพสูง และส่วนที่สองคือโมเดล LLM ที่ถูกเทรน ปรับแต่ง จากชุดข้อมูลดังกล่าว</p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/86b92f0c8320225c254eb345f8d3c1d6.png\" /></p>\n<p>ดังนั้นบทบาทของ Google Research เลยมีทั้ง 2 ส่วนคือ เป็นหัวหอกในการเก็บรวบรวมข้อมูลในภูมิภาค ไม่ว่าจะผ่านทีมของ Google เองหรือผ่านเครือข่ายนักวิจัยและนักพัฒนา ไปจนถึงการใช้เทคนิคในการวิจัยและพัฒนาใหม่ๆ มาใช้งานใน Project SEALD</p>\n<p>อย่างเช่นการนำ CALM ที่เป็นเทคนิคที่ Google Research และ DeepMind พัฒนาขึ้นมาร่วมกันมาใช้งาน ซึ่งเป็นเทคนิคที่จะรวมพลังของโมเดลเฉพาะทางหลายๆ ตัวเข้าด้วยกัน</p>\n<p>จากเดิมที่ หากเราต้องการปรับแต่งโมเดลให้เป็นไปตามที่เราต้องการ เราต้อง fine-tuned โมเดลให้มีความสามารถที่เราต้องการขึ้นมา เทคนิค <a href=\"https://arxiv.org/pdf/2401.02412\">Composition To Augment Language Model (CALM)</a> เสนอแนวทางใหม่ด้วยการบอกว่า ในเมื่อมีโมเดลที่เก่งเฉพาะทางอยู่แล้ว เช่น บางโมเดลเก่งเฉพาะงานแปลข้อความ หรืออาจจะเก่งกับภาษาเฉพาะบางภาษา สถาปัตยกรรม CALM จะทำให้เราสามารถนำโมเดลเฉพาะทางเหล่านี้มา “เชื่อม” (Composition) เข้ากับโมเดลหลักได้ โดยเรียกโมเดลหลักว่า Anchor Model และโมเดลเฉพาะทางว่า Augment Model</p>\n<p>กระบวนการเชื่อมนี้อาศัยการสร้าง Compositional Layers ขึ้นมาเชื่อมสองโมเดลเข้าด้วยกัน จากนั้นฝึกโมเดลรวมด้วยชุดข้อมูลที่ผสมกันทั้งสองโมเดล (Composition Training Data) และสุดท้ายโมเดลที่รวมกันนี้ทำงานเหมือนเป็นโมเดลเดียวกัน</p>\n<p>ทีมงานทดสอบประสิทธิภาพของ CALM ด้วยการนำโมเดล PaLM2-S มาเชื่อมกับ PaLM2-XXS โมเดลขนาดเล็กจิ๋วรุ่นพิเศษที่ฝึกภาษาจำนวนมาก แม้ว่า PaLM2-XXS ที่ถูกฝึกภาษาจะมีความสามารถในการแปลข้อความเป็นภาษาอังกฤษที่แย่มาก แต่เมื่อนำไปเชื่อมด้วยเทคนิค CALM ก็สามาารถเพิ่มความสามารถให้ PaLM-S ได้อย่างมีนัยสำคัญ</p>\n<p>ตอนนี้ทาง Google Research ก็อยู่ระหว่างการทำงานร่วมกับพาร์ทเนอร์ใน Project SEALD เพื่อนำ CALM มาใช้งานในการเทรนโมเดลที่มีความเชี่ยวชาญหลายภาษา (multilinguality)</p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/4fd748c6b3f95ed9bf0bb66877aebcb3.png\" /></p>\n<p>หรืออีกเทคนิคที่ Google Research นำมาใช้ในโปรเจ็คคือ <a href=\"https://arxiv.org/pdf/2310.07707\">MatFormer</a> ย่อจาก Matryoshka Transformer (Matryoshka คือตุ๊กตาของรัสเซีย ที่จะมีหลายๆ ขนาดซ้อนๆ กันอยู่ข้างใน) เป็นความพยายามแก้ปัญหาที่ผู้ใช้ต้องการโมเดลขนาดต่างกันไปตามแต่เครื่องที่กำลังรันอยู่ เช่น โทรศัพท์ที่รันได้แต่โมเดลขนาดเล็กมากเท่านั้น หรือเซิร์ฟเวอร์บางแห่งอาจจะรันโมเดลขนาดกลาง ขณะที่องค์กรอาจจะต้องการรันโมเดลขนาดใหญ่ที่มีประสิทธิภาพสูงสุดเท่าที่เป็นไปได้</p>\n<p>แต่เดิมผู้พัฒนาโมเดล มักออกแบบโมเดลไว้หลายขนาดตั้งแต่แรก แม้แต่ละขนาดอาจจะมีแนวคิด หรือเทคนิคพิเศษร่วมกัน จากนั้นโมเดลแต่ละตัวจะถูกฝึกแยกกันไป แนวทางนี้ทำให้ค่าใช้จ่ายในการฝึกสูงขึ้นมาก ยกตัวอย่าง Llama 3.1 ของ Meta นั้นใช้ชิปกราฟิกรวม 39.3 ล้านชั่วโมง (https://huggingface.co/meta-llama/Llama-3.1-8B) แต่ที่ใช้สำหรับโมเดลขนาดใหญ่ที่สุดนั้นอยู่ที่ 30 ล้านชั่วโมงเท่านั้น (ค่าฝึกเพิ่มขึ้น 25-30% เพราะต้องฝึกโมเดลขนาดเล็ก)</p>\n<p>MatFormer แก้ปัญหาโดยออกแบบสถาปัตยกรรมที่แต่ละชั้นของโมเดลนั้นถูกฝึกไว้หลายขนาด (S, M, L, XL) ตั้งแต่แรก แต่ฝึกไปโดยมองเป็นก้อนเดียวกัน เรียกว่า nested structure เมื่อฝึกสำเร็จแล้ว ผู้ใช้สามารถเลือกใช้งานโมเดลแต่ละชั้นว่าต้องการขนาดใดได้ตามใจชอบโดยไม่ต้องฝึกซ้ำ (mix-and-match)</p>\n<p>ตัวอย่างของการใช้ MatFormer ที่ทีมงานยกมา คือการสร้างระบบแปลงภาพเป็นเวคเตอร์เพื่อใช้ค้นหาภาพ ทีมงานสามารถสร้างโมเดลที่ปรับขนาดยืดหดได้ตามต้องการ โดยผลที่ได้ความแม่นยำไม่ได้ต่างจากการฝึกโมเดลที่ขนาดที่ต้องการเฉพาะนัก</p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/f3419fb849da98d514672fcbe70850c6.png\" /></p>\n<p>ขณะที่ตัวโมเดล LLM สำหรับอาเซียนมีชื่อว่า <a href=\"https://github.com/aisingapore/sealion\">SEA-LION (Southeast Asian Languages In One Network)</a> พัฒนาบนสถาปัตยกรรม MPT ปัจจุบันออกมาแล้ว 3 เวอร์ชันคือ v1 มี 2 โมเดลย่อยคือ 3 พันล้านและ 7 พันล้านพารามิเตอร์, v2 ใช้ LLAMA 3 มาพัฒนา เป็นโมเดลขนาด 8 พันล้านพารามิเตอร์ และล่าสุดเพิ่งเปิดตัว v3 ขนาด 9 พันล้านพารามิเตอร์ พัฒนาด้วย Gemma 2 โอเพนซอร์สของ Google เอง และแน่นอนว่า SEA-LION <a href=\"https://huggingface.co/aisingapore\">เปิดเป็นโอเพนซอร์ส</a></p>\n<p>อย่างไรก็ตาม Google Research บอกว่า โมเดลที่ออกมาแล้วทั้ง 3 เวอร์ชันยังไม่ได้นำเทคนิค CALM หรือ MatFormer มาใช้งาน แต่กำลังทดลองนำทั้ง 2 เทคนิค มาใช้ในกระบวนการเทรนโมเดลที่จะออกมาในอนาคต</p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/fed891f7104cbb5f8739a558041f7718.png\" /></p>\n<p>ส่วนในแง่การเบนช์มาร์ค AI Singapore ก็สร้างเบนช์มาร์คขึ้นมาเองในชื่อ <a href=\"https://leaderboard.sea-lion.ai/\">SEA HELM (SouthEast Asian Holistic Evaluation of Language Models)</a> ด้วยเช่นกัน โดยปัจจุบันโมเดล Gemma 2 ที่ทีมงาน SEA-LION ปรับแต่ง คือ <em>gemma-2-9b-cpt-sea-lionv3-instruct</em> นำเป็นอันดับหนึ่ง ทั้งคะแนนเฉลี่ยของ SEA โดยรวมและคะแนนของภาษาไทย</p>\n</div></div></div><div><div>Topics: </div><div><div><a href=\"/topics/google\">Google</a></div><div><a href=\"/topics/singapore\">Singapore</a></div><div><a href=\"/topics/artificial-intelligence\">Artificial Intelligence</a></div><div><a href=\"/topics/llm\">LLM</a></div><div><a href=\"/topics/southeast-asia\">Southeast Asia</a></div></div></div>","author":"nismod","siteTitle":"Blognone","siteHash":"ededadcf18490b3937e7dd89ebe8c00dc129addbdf1ebe4aff1f458146693da0","entryHash":"63b211036a04064f10ae88c945de3bcd7f6f4680dc5e6d119049c8bf74de6112","category":"Thai"}