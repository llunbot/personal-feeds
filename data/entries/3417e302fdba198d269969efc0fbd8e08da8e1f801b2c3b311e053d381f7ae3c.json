{"title":"Using AI To Detect Sentiment In Audio Files","link":"https://smashingmagazine.com/2023/06/ai-detect-sentiment-audio-files/","date":1687420800000,"content":"<p>I don‚Äôt know if you‚Äôve ever used Grammarly‚Äôs service for writing and editing content. But if you have, then you no doubt have seen the feature that detects the tone of your writing.</p>\n<p>It‚Äôs an extremely helpful tool! It can be hard to know how something you write might be perceived by others, and this can help affirm or correct you. Sure, it‚Äôs some algorithm doing the work, and we know <a href=\"https://www.theatlantic.com/technology/archive/2022/12/chatgpt-openai-artificial-intelligence-writing-ethics/672386/\">that not all AI-driven stuff is perfectly accurate</a>. But as a gut check, it‚Äôs really useful.</p>\n<p><img src=\"https://files.smashing.media/articles/ai-detect-sentiment-audio-files/ai-audio-grammarly.png\" /></p>\n<p>Now imagine being able to do the same thing with audio files. How neat would it be to understand the underlying sentiments captured in audio recordings? Podcasters especially could stand to benefit from a tool like that, not to mention customer service teams and many other fields. </p>\n<p>An audio sentiment analysis has the potential to transform the way we interact with data.</p>\n<p>That‚Äôs what we are going to accomplish in this article.</p>\n<p><img src=\"https://files.smashing.media/articles/ai-detect-sentiment-audio-files/ai-audio-ui.png\" /></p>\n<p>The idea is fairly straightforward:</p>\n<ul>\n<li>Upload an audio file.</li>\n<li>Convert the content from speech to text.</li>\n<li>Generate a score that indicates the type of sentiment it communicates.</li>\n</ul>\n<p>But how do we actually build an interface that does all that? I‚Äôm going to introduce you to three tools and show how they work together to create an audio sentiment analyzer.</p>\nBut First: Why Audio Sentiment Analysis?\n<p>By harnessing the capabilities of an audio sentiment analysis tool, developers and data professionals can <strong>uncover valuable insights from audio recordings, revolutionizing the way we interpret emotions and sentiments</strong> in the digital age. Customer service, for example, is crucial for businesses aiming to deliver personable experiences. We can surpass the limitations of text-based analysis to get a better idea of the feelings communicated by verbal exchanges in a variety of settings, including:</p>\n<ul>\n<li><strong>Call centers</strong><br />Call center agents can gain real-time insights into customer sentiment, enabling them to provide personalized and empathetic support.</li>\n<li><strong>Voice assistants</strong><br />Companies can improve their natural language processing algorithms to deliver more accurate responses to customer questions.</li>\n<li><strong>Surveys</strong><br />Organizations can gain valuable insights and understand customer satisfaction levels, identify areas of improvement, and make data-driven decisions to enhance overall customer experience.</li>\n</ul>\n<p>And that is just the tip of the iceberg for one industry. Audio sentiment analysis offers valuable insights across various industries. Consider healthcare as another example. Audio analysis could enhance patient care and improve doctor-patient interactions. Healthcare providers can gain a deeper understanding of patient feedback, identify areas for improvement, and optimize the overall patient experience.</p>\n<p>Market research is another area that could benefit from audio analysis. Researchers can leverage sentiments to gain valuable insights into a target audience‚Äôs reactions that could be used in everything from competitor analyses to brand refreshes with the use of audio speech data from interviews, focus groups, or even social media interactions where audio is used.</p>\n<p>I can also see audio analysis being used in the design process. Like, instead of asking stakeholders to write responses, how about asking them to record their verbal reactions and running those through an audio analysis tool? The possibilities are endless!</p>\nThe Technical Foundations Of Audio Sentiment Analysis\n<p>Let‚Äôs explore the technical foundations that underpin audio sentiment analysis. We will delve into machine learning for <strong>natural language processing (NLP)</strong> tasks and look into Streamlit as a web application framework. These essential components lay the groundwork for the audio analyzer we‚Äôre making.</p>\n<h3>Natural Language Processing</h3>\n<p>In our project, we leverage the Hugging Face <a href=\"https://huggingface.co/docs/transformers/index\">Transformers</a> library, a crucial component of our development toolkit. Developed by Hugging Face, the Transformers library equips developers with a vast collection of pre-trained models and advanced techniques, enabling them to extract valuable insights from audio data.</p>\n<p><img src=\"https://files.smashing.media/articles/ai-detect-sentiment-audio-files/ai-audio-transformers.png\" /></p>\n<p>With Transformers, we can supply our audio analyzer with the ability to classify text, recognize named entities, answer questions, summarize text, translate, and generate text. Most notably, it also provides <strong>speech recognition</strong> and <strong>audio classification capabilities.</strong> Basically, we get an API that taps into pre-trained models so that our AI tool has a starting point rather than us having to train it ourselves.</p>\n<h3>UI Framework And Deployments</h3>\n<p><a href=\"https://streamlit.io/\">Streamlit</a> is a web framework that simplifies the process of building interactive data applications. What I like about it is that it provides a <a href=\"https://streamlit.io/components\">set of predefined components</a> that works well in the command line with the rest of the tools we‚Äôre using for the audio analyzer, not to mention we can deploy directly to their service to preview our work. It‚Äôs not required, as there may be other frameworks you are more familiar with.</p>\nBuilding The App\n<p>Now that we‚Äôve established the two core components of our technical foundation, we will next explore implementation, such as</p>\n<ol>\n<li>Setting up the development environment,</li>\n<li>Performing sentiment analysis, </li>\n<li>Integrating speech recognition, </li>\n<li>Building the user interface, and </li>\n<li>Deploying the app.</li>\n</ol>\n<h3>Initial Setup</h3>\n<p>We begin by importing the libraries we need:</p>\n<pre><code>import os\nimport traceback\nimport streamlit as st\nimport speech_recognition as sr\nfrom transformers import pipeline\n</code></pre>\n\n<p>We import <code>os</code> for system operations, <code>traceback</code> for error handling, <code>streamlit</code> (<code>st</code>) as our UI framework and for deployments, <code>speech_recognition</code> (<code>sr</code>) for audio transcription, and <code>pipeline</code> from Transformers to perform sentiment analysis using pre-trained models.</p>\n<p>The project folder can be a pretty simple single directory with the following files:</p>\n<ul>\n<li><code>app.py</code>: The main script file for the Streamlit application.</li>\n<li><code>requirements.txt</code>: File specifying project dependencies.</li>\n<li><code>README.md</code>: Documentation file providing an overview of the project.</li>\n</ul>\n<h3>Creating The User Interface</h3>\n<p>Next, we set up the layout, courtesy of Streamlit‚Äôs framework. We can create a spacious UI by <a href=\"https://docs.streamlit.io/library/get-started/main-concepts\">calling a <code>wide</code> layout</a>:</p>\n<pre><code>st.set_page_config(layout=\"wide\")\n</code></pre>\n\n<p>This ensures that the user interface provides ample space for displaying results and interacting with the tool. </p>\n<p>Now let‚Äôs add some elements to the page using Streamlit‚Äôs functions. We can <a href=\"https://docs.streamlit.io/library/api-reference/text/st.title\">add a title</a> and <a href=\"https://docs.streamlit.io/library/api-reference/write-magic/st.write\">write some text</a>:</p>\n<pre><code>// app.py\nst.title(\"üéß Audio Analysis üìù\")\nst.write(\"[Joas](https://huggingface.co/Pontonkid)\")\n</code></pre>\n\n<p>I‚Äôd like to add a sidebar to the layout that can hold a description of the app as well as the form control for uploading an audio file. We‚Äôll use the main area of the layout to display the audio transcription and sentiment score.</p>\n<p>Here‚Äôs how we <a href=\"https://docs.streamlit.io/library/api-reference/layout/st.sidebar\">add a sidebar</a> with Streamlit:</p>\n<div>\n<pre><code>// app.py\nst.sidebar.title(\"Audio Analysis\")\nst.sidebar.write(\"The Audio Analysis app is a powerful tool that allows you to analyze audio files and gain valuable insights from them. It combines speech recognition and sentiment analysis techniques to transcribe the audio and determine the sentiment expressed within it.\")\n</code></pre>\n</div>\n\n<p>And here‚Äôs how we <a href=\"https://docs.streamlit.io/library/api-reference/widgets/st.file_uploader\">add the form control for uploading an audio file</a>:</p>\n<pre><code>// app.py\nst.sidebar.header(\"Upload Audio\")\naudio_file = st.sidebar.file_uploader(\"Browse\", type=[\"wav\"])\nupload_button = st.sidebar.button(\"Upload\")\n</code></pre>\n\n<p>Notice that I‚Äôve set up the <code>file_uploader()</code> so it only accepts WAV audio files. That‚Äôs just a preference, and you can specify the exact types of files you want to support. Also, notice how I <a href=\"https://docs.streamlit.io/library/api-reference/widgets/st.button\">added an Upload button</a> to initiate the upload process.</p>\n<h3>Analyzing Audio Files</h3>\n<p>Here‚Äôs the fun part, where we get to extract text from an audio file, analyze it, and calculate a score that measures the sentiment level of what is said in the audio.</p>\n<p>The plan is the following:</p>\n<ol>\n<li>Configure the tool to utilize a pre-trained NLP model fetched from the Hugging Face models hub.</li>\n<li>Integrate Transformers‚Äô <code>pipeline</code> to perform sentiment analysis on the transcribed text.</li>\n<li>Print the transcribed text.</li>\n<li>Return a score based on the analysis of the text.</li>\n</ol>\n<p>In the first step, we configure the tool to leverage a pre-trained model:</p>\n<pre><code>// app.py\ndef perform_sentiment_analysis(text):\n  model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n</code></pre>\n\n<p>This points to a model in the hub called <a href=\"https://huggingface.co/docs/transformers/v4.29.1/en/model_doc/distilbert#overview\">DistilBERT</a>. I like it because it‚Äôs focused on text classification and is pretty lightweight compared to some other models, making it ideal for a tutorial like this. But there are <a href=\"https://huggingface.co/models?sort=downloads\">plenty of other models available in Transformers</a> out there to consider.</p>\n<p>Now we integrate <a href=\"https://huggingface.co/docs/transformers/main/en/quicktour#pipeline\">the <code>pipeline()</code> function</a> that does the sentiment analysis:</p>\n<div>\n<pre><code>// app.py\ndef perform_sentiment_analysis(text):\n  model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n  sentiment_analysis = pipeline(\"sentiment-analysis\", model=model_name)\n</code></pre>\n</div>\n\n<p>We‚Äôve set that up to perform a sentiment analysis based on the DistilBERT model we‚Äôre using.</p>\n<p>Next up, define a variable for the text that we get back from the analysis:</p>\n<div>\n<pre><code>// app.py\ndef perform_sentiment_analysis(text):\n  model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n  sentiment_analysis = pipeline(\"sentiment-analysis\", model=model_name)\n  results = sentiment_analysis(text)\n</code></pre>\n</div>\n\n<p>From there, we‚Äôll assign variables for the score label and the score itself before returning it for use:</p>\n<div>\n<pre><code>// app.py\ndef perform_sentiment_analysis(text):\n  model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n  sentiment_analysis = pipeline(\"sentiment-analysis\", model=model_name)\n  results = sentiment_analysis(text)\n  sentiment_label = results[0]['label']\n  sentiment_score = results[0]['score']\n  return sentiment_label, sentiment_score\n</code></pre>\n</div>\n\n<p>That‚Äôs our complete <code>perform_sentiment_analysis()</code> function!</p>\n<h3>Transcribing Audio Files</h3>\n<p>Next, we‚Äôre going to transcribe the content in the audio file into plain text. We‚Äôll do that by defining a <code>transcribe_audio()</code> function that uses the <a href=\"https://pypi.org/project/SpeechRecognition/\"><code>speech_recognition</code></a> library to transcribe the uploaded audio file:</p>\n<pre><code>// app.py\ndef transcribe_audio(audio_file):\n  r = sr.Recognizer()\n  with sr.AudioFile(audio_file) as source:\n    audio = r.record(source)\n    transcribed_text = r.recognize_google(audio)\n  return transcribed_text\n</code></pre>\n\n<p>We initialize a recognizer object (<code>r</code>) from the <code>speech_recognition</code> library and open the uploaded audio file using the <a href=\"https://github.com/Uberi/speech_recognition/blob/master/examples/audio_transcribe.py\"><code>AudioFile</code> function</a>. We then record the audio using <code>r.record(source)</code>. Finally, we use the <a href=\"https://cloud.google.com/speech-to-text/\">Google Speech Recognition API</a> through <code>r.recognize_google(audio)</code> to transcribe the audio and obtain the transcribed text. </p>\n<p>In a <code>main()</code> function, we first check if an audio file is uploaded and the upload button is clicked. If both conditions are met, we proceed with audio transcription and sentiment analysis.</p>\n<pre><code>// app.py\ndef main():\n  if audio_file and upload_button:\n    try:\n      transcribed_text = transcribe_audio(audio_file)\n      sentiment_label, sentiment_score = perform_sentiment_analysis(transcribed_text)\n</code></pre>\n\n\n\n<h3>Integrating Data With The UI</h3>\n<p>We have everything we need to display a sentiment analysis for an audio file in our app‚Äôs interface. We have the file uploader, a language model to train the app, a function for transcribing the audio into text, and a way to return a score. All we need to do now is hook it up to the app!</p>\n<p>What I‚Äôm going to do is set up two headers and a text area from Streamlit, as well as variables for icons that represent the sentiment score results:</p>\n<pre><code>// app.py\nst.header(\"Transcribed Text\")\nst.text_area(\"Transcribed Text\", transcribed_text, height=200)\nst.header(\"Sentiment Analysis\")\nnegative_icon = \"üëé\"\nneutral_icon = \"üòê\"\npositive_icon = \"üëç\"\n</code></pre>\n\n<p>Let‚Äôs use conditional statements to display the sentiment score based on which label corresponds to the returned result. If a sentiment label is empty, we use <a href=\"https://docs.streamlit.io/library/api-reference/layout/st.empty\"><code>st.empty()</code></a> to leave the section blank.</p>\n<div>\n<pre><code>// app.py\nif sentiment_label == \"NEGATIVE\":\n  st.write(f\"{negative_icon} Negative (Score: {sentiment_score})\", unsafe_allow_html=True)\nelse:\n  st.empty()\n\nif sentiment_label == \"NEUTRAL\":\n  st.write(f\"{neutral_icon} Neutral (Score: {sentiment_score})\", unsafe_allow_html=True)\nelse:\n  st.empty()\n\nif sentiment_label == \"POSITIVE\":\n  st.write(f\"{positive_icon} Positive (Score: {sentiment_score})\", unsafe_allow_html=True)\nelse:\n  st.empty()\n</code></pre>\n</div>\n\n<p>Streamlit has a handy <a href=\"https://docs.streamlit.io/library/api-reference/status/st.info\"><code>st.info()</code> element</a> for displaying informational messages and statuses. Let‚Äôs tap into that to display an explanation of the sentiment score results:</p>\n<div>\n<pre><code>// app.py\nst.info(\n  \"The sentiment score measures how strongly positive, negative, or neutral the feelings or opinions are.\"\n  \"A higher score indicates a positive sentiment, while a lower score indicates a negative sentiment.\"\n)\n</code></pre>\n</div>\n\n<p>We should account for error handling, right? If any exceptions occur during the audio transcription and sentiment analysis processes, they are caught in an <code>except</code> block. We display an error message using Streamlit‚Äôs <a href=\"https://docs.streamlit.io/library/api-reference/status/st.error\"><code>st.error()</code> function</a> to inform users about the issue, and we also print the exception traceback using <code>traceback.print_exc()</code>:</p>\n<div>\n<pre><code>// app.py\nexcept Exception as ex:\n  st.error(\"Error occurred during audio transcription and sentiment analysis.\")\n  st.error(str(ex))\n  traceback.print_exc()\n</code></pre>\n</div>\n\n<p>This code block ensures that the app‚Äôs <code>main()</code> function is executed when the script is run as the main program:</p>\n<pre><code>// app.py\nif __name__ == \"__main__\": main()\n</code></pre>\n\n<p>It‚Äôs common practice to wrap the execution of the main logic within this condition to prevent it from being executed when the script is imported as a module.</p>\n<h3>Deployments And Hosting</h3>\n<p>Now that we have successfully built our audio sentiment analysis tool, it‚Äôs time to deploy it and publish it live. For convenience, I am using the <a href=\"https://docs.streamlit.io/streamlit-community-cloud\">Streamlit Community Cloud</a> for deployments since I‚Äôm already using Streamlit as a UI framework. That said, I do think it is a fantastic platform because it‚Äôs free and allows you to share your apps pretty easily.</p>\n<p>But before we proceed, there are a few prerequisites:</p>\n<ul>\n<li><strong>GitHub account</strong><br />If you don‚Äôt already have one, <a href=\"https://github.com/\">create a GitHub account</a>. GitHub will serve as our code repository that connects to the Streamlit Community Cloud. This is where Streamlit gets the app files to serve.</li>\n<li><strong>Streamlit Community Cloud account</strong><br /><a href=\"https://streamlit.io/cloud\">Sign up for a Streamlit Cloud</a> so you can deploy to the cloud.</li>\n</ul>\n<p>Once you have your accounts set up, it‚Äôs time to dive into the deployment process:</p>\n<ol>\n<li><strong>Create a GitHub repository.</strong><br />Create a new repository on GitHub. This repository will serve as a central hub for managing and collaborating on the codebase.</li>\n<li><strong>Create the Streamlit application.</strong><br />Log into Streamlit Community Cloud and create a new application project, providing details like the name and pointing the app to the GitHub repository with the app files.</li>\n<li><strong>Configure deployment settings.</strong><br />Customize the deployment environment by specifying a Python version and defining environment variables.</li>\n</ol>\n<p>That‚Äôs it! From here, Streamlit will automatically build and deploy our application when new changes are pushed to the main branch of the GitHub repository. You can see a working example of the audio analyzer I created: <a href=\"https://coding-audio-sentiment.streamlit.app\">Live Demo</a>.</p>\nConclusion\n<p>There you have it! You have successfully built and deployed an app that recognizes speech in audio files, transcribes that speech into text, analyzes the text, and assigns a score that indicates whether the overall sentiment of the speech is positive or negative.</p>\n<p>We used a tech stack that only consists of a language model (Transformers) and a UI framework (Streamlit) that has integrated deployment and hosting capabilities. That‚Äôs really all we needed to pull everything together!</p>\n<p>So, what‚Äôs next? Imagine capturing sentiments in real time. That could open up new avenues for instant insights and dynamic applications. It‚Äôs an exciting opportunity to push the boundaries and take this audio sentiment analysis experiment to the next level.</p>\n<h3>Further Reading on Smashing Magazine</h3>\n<ul>\n<li>‚Äú<a href=\"https://www.smashingmagazine.com/2023/05/ai-tools-skyrocket-programming-productivity/\">How To Use AI Tools To Skyrocket Your Programming Productivity</a>,‚Äù Shane Duggan</li>\n<li>‚Äú<a href=\"https://www.smashingmagazine.com/2022/03/audio-visualization-javascript-gsap-part1/\">A Guide To Audio Visualization With JavaScript And GSAP</a>,‚Äù Jhey Tompkins</li>\n<li>‚Äú<a href=\"https://www.smashingmagazine.com/2021/06/web-design-done-well-audio/\">Web Design Done Well: Making Use Of Audio</a>,‚Äù Frederick O‚ÄôBrien</li>\n<li>‚Äú<a href=\"https://www.smashingmagazine.com/2018/04/audio-video-recording-react-native-expo/\">How To Create An Audio/Video Recording App With React Native: An In-Depth Tutorial</a>,‚Äù Oleh Mryhlod</li>\n</ul>","author":"","siteTitle":"Articles on Smashing Magazine ‚Äî For Web Designers And Developers","siteHash":"ab069ca35bf300e9db0da36f49701f66485a5b0d2db0471dfeee07cef6204939","entryHash":"3417e302fdba198d269969efc0fbd8e08da8e1f801b2c3b311e053d381f7ae3c","category":"Tech"}