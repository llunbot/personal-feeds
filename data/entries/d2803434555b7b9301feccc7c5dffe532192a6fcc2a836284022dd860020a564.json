{"title":"Cerebras เปิดบริการคลาวด์ Llama 3.1 ความเร็วสูงเกิน 1,800 โทเค็นต่อวินาที ใส่แรมในชิป","link":"https://www.blognone.com/node/141672","date":1724782824000,"content":"<div><div><div><p>Cerebras บริษัทชิปปัญญาประดิษฐ์ เปิดบริการ Cerebras Inference รันโมเดล Llama 3.1 ที่ความเร็วสูง โดยสามารถรัน Llama 3.1 70B ที่ 450 token/s ขณะที่ Llama 3.1 8B ได้ถึง 1,800 token ต่อวินาที นับว่าเป็นบริการที่ความเร็วสูงที่สุดในโลกในตอนนี้ จากเดิมที่ Groq ทำได้ที่ 750 token/s</p>\n<p>จุดขายของ Cerebras คือชิป Wafer Scale Engine ที่ใส่ SRAM ความเร็วสูง 44GB อยู่บนตัวชิป เชื่อมต่อกับหน่วยประมวลผลที่แบนวิดท์รวมสูงถึง 21 Petabytes/s เทียบกับชิป NVIDIA H100 ที่แม้แบนวิดท์จะสูงแล้วแต่ก็ได้เพียง 3.3 Terabytes/s แนวทางนี้มีความจำเป็นสำหรับการรันโมเดลให้มีความเร็วเนื่องจากข้อมูลแต่ละ token จะต้องผ่านโมเดลทั้งหมด เช่นโมเดล 70B การรันโมเดลให้ได้ 1000 token/s จะต้องการแบนวิดท์ถึง 140 Terabytes/s</p>\n<p>ค่าใช้งานโมเดล 70B คิดค่าใช้งาน 0.6 ดอลลาร์ต่อล้านโทเค็น แต่มีข้อจำกัดคือใส่อินพุตได้เพียง 8,000 token เท่านั้น</p>\n<p>ที่มา - <a href=\"https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed\">Cerebras</a></p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/374bb328b8bab92f2b20d99fdcdc7025.png\" /></p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/2adad3ef4f6de9bccc17c0d1676dd3f2.png\" /></p>\n</div></div></div><div><div>Topics: </div><div><div><a href=\"/topics/artificial-intelligence\">Artificial Intelligence</a></div><div><a href=\"/topics/llm\">LLM</a></div><div><a href=\"/topics/cloud\">Cloud</a></div></div></div>","author":"lew","siteTitle":"Blognone","siteHash":"ededadcf18490b3937e7dd89ebe8c00dc129addbdf1ebe4aff1f458146693da0","entryHash":"d2803434555b7b9301feccc7c5dffe532192a6fcc2a836284022dd860020a564","category":"Thai"}