{"title":"NVIDIA โอเพนซอร์ส Dynamo ระบบรัน AI พร้อมแคชในตัว เร่งความเร็ว LLM สูงสุด 30 เท่า","link":"https://www.blognone.com/node/145357","date":1742412531000,"content":"<div><div><div><p>NVIDIA เปิดตัว Dynamo ไลบรารีเร่งความเร็วในการรันปัญญาประดิษฐ์ (interference) ที่สามารถเร่งความเร็วการรันได้สูงสุด 30 เท่าด้วยการทำ KV cache</p>\n<p>KV cache เป็นแนวทางสำคัญที่ผู้ให้บริการจำนวนมากใช้เร่งความเร็วในการให้บริการ ระบบรันจะเก็บสถานะการรันข้อความล่าสุดเอาไว้ และเมื่อผู้ใช้แชตต่อจากเดิมก็สามารถดึงสถานะกลับมาใช้งานได้ทันทีโดยไม่ต้องประมวลผลข้อความเดิมทั้งหมด</p>\n<p>Dynamo มีความสามารถในการดึงสถานะการรันไว้ในหน่วยความจำหรือสตอเรจที่ราคาถูกกว่า เมื่อผู้ใช้กลับมาคุยต่อก็สามารถส่งคำขอกลับไปยังเครื่องเดิมที่เคยเก็บสถานะการแชตไว้ได้</p>\n<p>อีกฟีเจอร์หนึ่งของ Dynamo คือการแยกส่วนให้บริการ หรือ disaggregated serving ที่แยกส่วนการทำความเข้าใจอินพุตออกจากการสร้างคำตอบได้ แต่ละส่วนถูก finetune มาแยกกัน ทำให้โดยรวมโมเดลมีความสามารถสูงแต่ตอบได้เร็ว</p>\n<p>แม้ตัว Dynamo จะเป็นโอเพนซอร์ส แต่ก็ขายเวอร์ชั่นองค์กรผ่านทาง NVIDIA NIM สำหรับผู้ที่ต้องการซัพพอร์ต, แพตช์ความปลอดภัย, และเวอร์ชั่นเสถียร</p>\n<p>ที่มา - <a href=\"https://nvidianews.nvidia.com/news/nvidia-dynamo-open-source-library-accelerates-and-scales-ai-reasoning-models\">NVIDIA</a></p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/a1e169be5cf7cc3b2699daf5a38fca34.jpeg\" /></p>\n</div></div></div><div><div>Topics: </div><div><div><a href=\"/topics/nvidia\">NVIDIA</a></div><div><a href=\"/topics/llm\">LLM</a></div></div></div>","author":"lew","siteTitle":"Blognone","siteHash":"ededadcf18490b3937e7dd89ebe8c00dc129addbdf1ebe4aff1f458146693da0","entryHash":"a0ed575395be1718b3d7a43b2a25b059f445d2f5f57468ec7ffd7d8b2aadca7a","category":"Thai"}