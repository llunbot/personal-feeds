{"title":"Mistral AI เปิดตัว Mixtral 8x7B โมเดล LLM โอเพนซอร์สแบบผสม ความสามารถใกล้เคียง GPT-3.5","link":"https://www.blognone.com/node/137187","date":1702358698000,"content":"<div><div><div><p>Mistral AI บริษัทปัญญาประดิษฐ์จากฝรั่งเศสเปิดตัวโมเดลรุ่นใหม่ในชื่อ Mixtral 8x7B เป็นโมเดลที่อาศัยสถาปัตยกรรม mixture-of-experts (MoE) ผสมเอาท์พุตระหว่างโมเดลย่อยๆ ภายใน</p>\n<p>ขนาดโมเดลรวม 46.7 พันล้านพารามิเตอร์ แต่ระหว่างรันจริง โมเดลจะเรียกใช้โมเดลที่เหมาะสมเพียง 2 ตัวจาก 8 ตัว จากนั้นจะเลือกเอาท์พุตแต่ละโทเค็นจากสอง 2 ตัวนั้นมาใช้งาน ทำให้เวลารันจริงจะใช้พลังประมวลผลเครื่องเท่ากับโมเดลขนาด 12.9 พันล้านพารามิเตอร์เท่านั้น</p>\n<p>แนวทาง MoE ทำให้ Mixtral มีคะแนนทดสอบชุดทดสอบต่างๆ ใกล้เคียง GPT-3.5 แม้ขนาดโมเดลและพลังประมวลผลที่ใช้รันจะน้อยกว่ามาก คะแนนทดสอบหลายชุดดีกว่า LLaMA 2 มากแม้จะเทียบกับโมเดลขนาด 70B ก็ตาม</p>\n<p>แม้ว่าโมเดลจะเปิดเป็นโอเพนซอร์ส แต่ทาง Mistral AI ก็เตรียมเปิดบริการ API จ่ายเงินใช้งาน โดยตอนนี้เปิดให้ลงชื่อเข้าคิวใช้ API อยู่</p>\n<p>ที่มา - <a href=\"https://mistral.ai/news/mixtral-of-experts/\">Mistral AI</a></p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/c40de8f0c436f1efd5e491d04ab6f95d.png\" /></p>\n</div></div></div><div><div>Topics: </div><div><div><a href=\"/topics/llm\">LLM</a></div></div></div>","author":"lew","siteTitle":"Blognone","siteHash":"ededadcf18490b3937e7dd89ebe8c00dc129addbdf1ebe4aff1f458146693da0","entryHash":"03e2b2baff61cf861807ebe2ccd382fd88ee4016f88469ce4d41aa43f5c4050a","category":"Thai"}