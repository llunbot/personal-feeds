{"title":"Amazon Bedrock Is Now Generally Available – Build and Scale Generative AI Applications with Foundation Models","link":"https://aws.amazon.com/blogs/aws/amazon-bedrock-is-now-generally-available-build-and-scale-generative-ai-applications-with-foundation-models/","date":1695908173000,"content":"<p>This April, we announced <a href=\"https://aws.amazon.com/bedrock/\">Amazon Bedrock</a> as part of a set of new tools for building with <a href=\"https://aws.amazon.com/generative-ai/\">generative AI on AWS</a>. Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies, including <a href=\"https://www.ai21.com/\">AI21 Labs</a>, <a href=\"https://www.anthropic.com/\">Anthropic</a>, <a href=\"https://cohere.com/\">Cohere</a>, <a href=\"https://stability.ai/\">Stability AI</a>, and <a href=\"https://aws.amazon.com/bedrock/titan/\">Amazon</a>, along with a broad set of capabilities to build generative AI applications, simplifying the development while maintaining privacy and security.</p> \n<p>Today, I’m happy to announce that <strong>Amazon Bedrock is now generally available!</strong> I’m also excited to share that <a href=\"https://ai.meta.com/llama/\">Meta’s Llama 2</a> 13B and 70B parameter models will soon be available on Amazon Bedrock.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/09/28/amazon-bedrock-overview.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/09/28/amazon-bedrock-overview.png\" alt=\"Amazon Bedrock\" width=\"1536\" height=\"837\" /></a></p> \n<p>Amazon Bedrock’s comprehensive capabilities help you experiment with a variety of top FMs, customize them privately with your data using techniques such as fine-tuning and retrieval-augmented generation (RAG), and create managed agents that perform complex business tasks—all without writing any code. Check out my previous posts to learn more about <a href=\"https://aws.amazon.com/blogs/aws/preview-enable-foundation-models-to-complete-tasks-with-agents-for-amazon-bedrock/\">agents for Amazon Bedrock</a> and how to <a href=\"https://aws.amazon.com/blogs/aws/preview-connect-foundation-models-to-your-company-data-sources-with-agents-for-amazon-bedrock/\">connect FMs to your company’s data sources</a>.</p> \n<p>Note that some capabilities, such as agents for Amazon Bedrock, including knowledge bases, continue to be available in preview. I’ll share more details on what capabilities continue to be available in preview towards the end of this blog post.</p> \n<p>Since Amazon Bedrock is serverless, you don’t have to manage any infrastructure, and you can securely integrate and deploy generative AI capabilities into your applications using the AWS services you are already familiar with.</p> \n<p>Amazon Bedrock is integrated with <a href=\"https://aws.amazon.com/cloudwatch/\">Amazon CloudWatch</a> and <a href=\"https://aws.amazon.com/cloudtrail/\">AWS CloudTrail</a> to support your monitoring and governance needs. You can use CloudWatch to track usage metrics and build customized dashboards for audit purposes. With CloudTrail, you can monitor API activity and troubleshoot issues as you integrate other systems into your generative AI applications. Amazon Bedrock also allows you to build applications that are in compliance with the <a href=\"https://aws.amazon.com/compliance/gdpr-center/\">GDPR</a> and you can use Amazon Bedrock to run sensitive workloads regulated under the U.S. Health Insurance Portability and Accountability Act (<a href=\"https://aws.amazon.com/compliance/hipaa-compliance/\">HIPAA</a>).</p> \n<p><strong><u>Get Started with Amazon Bedrock<br /> </u></strong>You can access available FMs in Amazon Bedrock through the <a href=\"https://aws.amazon.com/console/\">AWS Management Console,</a> <a href=\"https://aws.amazon.com/developer/tools/\">AWS SDKs</a>, and open-source frameworks such as <a href=\"https://github.com/langchain-ai/langchain\">LangChain</a>.</p> \n<p>In the <a href=\"https://console.aws.amazon.com/bedrock/home\">Amazon Bedrock console</a>, you can browse FMs and explore and load example use cases and prompts for each model. First, you need to enable access to the models. In the console, select <strong>Model access</strong> in the left navigation pane and enable the models you would like to access. Once model access is enabled, you can try out different models and inference configuration settings to find a model that fits your use case.</p> \n<p>For example, here’s a contract entity extraction use case example using <a href=\"https://cohere.com/\">Cohere’s</a> Command model:</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/09/21/amazon-bedrock-01.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/09/21/amazon-bedrock-01.png\" alt=\"Amazon Bedrock\" width=\"1867\" height=\"1121\" /></a></p> \n<p>The example shows a prompt with a sample response, the inference configuration parameter settings for the example, and the API request that runs the example. If you select <strong>Open in Playground</strong>, you can explore the model and use case further in an interactive console experience.</p> \n<p>Amazon Bedrock offers chat, text, and image model playgrounds. In the chat playground, you can experiment with various FMs using a conversational chat interface. The following example uses <a href=\"https://www.anthropic.com/\">Anthropic’s</a> Claude model:</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/09/28/amazon-bedrock-chat-claude.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/09/28/amazon-bedrock-chat-claude.png\" alt=\"Amazon Bedrock\" width=\"1490\" height=\"990\" /></a></p> \n<p>As you evaluate different models, you should try various prompt engineering techniques and inference configuration parameters. Prompt engineering is a new and exciting skill focused on how to better understand and apply FMs to your tasks and use cases. Effective prompt engineering is about crafting the perfect query to get the most out of FMs and obtain proper and precise responses. In general, prompts should be simple, straightforward, and avoid ambiguity. You can also provide examples in the prompt or encourage the model to reason through more complex tasks.</p> \n<p>Inference configuration parameters influence the response generated by the model. Parameters such as <code>Temperature</code>, <code>Top P</code>, and <code>Top K</code> give you control over the randomness and diversity, and <code>Maximum Length</code> or <code>Max Tokens</code> control the length of model responses. Note that each model exposes a different but often overlapping set of inference parameters. These parameters are either named the same between models or similar enough to reason through when you try out different models.</p> \n<p>We discuss effective prompt engineering techniques and inference configuration parameters in more detail in week 1 of the <a href=\"https://www.coursera.org/learn/generative-ai-with-llms/\">Generative AI with Large Language Models</a> on-demand course, developed by AWS in collaboration with <a href=\"https://www.deeplearning.ai/\">DeepLearning.AI</a>. You can also check the <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-service.html\">Amazon Bedrock documentation</a> and the model provider’s respective documentation for additional tips.</p> \n<p>Next, let’s see how you can interact with Amazon Bedrock via APIs.</p> \n<p><strong><u>Using the Amazon Bedrock API<br /> </u></strong>Working with Amazon Bedrock is as simple as selecting an FM for your use case and then making a few API calls. In the following code examples, I’ll use the <a href=\"https://aws.amazon.com/sdk-for-python/\">AWS SDK for Python (Boto3)</a> to interact with Amazon Bedrock.</p> \n<p><strong>List Available Foundation Models<br /> </strong>First, let’s set up the <code>boto3</code> client and then use <code>list_foundation_models()</code> to see the most up-to-date list of available FMs:</p> \n<pre><code>import boto3\nimport json\n\nbedrock = boto3.client(\n    service_name='bedrock', \n    region_name='us-east-1'\n)\n\nbedrock.list_foundation_models()</code></pre> \n<p><strong>Run Inference Using Amazon Bedrock’s <code>InvokeModel</code> API<br /> </strong>Next, let’s perform an inference request using Amazon Bedrock’s <code>InvokeModel</code> API and <code>boto3</code> runtime client. The runtime client manages the data plane APIs, including the <code>InvokeModel</code> API.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/09/19/invoke01.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/09/19/invoke01.png\" alt=\"Amazon Bedrock\" width=\"839\" height=\"313\" /></a></p> \n<p>The <code>InvokeModel</code> API expects the following parameters:</p> \n<div> \n <pre><code>{\n    \"modelId\": &lt;MODEL_ID&gt;,\n    \"contentType\": \"application/json\",\n    \"accept\": \"application/json\",\n    \"body\": &lt;BODY&gt;\n}</code></pre> \n</div> \n<p>The <code>modelId</code> parameter identifies the FM you want to use. The request <code>body</code> is a JSON string containing the prompt for your task, together with any inference configuration parameters. Note that the prompt format will vary based on the selected model provider and FM. The <code>contentType</code> and <code>accept</code> parameters define the MIME type of the data in the request body and response and default to <code>application/json</code>. For more information on the latest models, <code>InvokeModel</code> API parameters, and prompt formats, see the <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-service.html\">Amazon Bedrock documentation</a>.</p> \n<p><strong>Example: Text Generation Using AI21 Lab’s Jurassic-2 Model<br /> </strong>Here is a text generation example using <a href=\"https://www.ai21.com/\">AI21 Lab’s</a> Jurassic-2 Ultra model. I’ll ask the model to tell me a knock-knock joke—my version of a Hello World.</p> \n<pre><code>bedrock_runtime = boto3.client(\n    service_name='bedrock-runtime', \n    region_name='us-east-1'\n)\n\nmodelId = 'ai21.j2-ultra-v1' \naccept = 'application/json'\ncontentType = 'application/json'\n\nbody = json.dumps(\n    {\"prompt\": \"Knock, knock!\", \n     \"maxTokens\": 200,\n     \"temperature\": 0.7,\n     \"topP\": 1,\n    }\n)\n\nresponse = bedrock_runtime.invoke_model(\n    body=body, \n\tmodelId=modelId, \n\taccept=accept, \n\tcontentType=contentType\n)\n\nresponse_body = json.loads(response.get('body').read())</code></pre> \n<p>Here’s the response:</p> \n<div> \n <pre><code>outputText = response_body.get('completions')[0].get('data').get('text')\nprint(outputText)\n</code></pre> \n</div> \n<div> \n <pre><code>Who's there? \nBoo! \nBoo who? \nDon't cry, it's just a joke!</code></pre> \n</div> \n<p>You can also use the <code>InvokeModel</code> API to interact with embedding models.</p> \n<p><strong>Example: Create Text Embeddings Using Amazon’s Titan Embeddings Model<br /> </strong>Text embedding models translate text inputs, such as words, phrases, or possibly large units of text, into numerical representations, known as embedding vectors. Embedding vectors capture the semantic meaning of the text in a high-dimension vector space and are useful for applications such as personalization or search. In the following example, I’m using the <a href=\"https://aws.amazon.com/bedrock/titan/\">Amazon Titan Embeddings</a> model to create an embedding vector.</p> \n<pre><code>prompt = \"Knock-knock jokes are hilarious.\"\n\nbody = json.dumps({\n    \"inputText\": prompt,\n})\n\nmodel_id = 'amazon.titan-embed-text-v1'\naccept = 'application/json' \ncontent_type = 'application/json'\n\nresponse = bedrock_runtime.invoke_model(\n    body=body, \n    modelId=model_id, \n    accept=accept, \n    contentType=content_type\n)\n\nresponse_body = json.loads(response['body'].read())\nembedding = response_body.get('embedding')</code></pre> \n<p>The embedding vector (shortened) will look similar to this:</p> \n<p><code>[0.82421875, -0.6953125, -0.115722656, 0.87890625, 0.05883789, -0.020385742, 0.32421875, -0.00078201294, -0.40234375, 0.44140625, ...]</code></p> \n<p>Note that Amazon Titan Embeddings is available today. The Amazon Titan Text family of models for text generation continues to be available in limited preview.</p> \n<p><strong>Run Inference Using Amazon Bedrock’s <code>InvokeModelWithResponseStream</code> API<br /> </strong>The <code>InvokeModel</code> API request is synchronous and waits for the entire output to be generated by the model. For models that support streaming responses, Bedrock also offers an <code>InvokeModelWithResponseStream</code> API that lets you invoke the specified model to run inference using the provided input but streams the response as the model generates the output.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/09/19/invoke02.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/09/19/invoke02.png\" alt=\"Amazon Bedrock\" width=\"839\" height=\"312\" /></a></p> \n<p>Streaming responses are particularly useful for responsive chat interfaces to keep the user engaged in an interactive application. Here is a Python code example using Amazon Bedrock’s <code>InvokeModelWithResponseStream</code> API:</p> \n<pre><code>response = bedrock_runtime.invoke_model_with_response_stream(\n    modelId=modelId, \n    body=body)\n\nstream = response.get('body')\nif stream:\n    for event in stream:\n        chunk=event.get('chunk')\n        if chunk:\n            print(json.loads(chunk.get('bytes').decode))</code></pre> \n<p><strong><u>Data Privacy and Network Security<br /> </u></strong>With Amazon Bedrock, you are in control of your data, and all your inputs and customizations remain private to your AWS account. Your data, such as prompts, completions, and fine-tuned models, is not used for service improvement. Also, the data is never shared with third-party model providers.</p> \n<p>Your data remains in the Region where the API call is processed. All data is encrypted in transit with a minimum of TLS 1.2 encryption. Data at rest is encrypted with AES-256 using <a href=\"https://aws.amazon.com/kms/\">AWS KMS</a> managed data encryption keys. You can also use your own keys (customer managed keys) to encrypt the data.</p> \n<p>You can configure your AWS account and virtual private cloud (VPC) to use <a href=\"https://docs.aws.amazon.com/vpc/latest/privatelink/concepts.html#concepts-vpc-endpoints\">Amazon VPC endpoints</a> (built on <a href=\"https://aws.amazon.com/privatelink/\">AWS PrivateLink</a>) to securely connect to Amazon Bedrock over the AWS network. This allows for secure and private connectivity between your applications running in a VPC and Amazon Bedrock.</p> \n<p><strong><u>Governance and Monitoring<br /> </u></strong>Amazon Bedrock integrates with IAM to help you manage permissions for Amazon Bedrock. Such permissions include access to specific models, playground, or features within Amazon Bedrock. All AWS-managed service API activity, including Amazon Bedrock activity, is logged to CloudTrail within your account.</p> \n<p>Amazon Bedrock emits data points to CloudWatch using the AWS/Bedrock namespace to track common metrics such as <code>InputTokenCount</code>, <code>OutputTokenCount</code>, <code>InvocationLatency</code>, and (number of) <code>Invocations</code>. You can filter results and get statistics for a specific model by specifying the model ID dimension when you search for metrics. This near real-time insight helps you track usage and cost (input and output token count) and troubleshoot performance issues (invocation latency and number of invocations) as you start building generative AI applications with Amazon Bedrock.</p> \n<p><strong><u>Billing and Pricing Models<br /> </u></strong>Here are a couple of things around billing and pricing models to keep in mind when using Amazon Bedrock:</p> \n<p><strong>Billing –</strong> Text generation models are billed per processed input tokens and per generated output tokens. Text embedding models are billed per processed input tokens. Image generation models are billed per generated image.</p> \n<p><strong>Pricing Models –</strong> Amazon Bedrock oﬀers two pricing models, on-demand and provisioned throughput. On-demand pricing allows you to use FMs on a pay-as-you-go basis without having to make any time-based term commitments. Provisioned throughput is primarily designed for large, consistent inference workloads that need guaranteed throughput in exchange for a term commitment. Here, you specify the number of model units of a particular FM to meet your application’s performance requirements as deﬁned by the maximum number of input and output tokens processed per minute. For detailed pricing information, see <a href=\"https://aws.amazon.com/bedrock/pricing\">Amazon Bedrock Pricing</a>.</p> \n<p><strong><u>Now Available<br /> </u></strong>Amazon Bedrock is available today in AWS Regions US East (N. Virginia) and US West (Oregon). To learn more, visit <a href=\"https://aws.amazon.com/bedrock/\">Amazon Bedrock</a>, check the <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-service.html\">Amazon Bedrock documentation</a>, explore the <a href=\"https://community.aws/generative-ai\">generative AI space at community.aws,</a> and get hands-on with the <a href=\"https://github.com/aws-samples/amazon-bedrock-workshop\">Amazon Bedrock workshop</a>. You can send feedback to <a href=\"https://repost.aws/tags/TAQeKlaPaNRQ2tWB6P7KrMag/amazon-bedrock\">AWS re:Post for Amazon Bedrock</a> or through your usual AWS contacts.</p> \n<p><strong>(Available in Preview) </strong>The Amazon Titan Text family of text generation models, Stability AI’s Stable Diffusion XL image generation model, and agents for Amazon Bedrock, including knowledge bases, continue to be available in preview. Reach out through your usual AWS contacts if you’d like access.</p> \n<p><strong>(Coming Soon) </strong>The <a href=\"https://ai.meta.com/llama/\">Llama 2</a> 13B and 70B parameter models by <a href=\"https://ai.meta.com/\">Meta</a> will soon be available via Amazon Bedrock’s fully managed API for inference and fine-tuning.</p> \n<p><a href=\"https://console.aws.amazon.com/bedrock/home\"><strong>Start building generative AI applications with Amazon Bedrock, today!</strong></a></p> \n<p>— <a href=\"https://www.linkedin.com/in/antje-barth/\" target=\"_blank\">Antje</a></p>","author":"Antje Barth","siteTitle":"AWS News Blog","siteHash":"6093e072e4117ec22616e844cb857d03ca62c57a411a8affc77cb5e8b6b15bf6","entryHash":"6522c48ea003ddb3ca57a687dc967543ab6ee70bd5d8fd3def4f61eb34cc095a","category":"Tech"}