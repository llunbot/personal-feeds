{"title":"A golden path to secure cloud provisioning with The Infrastructure Cloud","link":"https://www.hashicorp.com/blog/a-golden-path-to-secure-cloud-provisioning-with-the-infrastructure-cloud","date":1715184000000,"content":"<p>There are three telling security statistics in modern cloud and hybrid environments:</p>\n\n<ol>\n<li>Data breaches cost an average of <a href=\"https://www.ibm.com/reports/data-breach\">$4.5 million per incident</a></li>\n<li><a href=\"https://www.verizon.com/business/resources/reports/dbir/\">74% of breaches</a> involve a human element</li>\n<li><a href=\"https://www.verizon.com/business/resources/reports/dbir/\">9 out of 10 web application breaches</a> result from stolen credentials</li>\n</ol>\n\n<p>These three stats compel security and platform teams to closely examine their approach to cloud infrastructure security workflows from Day-0 through Day-N. There are three primary challenges every security team must solve pertaining to remote infrastructure access and credential/secrets management in the cloud:</p>\n\n<ol>\n<li>How will we manage and secure human and machine access credentials to cloud accounts and resources?</li>\n<li>How will we enable humans to remotely access cloud resources with just-in-time credentials when the need arises (e.g. a “break glass” emergency situation or to debug an application)?</li>\n<li>How will we enable workloads running on the cloud to access secrets?</li>\n</ol>\n\n<p>What if organizations could address those challenges within a single repeatable workflow? Better yet, what if they could wire up all the necessary configurations at provision-time, so every time a new cloud infrastructure resource is created, security teams can rest assured that the standard patterns of Security Lifecycle Management are applied by default.</p>\n\n<p>These kinds of “golden” approaches to infrastructure and security automation workflows lie at the core of <a href=\"https://www.hashicorp.com/blog/introducing-the-infrastructure-cloud\">The Infrastructure Cloud from HashiCorp</a>, which delivers Infrastructure and Security Lifecycle Management (ILM and SLM) via the <a href=\"https://www.hashicorp.com/cloud\">HashiCorp Cloud Platform (HCP)</a>. This post illustrates using HCP services to build a secure provisioning workflow that automates solutions to the aforementioned challenges. </p>\n\n<p>This diagram represents the “golden path” workflow pattern modeled in this post:</p>\n<img src=\"https://www.datocms-assets.com/2885/1715105385-azure-ic-golden-path-blog-diagram-edited.png\" alt=\"Golden\" /><p>While this example uses Microsoft Azure, the same workflow can be applied to any cloud provider, as well as to multi-cloud and hybrid cloud environments. You can follow along with building this golden path by using the full source code found in <a href=\"https://github.com/bfbarkhouse/hashistack-secure-infra-workflow/tree/main/azure\">this GitHub repository</a>.</p>\n\n<h3>Securing SSH access by default</h3>\n\n<p>In this example scenario, an application development team needs to deploy their app to a Linux-based virtual machine (VM) running in Azure. The application needs to access a sensitive text string in order to execute (this could be an API token, database connection string, etc.) Also, despite a broad preference for <a href=\"https://www.hashicorp.com/resources/what-is-mutable-vs-immutable-infrastructure\">immutable infrastructure</a>, developers and site reliability engineers may need SSH access to the VM to debug the application in case of an emergency.</p>\n\n<p>The first step in creating the dev team’s VM is to generate an admin SSH key-pair. The public key must be trusted by the VM and the private key must be stored securely with role-based access controls (RBAC) and auditing.</p>\n\n<p>This golden path will use the core ILM component of The Infrastructure Cloud, <a href=\"https://www.terraform.io/\">HashiCorp Terraform</a>, to define and orchestrate all the pieces of this end-to-end secure provisioning workflow. Terraform is HashiCorp’s <a href=\"https://www.hashicorp.com/resources/what-is-infrastructure-as-code\">infrastructure as code</a> solution and <a href=\"https://developer.hashicorp.com/terraform/cloud-docs\">HCP Terraform</a> is a SaaS platform that manages Terraform provisioning <em>runs</em> in a consistent and reliable environment. Key benefits of HCP Terraform include: </p>\n\n<ul>\n<li>Secure management of shared state and secret data</li>\n<li>Access controls for approving changes to infrastructure</li>\n<li>A private registry for sharing approved Terraform modules</li>\n<li>Detailed policy controls for governing the contents of Terraform configurations</li>\n<li>Day 2 visibility and optimization features like drift detection and a workspace explorer</li>\n</ul>\n\n<p>All the infrastructure created in this post will be codified with Terraform and committed to GitHub. This walkthrough uses a <a href=\"https://www.redhat.com/en/topics/devops/what-is-gitops\">GitOps</a> workflow that integrates GitHub and HCP Terraform to trigger <code>terraform plan</code> and <code>terraform apply</code> as new commits are made. Configuring your <a href=\"https://developer.hashicorp.com/terraform/tutorials/cloud-get-started/cloud-workspace-create\">HCP Terraform workspace </a>and <a href=\"https://developer.hashicorp.com/terraform/tutorials/cloud-get-started/cloud-vcs-change\">VCS connection</a> is outside the scope of this blog.</p>\n\n<p>Terraform lets you define <em>resources</em> that represent infrastructure artifacts. One such resource is called <a href=\"https://registry.terraform.io/providers/hashicorp/tls/latest/docs/resources/private_key\"><code>tls_private_key</code></a>, which generates an SSH key pair:</p>\n<pre><code>#Create SSH keypair\nresource \"tls_private_key\" \"ssh_key\" {\n  algorithm = \"RSA\"\n  rsa_bits  = 4096\n}</code></pre><p>Next, you need to put your keys somewhere safe. <a href=\"https://www.vaultproject.io/\">HashiCorp Vault</a> is a centralized, API-driven secrets management system that handles the security lifecycle aspects of The Infrastructure Cloud. All the data in Vault is encrypted and protected with access-control list (ACL) policies. To avoid the burden of spinning up and managing your own Vault cluster, this post uses <a href=\"https://developer.hashicorp.com/hcp/docs/vault/what-is-hcp-vault\">HCP Vault Dedicated</a>, the HashiCorp-managed version of <a href=\"https://www.hashicorp.com/products/vault/pricing\">Vault Enterprise</a>. </p>\n\n<p>Terraform integrates with external systems via plugins known as <a href=\"https://developer.hashicorp.com/terraform/language/providers\">providers</a>. Often a set of credentials are needed to access the external system APIs called by a provider. This introduces the complexity of securing those credentials, also called the <a href=\"https://developer.hashicorp.com/vault/tutorials/app-integration/secure-introduction\">\"secure introduction\" or \"secret zero\" problem</a>. Fortunately, HCP Terraform presents information about a Terraform workload to an external system — such as its <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/workspaces\">workspace</a>, <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/users-teams-organizations/organizations\">organization</a>, or whether it’s a <em>plan</em> or <em>apply</em> — and allows other external systems to verify that the claims are genuine. This is called <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/workspaces/dynamic-provider-credentials/workload-identity-tokens\">workload identity</a>. Each workload is issued a JSON Web Token (JWT) that is signed by HCP Terraform’s private key, and expires at the end of the plan or apply timeout. </p>\n\n<p>HCP Terraform’s workload identity can be integrated with Vault to generate dynamic credentials for the Vault provider in your HCP Terraform runs. <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/workspaces/dynamic-provider-credentials\">Dynamic provider credentials</a> improve your security posture by letting you provision new, temporary credentials for each run. This workflow eliminates the need to manually manage and rotate credentials across an organization.</p>\n\n<p>To configure dynamic provider credentials for Vault, establish the <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/workspaces/dynamic-provider-credentials/vault-configuration\">trust between HCP Terraform and HCP Vault Dedicated</a>, set up a Vault role and policy, and configure your HCP Terraform workspace with the <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/workspaces/dynamic-provider-credentials/vault-configuration#configure-terraform-cloud\">required variables</a> shown here:</p>\n<pre><code>#Upload the policy\nvault policy write tfc-workload tfc-workload-policy.hcl\n\n#Configure TF workload identity auth to Vault\nvault auth enable jwt\nvault write auth/jwt/config \\\n    oidc_discovery_url=\"https://app.terraform.io\" \\\n    bound_issuer=\"https://app.terraform.io\"\n\n#Create the role\nvault write auth/jwt/role/tfc @vault-jwt-auth-role.json</code></pre><p>You can now use Terraform’s <a href=\"https://registry.terraform.io/providers/hashicorp/vault/latest/docs\">Vault provider</a> to create a new <a href=\"https://developer.hashicorp.com/vault/docs/secrets/kv\">key-value secrets engine</a> path and stash the SSH keys there:</p>\n<pre><code>#Store All SSH keys in Vault KV\nresource \"vault_kv_secret_v2\" \"example\" {\n  mount = \"kv\"\n  name = \"ssh/${var.vm_name}\"\n  data_json = jsonencode(\n    {\n      public_key_openssh  = tls_private_key.ssh_key.public_key_openssh,\n      private_key_openssh = tls_private_key.ssh_key.private_key_openssh,\n      public_key_pem      = tls_private_key.ssh_key.public_key_pem,\n      private_key_pem     = tls_private_key.ssh_key.private_key_pem\n      username            = var.vm_admin\n    }\n  )\n}</code></pre><p>Next, load the public key onto the machine. Your target cloud in this post is Microsoft Azure, so use <a href=\"https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs\">Terraform’s Azure provider</a> to create the VM and its associated networking components. The Azure provider’s resource called <a href=\"https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/linux_virtual_machine\"><code>azurerm_linux_virtual_machine</code></a> has an <a href=\"https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/linux_virtual_machine#admin_ssh_key\"><code>admin_ssh_key</code></a> block that defines the admin username and SSH public key. The VM resource configuration references the key-pair created earlier:</p>\n<pre><code>admin_ssh_key {\n    username   = var.vm_admin_username #adminuser\n    public_key = tls_private_key.ssh_key.public_key_openssh\n  }</code></pre><p>When HCP Terraform applies the code, it will create SSH keys, store them in Vault, and provision the VM with the public key added to the SSH <code>authorized_keys</code> file — all in a single codified configuration.</p>\n\n<h3>Human access to the virtual machine</h3>\n\n<p>The next step is to enable human operator access. This involves another product in the Infrastructure Cloud, <a href=\"https://www.boundaryproject.io/\">HashiCorp Boundary</a>. Boundary is a modern privileged access management (PAM) solution, consisting of a control plane and workers that proxy sessions to infrastructure. You can use the HashiCorp-managed <a href=\"https://developer.hashicorp.com/hcp/docs/boundary\">HCP Boundary</a> to run the control plane, register workers, and configure host endpoints with a default port and a protocol to establish a session (known as <em>targets</em>). </p>\n\n<p>The <a href=\"https://registry.terraform.io/providers/hashicorp/boundary/latest/docs\">Boundary Terraform provider</a> orchestrates the steps required to enable human access to the VM, all within the provisioning workflow. Now register the new VM as an <a href=\"https://developer.hashicorp.com/boundary/docs/common-workflows/manage-targets\">SSH target</a> in Boundary and configure a <a href=\"https://developer.hashicorp.com/boundary/docs/concepts/credential-management\">connection between Boundary and Vault</a> to let Boundary <a href=\"https://developer.hashicorp.com/boundary/docs/concepts/credential-management\">inject</a> the SSH private key into the user’s session:</p>\n<pre><code>#Get the Boundary project\ndata \"boundary_scope\" \"project\" {\n  name     = var.boundary_project_name\n  scope_id = var.boundary_scope_id\n}\n\nresource \"vault_token\" \"example\" {\n  policies  = [\"tfc-workload\"]\n  no_parent = true\n  renewable = true\n  period    = \"24h\"\n  metadata = {\n    \"purpose\" = \"boundary credential store token\"\n  }\n}\n#Create credential store\nresource \"boundary_credential_store_vault\" \"example\" {\n  name        = \"HCP Vault\"\n  description = \"HCP Vault Credential Store\"\n  address     = var.vault_addr\n  namespace   = var.vault_namespace\n  token       = vault_token.example.client_token\n  scope_id    = data.boundary_scope.project.id\n}\n\n#Create credential library for the SSH keys\nresource \"boundary_credential_library_vault\" \"example\" {\n  name                = \"${var.vm_name}-sshkeys\"\n  description         = \"VM SSH keys\"\n  credential_store_id = boundary_credential_store_vault.example.id\n  path                = vault_kv_secret_v2.example.path\n  http_method         = \"GET\"\n  credential_type     = \"ssh_private_key\"\n  credential_mapping_overrides = {\n    private_key_attribute = \"private_key_pem\"\n  }\n}\n#Add the VM to Boundary\nresource \"boundary_host_catalog_static\" \"example\" {\n  name        = \"azure-vm-catalog\"\n  description = \"Azure VM Host Catalog\"\n  scope_id    = data.boundary_scope.project.id\n}\nresource \"boundary_host_static\" \"example\" {\n  name            = var.vm_name\n  host_catalog_id = boundary_host_catalog_static.example.id\n  address = azurerm_linux_virtual_machine.example.private_ip_address\n}\n\nresource \"boundary_host_set_static\" \"example\" {\n  name            = \"azure-vm-host-set\"\n  host_catalog_id = boundary_host_catalog_static.example.id\n  host_ids = [\n    boundary_host_static.example.id\n  ]\n}\nresource \"boundary_target\" \"example\" {\n  name         = \"${var.vm_name}-ssh\"\n  description  = \"${var.vm_name}-SSH\"\n  type         = \"ssh\"\n  default_port = \"22\"\n  scope_id     = data.boundary_scope.project.id\n  host_source_ids = [\n    boundary_host_set_static.example.id\n  ]\n  injected_application_credential_source_ids = [\n    boundary_credential_library_vault.example.id\n  ]\n  egress_worker_filter = var.boundary_egress_filter\n  enable_session_recording = true\n  storage_bucket_id        = var.boundary_session_bucket\n}\nresource \"boundary_alias_target\" \"example\" {\n  name                      = \"boundary_alias_target\"\n  description               = \"Alias to target using host boundary_host_static.example\"\n  scope_id                  = \"global\"\n  value                     = \"ssh.${var.vm_name}.boundary\"\n  destination_id            = boundary_target.example.id\n  authorize_session_host_id = boundary_host_static.example.id\n}</code></pre><p>The snippet above shows <a href=\"https://developer.hashicorp.com/boundary/docs/configuration/session-recording\">session recording</a> is enabled on the Boundary target. This is a critical enterprise feature that records every SSH session initiated to that host for later review by security teams and auditors.</p>\n\n<p>By design, the Azure VM does not have a public IP, it's on a private subnet. The users who need access to it reside outside the private network, so this is where Boundary’s <a href=\"https://developer.hashicorp.com/boundary/docs/concepts/connection-workflows/multi-hop\">multi-hop</a> proxying capability comes in. If you place a <a href=\"https://developer.hashicorp.com/boundary/docs/concepts/workers\">Boundary worker</a> into the same Azure virtual network (VNet) as the VM, you can leverage it as a <a href=\"https://www.nginx.com/resources/glossary/reverse-proxy-server/\">reverse proxy</a>. </p>\n\n<p>The Boundary worker in Azure establishes a persistent outbound connection to an upstream worker that resides in HCP. The worker in Azure is configured as an <a href=\"https://developer.hashicorp.com/boundary/docs/configuration/worker#:%7E:text=Ingress%20worker%20filters%20determine%20which%20workers%20you%20connect%20with%20to%20initiate%20a%20session%2C%20and%20egress%20worker%20filters%20determine%20which%20workers%20are%20used%20to%20access%20targets.\">egress worker</a> and given an <code>azure</code> tag and an egress worker filter is set on the Boundary target. The filter matches workers that have the tag <code>azure</code>. When users connect to the Boundary target, Boundary will route the connection only to the workers that match the tags set in the target filter. Here is a diagram to help you visualize the routing:</p>\n<img src=\"https://www.datocms-assets.com/2885/1715106074-boundary-arch.png\" alt=\"Boundary\" /><p>Boundary workers can run on VMs, bare-metal servers, and containers. In this example, the worker will run as a container. This is an efficient way to run Boundary workers since containers are lightweight, fast, and can scale as demand changes. Spin up your Boundary worker in an Azure Container Instance using Terraform:</p>\n<pre><code>resource \"boundary_worker\" \"az_worker\" {\n  scope_id = \"global\"\n  name = \"azure boundary worker 1\"\n}\n\nresource \"azurerm_subnet\" \"cg\" {\n  name                 = \"container-group-subnet\"\n  resource_group_name  = var.resource_group\n  virtual_network_name = azurerm_virtual_network.example.name\n  address_prefixes     = [\"10.0.3.0/24\"]\n  delegation {\n    name = \"delegation\"\n\n    service_delegation {\n      name    = \"Microsoft.ContainerInstance/containerGroups\"\n      actions = [\"Microsoft.Network/virtualNetworks/subnets/join/action\", \"Microsoft.Network/virtualNetworks/subnets/prepareNetworkPolicies/action\"]\n    }\n  }\n}\n\nresource \"azurerm_container_group\" \"container\" {\n  name                = \"boundary-worker-group\"\n  location            = var.az_location\n  resource_group_name = var.resource_group\n  ip_address_type = \"Private\"\n  subnet_ids      = [azurerm_subnet.cg.id]\n  os_type         = \"Linux\"\n  restart_policy  = \"Never\"\n\n  container {\n    name   = \"boundary-worker\"\n    image  = \"hashicorp/boundary-enterprise\"\n    cpu    = 1\n    memory = 2\n\n    ports {\n      port     = 9202\n      protocol = \"TCP\"\n    }\n    environment_variables = { \"HCP_BOUNDARY_CLUSTER_ID\" = var.hcp_boundary_cluster_id, \"WORKER_ACTV_TOKEN\" = boundary_worker.az_worker.controller_generated_activation_token }\n    volume {\n      name       = \"boundary-config\"\n      mount_path = \"/boundary\"\n      git_repo {\n        url = \"https://github.com/bfbarkhouse/hashistack-secure-infra-workflow\"\n      }\n    }\n    commands = [\n      \"/bin/sh\", \"-c\", \"mv /boundary/hashistack-secure-infra-workflow/azure/boundary-worker-config.hcl /boundary/config.hcl; rm -rf /boundary/hashistack-secure-infra-workflow; /usr/local/bin/docker-entrypoint.sh server -config /boundary/config.hcl\"\n    ]\n  }\n}</code></pre><p>You now have a fully automated, codified, and secured access workflow for operator access to the VM. When a <a href=\"https://developer.hashicorp.com/boundary/docs/concepts/security/permissions\">properly authorized</a> user logs into a <a href=\"https://developer.hashicorp.com/boundary/tutorials/hcp-getting-started/hcp-getting-started-desktop-app\">Boundary client</a>, they will see the VM as an available SSH target. When they click the Connect button, Boundary pulls the SSH private key from the Vault key-value path where it's stored and injects it into the session using <a href=\"https://developer.hashicorp.com/boundary/docs/concepts/workers#protocol-decryption\">protocol decryption</a>. The key is never exposed to the user. The user is then proxied and authenticated to the VM within a recorded and time-restricted session. Since Boundary exposes APIs, it can be easily integrated into service management solutions as part of an approval workflow. </p>\n\n<p>Here you can see the target listed in the <a href=\"https://developer.hashicorp.com/boundary/docs/api-clients/desktop\">Boundary Desktop</a> UI:</p>\n<img src=\"https://www.datocms-assets.com/2885/1715108848-boundary-target.png\" alt=\"Boundary\" /><h3>Automatic application secret retrieval</h3>\n\n<p>Now that you’ve solved two of the original challenges, it’s time to address the final one: the workload running on the VM needs to retrieve a secret at runtime. </p>\n\n<p>Having a central secrets management system is a core tenet of modern IT security, but it's only as effective as the level of adoption by developers. Developers could implement <a href=\"https://developer.hashicorp.com/vault/docs/get-started/developer-qs\">application logic</a> to authenticate to Vault, retrieve secrets, and manage the lifecycle of the Vault token. However, this requires application refactoring to make it “Vault-aware”, and developers often don’t have the bandwidth or Vault knowledge needed to refactor. Security teams requiring developers to refactor hundreds (or thousands) of applications creates significant cost and time implications, establishing barriers to adoption of secrets management.</p>\n\n<p>To help lower the barrier to adoption by providing a more scalable and simpler way for applications to integrate with Vault, HashiCorp created <a href=\"https://developer.hashicorp.com/vault/docs/agent-and-proxy/agent\">Vault Agent</a>. Vault Agent is a client daemon mode of the Vault binary. It has several uses, but one of its primary functions is to automatically authenticate to Vault, manage the lifecycle of the token, and fetch secrets from Vault, whether those are static secrets, dynamic credentials, or certificates. Your dev team’s VM will leverage Vault Agent to render secrets for use by applications without the need for custom application logic.</p>\n\n<p>To install Vault Agent on the VM, adhering to the principles of immutability and codification, you will use another component in The Infrastructure Cloud: <a href=\"https://www.packer.io/\">HashiCorp Packer</a>. Packer will codify installation and configuration of the Vault Agent into the VM image, so it's up and running immediately after provisioning the VM. The <a href=\"https://developer.hashicorp.com/packer/tutorials/docker-get-started/get-started-install-cli\">Packer CLI</a> builds the Azure VM image from an <a href=\"https://developer.hashicorp.com/packer/docs/templates/hcl_templates\">HCL template</a>, pushes the image artifact to Azure, and registers the image metadata to <a href=\"https://developer.hashicorp.com/hcp/docs/packer\">HCP Packer</a>. </p>\n\n<p>HCP Packer is a hosted artifact registry that tracks the metadata of your golden images, with features including versioning, release channels, ancestry tracking, revocation, and Terraform integrations. This <a href=\"https://www.packer.io/use-cases/integrate-with-terraform\">Terraform integration</a> is important because it lets developers reference a <a href=\"https://developer.hashicorp.com/packer/tutorials/cloud-production/golden-image-with-hcp-packer#\">golden image</a> by querying HCP Packer rather than hard-coding the image identifier. This ensures that only fully tested, compliant, and patched images are used to provision VMs and containers.</p>\n\n<p>Your VM will use an image built from a Packer HCL template using Azure’s standard Canonical Ubuntu 22.04 LTS as the base. With the help of Packer’s <a href=\"https://developer.hashicorp.com/packer/docs/provisioners/shell\">shell provisioner</a>, it will layer on the Vault Agent binary, its configuration file, a <a href=\"https://systemd.io/\">systemd</a> unit file, and a <a href=\"https://developer.hashicorp.com/vault/docs/agent-and-proxy/agent/template\">Consul template file</a> used to render the secret out to a text file:</p>\n<pre><code>#Stage Vault\n  provisioner \"shell\" {\n    execute_command = \"chmod +x {{ .Path }}; {{ .Vars }} sudo -E sh '{{ .Path }}'\"\n    inline = [\n      \"wget -O- https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg\",\n      \"echo \\\"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\\\" | sudo tee /etc/apt/sources.list.d/hashicorp.list\",\n      \"apt update &amp;&amp; sudo apt install vault\"\n    ]\n    inline_shebang = \"/bin/sh -x\"\n  }\n  provisioner \"shell\" {\n    execute_command = \"chmod +x {{ .Path }}; {{ .Vars }} sudo -E sh '{{ .Path }}'\"\n    inline          = [\"mkdir /vault\"]\n  }\n  provisioner \"file\" {\n    destination = \"/tmp/agent-config.hcl\"\n    source      = \"./agent-config.hcl\"\n  }\n  provisioner \"file\" {\n    destination = \"/tmp/app-secret.ctmpl\"\n    source      = \"./app-secret.ctmpl\"\n  }\n  provisioner \"file\" {\n    destination = \"/tmp/vault.service\"\n    source      = \"./vault.service\"\n  }\n  provisioner \"shell\" {\n    execute_command = \"chmod +x {{ .Path }}; {{ .Vars }} sudo -E sh '{{ .Path }}'\"\n    inline          = [\"mv /tmp/agent-config.hcl /etc/vault.d\"]\n  }\n  provisioner \"shell\" {\n    execute_command = \"chmod +x {{ .Path }}; {{ .Vars }} sudo -E sh '{{ .Path }}'\"\n    inline          = [\"mv /tmp/app-secret.ctmpl /etc/vault.d\"]\n  }\n  provisioner \"shell\" {\n    execute_command = \"chmod +x {{ .Path }}; {{ .Vars }} sudo -E sh '{{ .Path }}'\"\n    inline          = [\"mv /tmp/vault.service /usr/lib/systemd/system\"]\n  }\n  provisioner \"shell\" {\n    execute_command = \"chmod +x {{ .Path }}; {{ .Vars }} sudo -E sh '{{ .Path }}'\"\n    inline          = [\"systemctl enable vault.service\"]\n  }\n}</code></pre><p>Keeping in mind the need to secure credentials across the workflow, you need to decide how Vault Agent should authenticate to Vault. There needs to be an initial credential/identity to log in to Vault and get the token to use on subsequent requests for secrets. </p>\n\n<p>One benefit of using a cloud platform to host infrastructure is that the platform creates <a href=\"https://learn.microsoft.com/en-us/entra/identity/managed-identities-azure-resources/overview\">identities for resources</a> and lets external systems validate those identities. Using <a href=\"https://developer.hashicorp.com/vault/docs/auth/azure\">Vault’s Azure Authentication Method</a>, you can configure <a href=\"https://developer.hashicorp.com/vault/docs/agent-and-proxy/autoauth/methods/azure\">Vault Agent auto-auth</a> with the VM’s Azure-managed identity JWT token and map the identity to a Vault role that has ACL policy access to the application secret. This resolves the “<a href=\"https://developer.hashicorp.com/vault/tutorials/app-integration/secure-introduction\">secure introduction</a>” problem by proving that the VM is a legitimate recipient for the secret and avoids managing yet another set of credentials. To configure the integration, log in to Vault, set up the Azure Authentication Method, and create a role for the agent:</p>\n<pre><code>#Upload the policy\nvault policy write vault-agent vault-agent-policy.hcl\n\n#Configure Azure auth method for Vault Agent\nvault auth enable azure\nvault write auth/azure/config \\\n    tenant_id=$YOUR_TENANT_ID \\\n    resource=https://management.azure.com/ \\\n    client_id=$YOUR_CLIENT_ID \\\n    client_secret=$YOUR_CLIENT_SECRET\n\n#Create the role\nvault write auth/azure/role/vault-agent-role \\\n    policies=\"vault-agent\" \\\n    bound_subscription_ids=$YOUR_SUBSCRIPTION_ID</code></pre><p>In the example configuration above, any resource from the Azure subscription can use the role <code>\"vault-agent-role\"</code> and access secrets based on its ACL policy <code>\"vault-agent\"</code>. You can, of course, define finer-grained bindings. </p>\n\n<p>Here’s the auto-auth snippet from the agent config file to include in the Packer image:</p>\n<pre><code>auto_auth {\n  method {\n    type = \"azure\"\n    namespace = \"admin\"\n    config = {\n      authenticate_from_environment = true\n      role = \"vault-agent-role\"\n      resource = \"https://management.azure.com/\"\n    }\n  }</code></pre><p>After <a href=\"https://developer.hashicorp.com/packer/docs/commands/build\">building the image</a> and <a href=\"https://developer.hashicorp.com/hcp/docs/packer/store/push-metadata?page=packer&amp;page=store-image-metadata&amp;page=packer-template-configuration\">registering it in HCP Packer</a>, the next step is to configure Terraform to provision the VM from it. First you need to query the latest version from HCP Packer using the <a href=\"https://registry.terraform.io/providers/hashicorp/hcp/latest/docs\">HCP provider</a>:</p>\n<pre><code># Locate the Packer built image\ndata \"hcp_packer_artifact\" \"secure-infra-workflow\" {\n  bucket_name   = var.packer_bucket_name\n  channel_name  = var.packer_channel_name\n  platform      = var.packer_platform\n  region        = var.packer_region\n}</code></pre><p>Then set an environment variable on the VM that tells Vault Agent the address of the Vault cluster to connect to. By using variables injected at provision time, you can use the same VM image across different environments such as dev, test, staging, and production. Use <a href=\"https://registry.terraform.io/providers/hashicorp/template/latest/docs/data-sources/cloudinit_config\">cloud-init</a> to do this:</p>\n<pre><code>#Injecting Vault cluster address to .env file used by systemd\ndata \"template_cloudinit_config\" \"vault-config\" {\n  gzip = true\n  base64_encode = true\n  part {\n    content_type = \"text/cloud-config\"\n    content = \"bootcmd: [echo VAULT_ADDR=${var.vault_addr} &gt;&gt; /etc/vault.d/vault.env]\"\n  }\n}</code></pre><p>The code to provision the Linux VM resource <a href=\"https://developer.hashicorp.com/hcp/docs/packer/store/reference\">references the image you looked up in HCP Packer</a> and inserts the cloud-init config into the VM’s <a href=\"https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/linux_virtual_machine#custom_data\">custom_data</a>. Notice the VM has a system-assigned identity:</p>\n<pre><code>resource \"azurerm_linux_virtual_machine\" \"example\" {\n  name                = var.vm_name\n  resource_group_name = var.resource_group\n  location            = var.az_location\n  size                = var.vm_size\n  source_image_id = data.hcp_packer_artifact.secure-infra-workflow.external_identifier\n  admin_username      = var.vm_admin\n  custom_data = data.template_cloudinit_config.vault-config.rendered\n identity {\n    type = \"SystemAssigned\"\n }</code></pre><h3>Putting it all together</h3>\n\n<p>All the pieces to solve the three security challenges are now codified with Terraform. Run an HCP Terraform plan in your workspace and review the output:</p>\n<img src=\"https://www.datocms-assets.com/2885/1715109478-tf-plan.png\" alt=\"Resources\" /><p>Now you have one provisioning workflow to create and store SSH keys, create a VM for an application from a golden image, access secrets, and register the VM into a remote access system.</p>\n\n<p>Note that there is a <a href=\"https://developer.hashicorp.com/packer/tutorials/hcp/setup-tfc-run-task\">Terraform run task</a> configured that validates the Packer image being used is valid and not revoked:</p>\n<img src=\"https://www.datocms-assets.com/2885/1715109493-run-task.png\" alt=\"HCP\" /><p>Apply the plan and provision everything. Log into the VM through Boundary to verify Vault Agent rendered the application secret to the file as expected:</p>\n<img src=\"https://www.datocms-assets.com/2885/1715109519-vault-secret.png\" alt=\"Vault\" /><h2>Getting started with an iterative approach</h2>\n\n<p>While it might seem daunting to adopt all the components presented in this blog post, you can get started with an iterative approach. Signing up for the HashiCorp Cloud Platform gives you immediate access to <a href=\"https://portal.cloud.hashicorp.com/sign-up\">free-tier versions</a> of all the products used in this tutorial.</p>\n\n<p>It makes sense to start your <a href=\"https://www.hashicorp.com/infrastructure-cloud\">Infrastructure Cloud journey</a> by getting your secrets under management with HCP Vault Dedicated, then move on to adopting infrastructure as code with HCP Terraform and scale up to codified image management with HCP Packer and identity-based access management with HCP Boundary. The investment to enable a secure provisioning workflow will be well worth it to strengthen and speed up your organization’s ILM and SLMSecurity Lifecycle Management practices. </p>\n\n<p>For more tutorials on using HashiCorp solutions, visit our <a href=\"https://developer.hashicorp.com/\">HashiCorp Developer</a> education site and watch our <a href=\"https://www.youtube.com/playlist?list=PL81sUbsFNc5YMJ7Diq1fB54KqedNNJk1O\">demo video series</a>. If you’d like to discuss your specific infrastructure and security transformation challenges, our sales and solutions engineers <a href=\"https://www.hashicorp.com/contact-sales\">are here to listen</a>.</p>\n\n<p><em>The full source code referenced in this post can be found in the<a href=\"https://github.com/bfbarkhouse/hashistack-secure-infra-workflow/tree/main/azure\"> Secure Infrastructure Workflow GitHub repo</a>.</em></p>\n","author":"Ben Barkhouse","siteTitle":"HashiCorp Blog","siteHash":"219aa6310b3388f2335eba49871f4df9581f2c58eaeb5e498363b54e835b7001","entryHash":"5f40df5c818cbf198d19c6f9d269b6dc0c04cb2438fdac9e5fe41b2519134f24","category":"Tech"}