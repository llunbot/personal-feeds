{"title":"Sourcegraph ลองใช้ Gemini ความยาวอินพุต 1M ช่วยแนะนำโค้ด ผลลัพธ์ออกมาดีขึ้นชัดเจนจากโมเดล 10k","link":"https://www.blognone.com/node/143042","date":1731289900000,"content":"<div><div><div><p>กูเกิลจับมือกับบริษัท Sourcegraph ผู้พัฒนา AI ช่วยเขียนโค้ดชื่อ <a href=\"https://sourcegraph.com/cody\">Cody</a> ทดลองนำ<a href=\"https://www.blognone.com/node/138226\">โมเดล Gemini 1.5 ที่รองรับอินพุตขนาดยาว 1 ล้านโทเคน</a> ว่าช่วยให้คุณภาพของคำตอบดีขึ้นอย่างไร</p>\n<p>Cody เป็นการนำ AI มาอ่านโค้ดภายในขององค์กรลูกค้า เพื่อช่วยให้ค้นหาและแนะนำการเขียนโค้ดใหม่ ใช้ร่วมกับ IDE ยอดนิยมทั้ง Visual Studio และตระกูล JetBrains ได้ โมเดลภาษาที่ Cody เลือกใช้งานเป็นโมเดลยอดนิยมหลายตัวในตลาด เช่น Claude 3/3.5, GPT-4o, Gemini, Mixtral (ลูกค้าเลือกเองโมเดลได้) โดยโมเดลที่ใช้งานในระดับโปรดักชันมีขนาด context window ยาว 10,000 โทเคน (10k)</p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/a8f8912785238fd2013542577fd2a1b0.png\" /></p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/329eed9edacdcf979f0681d6141c03f5.png\" /></p>\n<p>Sourcegraph ทดลองนำ Gemini 1.5 Flash ขนาด 1M context window สลับมาใช้งานแทน แล้วเปรียบเทียบผลลัพธ์ใน 4 มิติ ได้แก่ Essential Recall, Essential Concision, Helpfulness, Hallucination</p>\n<p>คำตอบที่ได้จากการใช้โมเดลที่รองรับอินพุตยาวขึ้น พบว่าได้คำตอบผลลัพธ์ที่ละเอียด ครบถ้วนกว่าเดิม (more comprehensive, authoritative answer) ลดการเสริมเนื้อหาที่ไม่สำคัญหรือไม่เกี่ยวข้องเข้ามาในคำตอบ (less noise and irrelevant filler) เพราะโมเดลไม่จำเป็นต้องเพิ่มข้อมูลอื่นๆ มาเสริมคำตอบที่ตรงเป้าอยู่แล้ว (ดูตัวอย่างคำถามและคำตอบได้จากที่มา)</p>\n<p>ข้อเสียของการใช้โมเดลที่อินพุตยาว 1M คือเพิ่มระยะเวลาในการรอคำตอบโทเคนแรก (time to first token) อย่างชัดเจน โดยระยะเวลาแปรผันตรงเป็นเชิงเส้นกับความยาวของอินพุตที่ใส่เข้าไป ซึ่ง Sourcegraph บอกว่าแก้ปัญหาด้วยการ prefetch ข้อมูลบางส่วนไปก่อนการรันโมเดล แล้วแคชเก็บไว้ ช่วยลดระยะเวลารอจาก 30-40 วินาทีลงมาเหลือ 5 วินาทีได้</p>\n<p>ที่มา - <a href=\"https://sourcegraph.com/blog/towards-infinite-context-for-code\">Sourcegraph</a>, <a href=\"https://developers.googleblog.com/en/supercharging-ai-coding-assistants-with-massive-context/\">Google Cloud</a></p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/8dbd9afeb01ee65dcdad7e410d288d59.png\" /></p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/f7152d7c4a79103ad256089f1b941bab.png\" /></p>\n</div></div></div><div><div>Topics: </div><div><div><a href=\"/topics/sourcegraph\">Sourcegraph</a></div><div><a href=\"/topics/gemini\">Gemini</a></div><div><a href=\"/topics/development\">Development</a></div><div><a href=\"/topics/llm\">LLM</a></div><div><a href=\"/topics/google\">Google</a></div></div></div>","author":"mk","siteTitle":"Blognone","siteHash":"ededadcf18490b3937e7dd89ebe8c00dc129addbdf1ebe4aff1f458146693da0","entryHash":"e39e5a2160c86a7ac3c4f843ae603a2149ca831c6214cc35666624b4aa5dc2ff","category":"Thai"}