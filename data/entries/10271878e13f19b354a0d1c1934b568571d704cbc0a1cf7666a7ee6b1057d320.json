{"title":"PyTorch ทดสอบรัน LLM ด้วย OpenAI Triton แทน CUDA พบอสูงสุด 82% ของ CUDA","link":"https://www.blognone.com/node/141893","date":1725784558000,"content":"<div><div><div><p>ทีมวิศวกรจาก IBM และ Meta รายงานถึงการทดลองเปลี่ยนเคอร์เนลการรัน LLM ใน PyTorch จากเดิมที่ใช้ CUDA เป็นหลัก มาเป็น<a href=\"https://github.com/triton-lang/triton\">ภาษา Triton ของ OpenAI</a> โดยพบว่าประสิทธิภาพเริ่มใกล้เคียงกับ CUDA</p>\n<p><a href=\"https://openai.com/index/triton/\">OpenAI เปิดตัวโครงการ Triton มาตั้งแต่ปี 2021</a> โดยมุ่งจะพัฒนาภาษาที่ทำให้โปรแกรมเมอร์เขียนโปรแกรมโดยตรงบนชิปกราฟิกได้ง่ายขึ้น นอกจากการถอด CUDA แล้วยังต้องเลือกเอนจิน Flash Attention มาแทน cuDNN Flash Attention เพื่อรันโมเดล LLM พบว่า AMD Flash Attention ทำงานได้ครบถ้วนทุกโหมด</p>\n<p>ประสิทธิภาพโดยรวมของการรัน LLM โดยถอด CUDA ออกทั้งหมดเช่นนี้ สามารถรันได้ที่ 76-78% ของ CUDA บนชิป A100 และได้ 62-82% บนชิป H100</p>\n<p>CUDA เป็นจุดขายสำคัญของชิป NVIDIA ที่ทำให้นักพัฒนาแน่ใจว่าจะสามารถรันโมเดลปัญญาประดิษฐ์ต่างๆ ได้ประสิทธิภาพดี และเข้ากับโมเดลต่างๆ ได้ครบถ้วน แม้ชิปแบรนด์อื่นๆ จะชูความได้เปรียบราคาถูกกว่าก็ตาม</p>\n<p>ที่มา - <a href=\"https://pytorch.org/blog/cuda-free-inference-for-llms/\">PyTorch</a></p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/ba91032f8ad037e3e21bdbf0028fc16c.png\" /></p>\n</div></div></div><div><div>Topics: </div><div><div><a href=\"/topics/pytorch\">PyTorch</a></div><div><a href=\"/topics/cuda\">CUDA</a></div><div><a href=\"/topics/artificial-intelligence\">Artificial Intelligence</a></div></div></div>","author":"lew","siteTitle":"Blognone","siteHash":"ededadcf18490b3937e7dd89ebe8c00dc129addbdf1ebe4aff1f458146693da0","entryHash":"10271878e13f19b354a0d1c1934b568571d704cbc0a1cf7666a7ee6b1057d320","category":"Thai"}