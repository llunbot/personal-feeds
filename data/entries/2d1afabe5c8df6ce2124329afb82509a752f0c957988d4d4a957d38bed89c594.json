{"title":"TransMLA: Multi-head latent attention is all you need","link":"https://arxiv.org/abs/2502.07864","date":1747106987000,"content":"<a href=\"https://news.ycombinator.com/item?id=43969442\">Comments</a>","author":"","siteTitle":"Hacker News","siteHash":"37bb545430005dba450c1e40307450d8e4e791b434e83f3d38915ebad510fd50","entryHash":"2d1afabe5c8df6ce2124329afb82509a752f0c957988d4d4a957d38bed89c594","category":"Tech"}