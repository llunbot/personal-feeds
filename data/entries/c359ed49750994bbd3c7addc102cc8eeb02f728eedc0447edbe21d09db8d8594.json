{"title":"กูเกิลเผยรายละเอียดเครื่อง TPU v4 ใช้สวิตช์แสงเชื่อมต่อชิป, แรงกว่า NVIDIA A100","link":"https://www.blognone.com/node/133357","date":1681028684000,"content":"<div><div><div><p>กูเกิลเผยรายละเอียดของซูเปอร์คอมพิวเตอร์ที่ใช้ชิปออกแบบเอง Tensor Processing Unit (TPU) v4 ซึ่งเริ่มใช้ในโปรดักชันมาตั้งแต่ปี 2020 (แต่เพิ่งเผยรายละเอียดปี 2023) ว่าสามารถยกระดับประสิทธิภาพ machine learning ได้เกือบ 10 เท่าจากเครื่อง TPU v3 และสามารถเอาชนะเครื่องที่ใช้จีพียู NVIDIA A100 ได้ด้วย</p>\n<p>TPU v4 เปิดตัวต่อสาธารณะเมื่อปี 2021 และ<a href=\"https://www.blognone.com/node/129166\">ทำผลงานเบนช์มาร์คด้าน AI ได้ดี</a> เรื่องใหม่ที่กูเกิลเปิดเผยเพิ่มเติมในรอบนี้คือเครื่อง TPU v4 มีฟีเจอร์สำคัญ 2 ประการ</p>\n<ul>\n<li>ใช้สวิตช์แบบแสง (Optical Circuit Switching)</li>\n<li>มีตัวช่วยประมวลผลข้อมูลชื่อ SparseCore</li>\n</ul>\n<p>ฟีเจอร์ 2 อย่างนี้ช่วยให้เครื่อง TPU v4 สามารถรีดประสิทธิภาพงานด้าน AI ได้ดีขึ้นกว่าเดิม</p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/b997fdc91df8894c78f68b6564b7efa5.jpg\" /></p>\n<h3>Optical Circuit Switching</h3>\n<p>ชิป TPU ถูกออกแบบมาให้ต่อกันเป็นคลัสเตอร์ขนาดใหญ่ มีชิปจำนวนมาก ซึ่งกูเกิลตั้งเป้าว่า TPU v4 ต้องมีจำนวนชิปเพิ่มขึ้น 4 เท่าจาก TPU v3 (4096 ตัว vs 1024 ตัว) จึงเริ่มเจอข้อจำกัดในการเชื่อมต่อ (interconnect) ระหว่างชิปแต่ละตัว เพราะระยะห่างระหว่างชิป (ที่อาจต้องอยู่ข้ามแร็คกัน) เริ่มไกลเกินการส่งข้อมูลด้วยสัญญาณไฟฟ้า ทางออกเดียวจึงเป็นการใช้แสง (optical link)</p>\n<p>ทีมเครือข่ายของกูเกิลได้พัฒนาสวิตช์แสง (Optical Circuit Switching) ชื่อว่า Palomar โดยอาศัยเทคโนโลยีกระจกแบบ 3D Micro-Electro-Mechanical Systems (MEMS) ช่วยสลับวงจรได้รวดเร็วระดับมิลลิวินาที รายละเอียดอ่านได้จากบล็อก <a href=\"https://cloud.google.com/blog/topics/systems/the-evolution-of-googles-jupiter-data-center-network\">Jupiter evolving: Reflecting on Google’s data center network transformation</a></p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/aa340b5cb4854f7309cea02888c39ee3.jpg\" /></p>\n<p>สวิตช์แบบ OCS ยังทำใช้เชื่อมต่อชิป TPU v4 แบบ 3D ได้ (ของ TPU v3 ต่อเป็น 2D) ช่วยให้แบนด์วิดท์ระหว่างกันเพิ่มขึ้น และลดปัญหาชิปบางตัวไม่ทำงานได้ด้วย ด้วยข้อจำกัดของพื้นที่แร็คทำให้กูเกิลเลือกต่อเป็น 4x4x4 ชุดละ 64 ตัว</p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/eed34e789aca5e2c8742c745de979fa7.png\" /></p>\n<p>ตัวอย่างเครื่อง TPU v4 ของจริง จำนวน 8 ชุด ซึ่งคิดเป็น 1/8 ของทั้งระบบ</p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/08dcbcb22f402275183e7ee1463a3cdc.jpg\" /></p>\n<p>TPU v4 ใช้สวิตช์แสง OCS แบบ reconfigurable คือปรับแต่งการวาง topology ของเครือข่ายได้ตามต้องการ ทำให้โครงสร้างของการวางชิป TPU v4 ยืดหยุ่นตามเวิร์คโหลดของงาน AI ประเภทต่างๆ ได้</p>\n<p>ผลในภาพรวมคือ สวิตช์แบบ OCS ทำงานได้เร็วกว่า ประหยัดพลังงานกว่า ราคาถูกกว่า การเชื่อมต่อแบบ Infiniband (ปัจจุบันเป็นของ NVIDIA) ที่นิยมใช้ในเซิร์ฟเวอร์สมรรถนะสูง ตอนนี้ชิ้นส่วน OCS มีต้นทุนน้อยกว่า 5% ของระบบ TPU v4 ทั้งหมด และใช้พลังงานน้อยกว่า 5% ของทั้งระบบเช่นกัน</p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/72e5c67fd1284268bb81a30da8e39847.jpg\" /></p>\n<h3>ตัวช่วยประมวลผล SparseCore</h3>\n<p>งานประมวลผล AI มีหลายประเภท โดยงานประเภทหนึ่งที่พบบ่อยคือ Deep learning recommendation models (DLRM) ซึ่งเป็นงานแนะนำสิ่งที่ผู้ใช้น่าจะชอบ ที่เราคุ้นเคยกันจากผลิตภัณฑ์ต่างๆ ของกูเกิล เช่น Search, Ads, YouTube, Google Play</p>\n<p>อัลกอริทึมแบบ DLRM จะมีชั้นของการนำข้อมูลที่กระจัดกระจายมาจัดให้เป็นหมวดหมู่ เรียกว่า <a href=\"https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture\">embedding</a> ซึ่งจะช่วยให้เทรนโมเดลได้เร็วขึ้น อย่างไรก็ตาม ตารางข้อมูลที่ใช้เปรียบเทียบเพื่อทำ embedding มักมีขนาดใหญ่ ใช้แรมเยอะ แต่ประมวลผลจริงๆ น้อย (รูปแบบเวิร์คโหลดคือหาข้อมูลในตารางขนาดใหญ่) จึงกลายเป็นคอขวดของแรม</p>\n<p>เมื่องานประเภท DLRM มีสัดส่วนการใช้งานราว 25% ของเวิร์คโหลดทั้งหมดในระบบ กูเกิลจึงประดิษฐ์ชิปชื่อ SparseCore มาประมวลผลงานส่วน embedding โดยเฉพาะ แยกจากชิป TensorCore ที่ใช้ประมวลผลโมเดล AI หลัก ซึ่งมีธรรมชาติแตกต่างกัน</p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/106321f9524c749451844ddda2390468.png\" /></p>\n<p>ชิป SparseCore เอาเข้าจริงแล้วเริ่มใช้งานมาตั้งแต่ TPU v2 (แต่เพิ่งมาเผยตัวต่อชาวโลก) และปรับปรุงมาเรื่อยๆ ใน TPU v3 และ v4 จนตอนนี้ช่วยเพิ่มประสิทธิภาพของเวิร์คโหลด DLRM ได้มาก โดยใช้พลังงานเพียง 5% และพื้นที่ชิปเพียง 5% ของ TPU ทั้งหมดเท่านั้น</p>\n<p>ประสิทธิภาพของ TPU v4 เพิ่มขึ้นราว 3 เท่าจาก TPU v3 และหากเทียบกับการใช้ซีพียูมาตรฐานก็สูงกว่ากันถึง 30 เท่าเลยทีเดียว</p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/9075a13f7a32056672141ec0ce42a269.jpg\" /></p>\n<p>ด้วยเทคนิคการปรับแต่งประสิทธิภาพของ TPU v4 หลายอย่างข้างต้น ทำให้เครื่อง TPU v4 สามารถเอาชนะ TPU v3 ได้สบายๆ ประสิทธิภาพต่อวัตต์ดีขึ้น 40%, ประสิทธิภาพดีขึ้นราว 1.5-3 เท่า</p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/e2842af2e539eae4c46e366e92808e03.png\" /></p>\n<p>กูเกิลยังนำเครื่อง TPU v4 ไปเทียบกับเครื่องที่ใช้ชิปประมวลผล AI ของคู่แข่งคือ NVIDIA A100 (ยังไม่ใช่ H100 ตัวใหม่ล่าสุดที่เพิ่งเริ่มออกขายปีนี้) และ Graphcore MK2 IPU ก็สามารถรันเบนช์มาร์ค MLPerf 2.0 เอาชนะ A100 ได้เช่นกัน</p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/d654a9001ba31fed187df52d6d866737.png\" /></p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/b856de7399c0027a1f511f21948573c8.png\" /></p>\n<p>ตัวอย่างลูกค้าที่รันงานบน TPU v4 ผ่าน Google Cloud คือ Midjourney บริการ AI สร้างรูปจากข้อความชื่อดัง และสถาบัน Allen Institute for AI ที่ก่อตั้งโดย Paul Allen ผู้ร่วมก่อตั้งไมโครซอฟท์</p>\n<p>ที่มา - <a href=\"https://cloud.google.com/blog/topics/systems/tpu-v4-enables-performance-energy-and-co2e-efficiency-gains/\">Google Cloud Blog</a>, <a href=\"https://arxiv.org/ftp/arxiv/papers/2304/2304.01433.pdf\">เปเปอร์ฉบับเต็ม (PDF)</a>, <a href=\"https://www.theregister.com/2023/04/06/google_tpuv4_hardware_nvidia/\">The Register</a></p>\n</div></div></div><div><div>Topics: </div><div><div><a href=\"/topics/tpu\">TPU</a></div><div><a href=\"/topics/google\">Google</a></div><div><a href=\"/topics/supercomputer\">Supercomputer</a></div><div><a href=\"/topics/high-performance-computing\">High Performance Computing</a></div><div><a href=\"/topics/google-cloud\">Google Cloud</a></div><div><a href=\"/topics/artificial-intelligence\">Artificial Intelligence</a></div></div></div>","author":"mk","siteTitle":"Blognone","siteHash":"ededadcf18490b3937e7dd89ebe8c00dc129addbdf1ebe4aff1f458146693da0","entryHash":"c359ed49750994bbd3c7addc102cc8eeb02f728eedc0447edbe21d09db8d8594","category":"Thai"}