{"title":"คลาวด์รายหลักเปิดให้บริการ Llama 3.1 โดยพร้อมเพรียง, Meta อนุญาตให้นำ output ไปฝึกโมเดลของตัวเองได้","link":"https://www.blognone.com/node/141059","date":1721810127000,"content":"<div><div><div><p>หลังจากเมื่อคืนที่ผ่านมา Meta เปิดตัว Llama 3.1 โมเดลปัญญาประดิษฐ์ บรรดาคลาวด์รายเล็กและรายใหญ่ก็ประกาศเปิดบริการ Llama 3.1 โดยพร้อมเพรียงกัน โดยบริการที่ครบถ้วนที่สุดคือ Microsoft Azure เปิดให้บริการพร้อมกับประกาศราคาออกมาพร้อมกันทั้งสามขนาด Google Cloud นั้นเปิดให้ใช้งานโมเดล 405B แล้วแต่ยังไม่ประกาศราคา ขณะที่ AWS นั้นประกาศราคาเฉพาะรุ่น 70B และ 8B โดยรุ่น 405B ต้องเปิดเคสขอใช้งาน</p>\n<p>คลาวด์เฉพาะทางค่ายต่างๆ ก็ประกาศราคาและเปิดให้บริการออกมาเช่นกัน เช่น Together.AI ประกาศราคาต่ำสุดแต่เป็นรุ่น Turbo ที่ถูก quantized มาแล้วอ แม้บริษัทจะระบุว่าเทคโนโลยีการ quantize ของบริษัทนั้นมีความสามารถใกล้เคียงโมเดลเต็ม ขณะที่ Groq ผู้ให้บริการเฉพาะทางก็เปิดให้บริการรุ่น 70B และ 8B แต่ยังไม่ประกาศราคาออกมา อย่างไรก็ดีหากเทียบราคา Llama 3 ก่อนหน้านี้ก็จะพบว่าราคาถูกกว่าเจ้าอื่นๆ มากอยู่แล้ว</p>\n<p>โครงการรัน LLM บนเครื่องส่วนตัวอย่าง Ollama <a href=\"https://ollama.com/library/llama3.1:405b\">ก็รองรับ Llama 3.1 เป็นที่เรียบร้อยแล้ว</a> อย่างไรก็ดี รุ่น 405B นั้นมีเฉพาะแบบ quantized 4 bit เท่านั้น ไม่มีแบบ FP16 ทางด้าน NVIDIA ก็ประกาศรองรับ Llama 3.1 บน NVIDIA NIM โดยโชว์ว่ามีบริการ NVIDIA NeMo Retriever เต็มรูปแบบ ทำให้องค์กรสามารถสร้างแอปพลิเคชั่นแบบ retrieval-augmented generation (RAG) ได้ในองค์กรอย่างสมบูรณ์</p>\n<p>ความพิเศษของ <a href=\"https://x.com/AIatMeta/status/1815766335219249513\">Llama 3.1 คือสัญญาอนุญาตเปลี่ยนไลเซนส์โดยอนุญาตให้ใช้เอาท์พุตของโมเดลไปใช้งานอื่นๆ ได้</a> ต่างกับโมเดลอื่นๆ เช่น OpenAI ห้ามนำเอาท์พุตไปใช้ฝึกโมเดลอื่นๆ ทำให้องค์กรสามารถออปติไมซ์การทำงานเพิ่มเติม เช่น สร้างชุดข้อมูลจาก Llama 3.1 405B แต่นำไปฝึกกับโมเดล LLM ที่ขนาดเล็กกว่า</p>\n<p>ที่มา - <a href=\"https://nvidianews.nvidia.com/news/nvidia-ai-foundry-custom-llama-generative-models?ncid=so-twit-961037\">NVIDIA</a>, <a href=\"https://aws.amazon.com/blogs/aws/announcing-llama-3-1-405b-70b-and-8b-models-from-meta-in-amazon-bedrock/\">AWS</a>, <a href=\"https://cloud.google.com/blog/products/ai-machine-learning/llama-3-1-on-vertex-ai\">Google Cloud Blog</a>, <a href=\"https://techcommunity.microsoft.com/t5/ai-ai-platform-blog/meta-s-next-generation-model-llama-3-1-405b-is-now-available-on/ba-p/4198379\">Azure Blog</a></p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/fdae978a725f18520246d9c048f0b571.png\" /></p>\n<table>\n<tr>\n<th>Provider</th>\n<th>405B</th>\n<th>70B</th>\n<th>8B</th>\n</tr>\n<tr>\n<td>Azure</td>\n<td>5.33USD / 16USD</td>\n<td>2.68USD / 3.54USD</td>\n<td>0.3USD / 0.61USD</td>\n</tr>\n<tr>\n<td>AWS Bedrock</td>\n<td>N/A</td>\n<td>2.65USD / 3.5USD</td>\n<td>0.3USD / 0.6USD</td>\n</tr>\n<tr>\n<td>Databricks</td>\n<td>10USD / 30USD</td>\n<td>1USD / 3USD</td>\n<td>N/A</td>\n</tr>\n<tr>\n<td>GCP</td>\n<td>N/A</td>\n<td>N/A</td>\n<td>N/A</td>\n</tr>\n<tr>\n<td>Together.AI (quantized)</td>\n<td>5USD / 5USD</td>\n<td>0.88USD / 0.88USD</td>\n<td>0.18USD / 0.18USD</td>\n</tr>\n<tr>\n<td>Groq (Llama 3)</td>\n<td>N/A</td>\n<td>0.59USD / 0.79USD</td>\n<td>0.05USD / 0.08 USD</td>\n</tr>\n</table>\n<p>ราคาของ Llama 3.1 ขนาดต่างๆ โดยคิดราคาต่อ 1 ล้านโทเค็น จากผู้ให้บริการคลาวด์ Google Cloud นั้นยังไม่เปิดเผยราคา <a href=\"https://www.together.ai/blog/meta-llama-3-1\">ขณะที่ Together.AI นั้นเป็นโมเดลแบบย่อ</a> และ Groq ก็ยังไม่เปิดเผยราคาเช่นกัน</p>\n</div></div></div><div><div>Topics: </div><div><div><a href=\"/topics/llama\">Llama</a></div><div><a href=\"/topics/meta\">Meta</a></div><div><a href=\"/topics/llm\">LLM</a></div></div></div>","author":"lew","siteTitle":"Blognone","siteHash":"ededadcf18490b3937e7dd89ebe8c00dc129addbdf1ebe4aff1f458146693da0","entryHash":"0c831617867c23586d724e334335bf02775da0883da8156b5313088fa49de4f7","category":"Thai"}