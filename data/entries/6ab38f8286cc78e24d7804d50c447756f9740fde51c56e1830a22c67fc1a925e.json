{"title":"Amazon Bedrock Guardrails now supports multimodal toxicity detection with image support (preview)","link":"https://aws.amazon.com/blogs/aws/amazon-bedrock-guardrails-now-supports-multimodal-toxicity-detection-with-image-support/","date":1733333896000,"content":"<p>Today, we’re announcing the preview of multimodal toxicity detection with image support in <a href=\"https://aws.amazon.com/bedrock/guardrails/\">Amazon Bedrock Guardrails</a>. This new capability detects and filters out undesirable image content in addition to text, helping you improve user experiences and manage model outputs in your <a href=\"https://aws.amazon.com/ai/generative-ai/\">generative AI</a> applications.</p> \n<p>Amazon Bedrock Guardrails helps you implement safeguards for generative AI applications by filtering undesirable content, redacting personally identifiable information (PII), and enhancing content safety and privacy. You can configure policies for denied topics, content filters, word filters, PII redaction, contextual grounding checks, and Automated Reasoning checks (preview), to tailor safeguards to your specific use cases and responsible AI policies.</p> \n<p>With this launch, you can now use the existing content filter policy in Amazon Bedrock Guardrails to detect and block harmful image content across categories such as hate, insults, sexual, and violence. You can configure thresholds from low to high to match your application’s needs.</p> \n<p>This new image support works with all <a href=\"https://aws.amazon.com/what-is/foundation-models/\">foundation models (FMs)</a> in Amazon Bedrock that support image data, as well as any custom fine-tuned models you bring. It provides a consistent layer of protection across text and image modalities, making it easier to build responsible AI applications.</p> \n<p><a href=\"https://www.linkedin.com/in/terohottinen/\">Tero Hottinen</a>, VP, Head of Strategic Partnerships at <a href=\"http://www.kone.com\">KONE</a>, envisions the following use case:</p> \n<blockquote>\n <p>In its ongoing evaluation, KONE recognizes the potential of Amazon Bedrock Guardrails as a key component in protecting gen AI applications, particularly for relevance and contextual grounding checks, as well as the multimodal safeguards. The company envisions integrating product design diagrams and manuals into its applications, with Amazon Bedrock Guardrails playing a crucial role in enabling more accurate diagnosis and analysis of multimodal content.</p>\n</blockquote> \n<p>Here’s how it works.</p> \n<p><strong><u>Multimodal toxicity detection in action<br /> </u></strong>To get started, create a guardrail in the <a href=\"https://aws.amazon.com/console/\">AWS Management Console</a> and configure the content filters for either text or image data or both. You can also use <a href=\"https://aws.amazon.com/developer/tools/\">AWS SDKs</a> to integrate this capability into your applications.</p> \n<p><strong>Create guardrail<br /> </strong>On the <a href=\"https://console.aws.amazon.com/console/home\">console</a>, navigate to<strong> Amazon Bedrock</strong> and select <strong>Guardrails</strong>. From there, you can create a new guardrail and use the existing content filters to detect and block image data in addition to text data. The categories for <strong>Hate</strong>, <strong>Insults</strong>, <strong>Sexual</strong>, and <strong>Violence</strong> under <strong>Configure content filters</strong> can be configured for either text or image content or both. The <strong>Misconduct</strong> and <strong>Prompt attacks</strong> categories can be configured for text content only.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/23/2024-guardrails-toxicity-7.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/23/2024-guardrails-toxicity-7.png\" alt=\"Amazon Bedrock Guardrails Multimodal Support\" width=\"1442\" height=\"1138\" /></a></p> \n<p>After you’ve selected and configured the content filters you want to use, you can save the guardrail and start using it to build safe and responsible generative AI applications.</p> \n<p>To test the new guardrail in the console, select the guardrail and choose <strong>Test</strong>. You have two options: test the guardrail by choosing and invoking a model or to test the guardrail without invoking a model by using the Amazon Bedrock Guardrails independent <code>ApplyGuardail</code> API.</p> \n<p>With the <code>ApplyGuardrail</code> API, you can validate content at any point in your application flow before processing or serving results to the user. You can also use the API to evaluate inputs and outputs for any self-managed (custom), or third-party FMs, regardless of the underlying infrastructure. For example, you could use the API to evaluate a <a href=\"https://www.llama.com/\">Meta Llama 3.2</a> model hosted on <a href=\"https://aws.amazon.com/sagemaker/\">Amazon SageMaker</a> or a <a href=\"https://mistral.ai/news/mistral-nemo/\">Mistral NeMo</a> model running on your laptop.</p> \n<p><strong>Test guardrail by choosing and invoking a model<br /> </strong>Select a model that supports image inputs or outputs, for example, Anthropic’s Claude 3.5 Sonnet. Verify that the prompt and response filters are enabled for image content. Next, provide a prompt, upload an image file, and choose <strong>Run</strong>.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/16/2024-guardrails-toxicity-2-1.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/16/2024-guardrails-toxicity-2-1.png\" alt=\"Amazon Bedrock Guardrails Multimodal Support\" width=\"1165\" height=\"803\" /></a></p> \n<p>In my example, Amazon Bedrock Guardrails intervened. Choose <strong>View trace</strong> for more details.</p> \n<p>The guardrail trace provides a record of how safety measures were applied during an interaction. It shows whether Amazon Bedrock Guardrails intervened or not and what assessments were made on both input (prompt) and output (model response). In my example, the content filters blocked the input prompt because they detected insults in the image with a high confidence.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/16/2024-guardrails-toxicity-3.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/16/2024-guardrails-toxicity-3.png\" alt=\"Amazon Bedrock Guardrails Multimodal Support\" width=\"1172\" height=\"890\" /></a></p> \n<p><strong>Test guardrail without invoking a model<br /> </strong>In the console, choose <strong>Use Guardrails independent API</strong> to test the guardrail without invoking a model. Choose whether you want to validate an input prompt or an example of a model generated output. Then, repeat the steps from before. Verify that the prompt and response filters are enabled for image content, provide the content to validate, and choose <strong>Run</strong>.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/16/2024-guardrails-toxicity-5.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/16/2024-guardrails-toxicity-5.png\" alt=\"Amazon Bedrock Guardrails Multimodal Support\" width=\"1163\" height=\"799\" /></a></p> \n<p>I reused the same image and input prompt for my demo, and Amazon Bedrock Guardrails intervened again. Choose <strong>View trace</strong> again for more details.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/16/2024-guardrails-toxicity-6.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/16/2024-guardrails-toxicity-6.png\" alt=\"Amazon Bedrock Guardrails Multimodal Support\" width=\"1176\" height=\"840\" /></a></p> \n<p><strong><u>Join the preview<br /> </u></strong>Multimodal toxicity detection with image support is available today in preview in Amazon Bedrock Guardrails in the US East (N. Virginia, Ohio), US West (Oregon), Asia Pacific (Mumbai, Seoul, Singapore, Tokyo), Europe (Frankfurt, Ireland, London), and AWS GovCloud (US-West) <a href=\"https://docs.aws.amazon.com/glossary/latest/reference/glos-chap.html#region\">AWS Regions</a>. To learn more, visit <a href=\"https://aws.amazon.com/bedrock/guardrails/\">Amazon Bedrock Guardrails</a>.</p> \n<p>Give the multimodal toxicity detection content filter a try today in the <a href=\"https://console.aws.amazon.com/bedrock/home#/guardrails\">Amazon Bedrock console</a> and let us know what you think! Send feedback to <a href=\"https://repost.aws/tags/TAQeKlaPaNRQ2tWB6P7KrMag\">AWS re:Post for Amazon Bedrock</a> or through your usual AWS Support contacts.</p> \n<p>— <a href=\"https://www.linkedin.com/in/antje-barth/\" target=\"_blank\">Antje</a></p>","author":"Antje Barth","siteTitle":"AWS News Blog","siteHash":"6093e072e4117ec22616e844cb857d03ca62c57a411a8affc77cb5e8b6b15bf6","entryHash":"6ab38f8286cc78e24d7804d50c447756f9740fde51c56e1830a22c67fc1a925e","category":"Tech"}