{"title":"Amazon SageMaker adds new inference capabilities to help reduce foundation model deployment costs and latency","link":"https://aws.amazon.com/blogs/aws/amazon-sagemaker-adds-new-inference-capabilities-to-help-reduce-foundation-model-deployment-costs-and-latency/","date":1701285276000,"content":"<p>Today, we are announcing new <a href=\"https://aws.amazon.com/sagemaker/deploy/\">Amazon SageMaker</a> inference capabilities that can help you optimize deployment costs and reduce latency. With the new inference capabilities, you can deploy one or more foundation models (FMs) on the same SageMaker endpoint and control how many accelerators and how much memory is reserved for each FM. This helps to improve resource utilization, reduce model deployment costs on average by 50 percent, and lets you scale endpoints together with your use cases.</p> \n<p>For each FM, you can define separate scaling policies to adapt to model usage patterns while further optimizing infrastructure costs. In addition, SageMaker actively monitors the instances that are processing inference requests and intelligently routes requests based on which instances are available, helping to achieve on average 20 percent lower inference latency.</p> \n<p><strong><u>Key components<br /> </u></strong>The new inference capabilities build upon SageMaker real-time inference endpoints. As before, you create the SageMaker endpoint with an endpoint configuration that defines the instance type and initial instance count for the endpoint. The model is configured in a new construct, an inference component. Here, you specify the number of accelerators and amount of memory you want to allocate to each copy of a model, together with the model artifacts, container image, and number of model copies to deploy.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/10/27/sm-mme-01.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/10/27/sm-mme-01.png\" alt=\"Amazon SageMaker - MME\" width=\"884\" height=\"452\" /></a></p> \n<p>Let me show you how this works.</p> \n<p><strong><u>New inference capabilities in action<br /> </u></strong>You can start using the new inference capabilities from <a href=\"https://aws.amazon.com/sagemaker/studio/\">SageMaker Studio</a>, the <a href=\"https://sagemaker.readthedocs.io/en/stable/\">SageMaker Python SDK</a>, and the <a href=\"https://aws.amazon.com/developer/tools/\">AWS SDKs</a> and <a href=\"https://aws.amazon.com/cli/\">AWS Command Line Interface (AWS CLI)</a>. They are also supported by <a href=\"https://aws.amazon.com/cloudformation/\">AWS CloudFormation</a>.</p> \n<p>For this demo, I use the <a href=\"https://aws.amazon.com/sdk-for-python/\">AWS SDK for Python (Boto3)</a> to deploy a copy of the <a href=\"https://huggingface.co/databricks/dolly-v2-7b\">Dolly v2 7B model</a> and a copy of the <a href=\"https://huggingface.co/google/flan-t5-xxl\">FLAN-T5 XXL model</a> from the <a href=\"https://huggingface.co/models\">Hugging Face model hub</a> on a SageMaker real-time endpoint using the new inference capabilities.</p> \n<p><strong>Create a SageMaker endpoint configuration</strong></p> \n<pre><code>import boto3\nimport sagemaker\n\nrole = sagemaker.get_execution_role()\nsm_client = boto3.client(service_name=\"sagemaker\")\n\nsm_client.create_endpoint_config(\n    EndpointConfigName=endpoint_config_name,\n    ExecutionRoleArn=role,\n    ProductionVariants=[{\n        \"VariantName\": \"AllTraffic\",\n        \"InstanceType\": \"ml.g5.12xlarge\",\n        \"InitialInstanceCount\": 1,\n\t\t\"RoutingConfig\": {\n            \"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"\n        }\n    }]\n)</code></pre> \n<p><strong>Create the SageMaker endpoint</strong></p> \n<pre><code>sm_client.create_endpoint(\n    EndpointName=endpoint_name,\n    EndpointConfigName=endpoint_config_name,\n)</code></pre> \n<p>Before you can create the inference component, you need to create a SageMaker-compatible model and specify a container image to use. For both models, I use the <a href=\"https://huggingface.co/blog/sagemaker-huggingface-llm\">Hugging Face LLM Inference Container</a> for Amazon SageMaker. These deep learning containers (DLCs) include the necessary components, libraries, and drivers to host large models on SageMaker.</p> \n<p><strong>Prepare the Dolly v2 model</strong></p> \n<pre><code>from sagemaker.huggingface import get_huggingface_llm_image_uri\n\n# Retrieve the container image URI\nhf_inference_dlc = get_huggingface_llm_image_uri(\n  \"huggingface\",\n  version=\"0.9.3\"\n)\n\n# Configure model container\ndolly7b = {\n    'Image': hf_inference_dlc,\n    'Environment': {\n        'HF_MODEL_ID':'databricks/dolly-v2-7b',\n        'HF_TASK':'text-generation',\n    }\n}\n\n# Create SageMaker Model\nsagemaker_client.create_model(\n    ModelName        = \"dolly-v2-7b\",\n    ExecutionRoleArn = role,\n    Containers       = [dolly7b]\n)</code></pre> \n<p><strong>Prepare the FLAN-T5 XXL model</strong></p> \n<pre><code># Configure model container\nflant5xxlmodel = {\n    'Image': hf_inference_dlc,\n    'Environment': {\n        'HF_MODEL_ID':'google/flan-t5-xxl',\n        'HF_TASK':'text-generation',\n    }\n}\n\n# Create SageMaker Model\nsagemaker_client.create_model(\n    ModelName        = \"flan-t5-xxl\",\n    ExecutionRoleArn = role,\n    Containers       = [flant5xxlmodel]\n)</code></pre> \n<p>Now, you’re ready to create the inference component.</p> \n<p><strong>Create an inference component for each model<br /> </strong>Specify an inference component for each model you want to deploy on the endpoint. Inference components let you specify the SageMaker-compatible model and the compute and memory resources you want to allocate. For CPU workloads, define the number of cores to allocate. For accelerator workloads, define the number of accelerators. <code>RuntimeConfig</code> defines the number of model copies you want to deploy.</p> \n<pre><code># Inference compoonent for Dolly v2 7B\nsm_client.create_inference_component(\n    InferenceComponentName=\"IC-dolly-v2-7b\",\n    EndpointName=endpoint_name,\n    VariantName=variant_name,\n    Specification={\n        \"ModelName\": \"dolly-v2-7b\",\n        \"ComputeResourceRequirements\": {\n\t\t    \"NumberOfAcceleratorDevicesRequired\": 2, \n\t\t\t\"NumberOfCpuCoresRequired\": 2, \n\t\t\t\"MinMemoryRequiredInMb\": 1024\n\t    }\n    },\n    RuntimeConfig={\"CopyCount\": 1},\n)\n\n# Inference component for FLAN-T5 XXL\nsm_client.create_inference_component(\n    InferenceComponentName=\"IC-flan-t5-xxl\",\n    EndpointName=endpoint_name,\n    VariantName=variant_name,\n    Specification={\n        \"ModelName\": \"flan-t5-xxl\",\n        \"ComputeResourceRequirements\": {\n\t\t    \"NumberOfAcceleratorDevicesRequired\": 2, \n\t\t\t\"NumberOfCpuCoresRequired\": 1, \n\t\t\t\"MinMemoryRequiredInMb\": 1024\n\t    }\n    },\n    RuntimeConfig={\"CopyCount\": 1},\n)</code></pre> \n<p>Once the inference components have successfully deployed, you can invoke the models.</p> \n<p><strong>Run inference</strong><br /> To invoke a model on the endpoint, specify the corresponding inference component.</p> \n<pre><code>import json\nsm_runtime_client = boto3.client(service_name=\"sagemaker-runtime\")\npayload = {\"inputs\": \"Why is California a great place to live?\"}\n\nresponse_dolly = sm_runtime_client.invoke_endpoint(\n    EndpointName=endpoint_name,\n    InferenceComponentName = \"IC-dolly-v2-7b\",\n    ContentType=\"application/json\",\n    Accept=\"application/json\",\n    Body=json.dumps(payload),\n)\n\nresponse_flant5 = sm_runtime_client.invoke_endpoint(\n    EndpointName=endpoint_name,\n    InferenceComponentName = \"IC-flan-t5-xxl\",\n    ContentType=\"application/json\",\n    Accept=\"application/json\",\n    Body=json.dumps(payload),\n)\n\nresult_dolly = json.loads(response_dolly['Body'].read().decode())\nresult_flant5 = json.loads(response_flant5['Body'].read().decode())\n</code></pre> \n<p>Next, you can define separate scaling policies for each model by registering the scaling target and applying the scaling policy to the inference component. Check out the <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deploy-models.html\">SageMaker Developer Guide</a> for detailed instructions.</p> \n<p>The new inference capabilities provide per-model CloudWatch metrics and CloudWatch Logs and can be used with any SageMaker-compatible container image across SageMaker CPU- and GPU-based compute instances. Given support by the container image, you can also use response streaming.</p> \n<p><b><u>Now available<br /> </u></b>The new Amazon SageMaker inference capabilities are available today in AWS Regions US East (Ohio, N. Virginia), US West (Oregon), Asia Pacific (Jakarta, Mumbai, Seoul, Singapore, Sydney, Tokyo), Canada (Central), Europe (Frankfurt, Ireland, London, Stockholm), Middle East (UAE), and South America (São Paulo). For pricing details, visit <a href=\"https://aws.amazon.com/sagemaker/pricing\">Amazon SageMaker Pricing</a>. To learn more, visit <a href=\"https://aws.amazon.com/sagemaker/deploy/\">Amazon SageMaker</a>.</p> \n<p><span><strong>Get started</strong></span><br /> Log in to the <a href=\"https://console.aws.amazon.com/sagemaker/home\">AWS Management Console</a> and deploy your FMs using the new SageMaker inference capabilities today!</p> \n<p>— <a href=\"https://www.linkedin.com/in/antje-barth/\" target=\"_blank\">Antje</a></p>","author":"Antje Barth","siteTitle":"AWS News Blog","siteHash":"6093e072e4117ec22616e844cb857d03ca62c57a411a8affc77cb5e8b6b15bf6","entryHash":"c4eb268a375aaac7dc537320dddf8ec542d5fcc2389499c5132e4efe97a11175","category":"Tech"}