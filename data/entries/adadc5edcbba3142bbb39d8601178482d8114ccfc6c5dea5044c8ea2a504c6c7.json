{"title":"New – Introducing SageMaker Training Compiler","link":"https://aws.amazon.com/blogs/aws/new-introducing-sagemaker-training-compiler/","date":1638381977000,"content":"<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/11/27/Site-Merch_Hopper_ConsoleSign-In-1-1.png\"><img src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/11/27/Site-Merch_Hopper_ConsoleSign-In-1-1.png\" /></a>Today, we’re pleased to announce <span>Amazon SageMaker Training Compiler</span>, a new <a href=\"https://aws.amazon.com/sagemaker/\"><span>Amazon SageMaker</span></a> capability that can accelerate the training of deep learning (DL) models by up to 50%.</p> \n<p>As DL models grow in complexity, so too does the time it can take to optimize and train them. For example, it can take 25,000 GPU-hours to train popular natural language processing (NLP) model “<a href=\"https://arxiv.org/abs/1907.11692\">RoBERTa</a>“. Although there are techniques and optimizations that customers can apply to reduce the time it can take to train a model, these also take time to implement and require a rare skillset. This can impede innovation and progress in the wider adoption of artificial intelligence (AI).</p> \n<p><span><strong>How has this been done to date?<br /> </strong></span>Typically, there are three ways to speed up training:</p> \n<ol> \n <li>Using more powerful, individual machines to process the calculations</li> \n <li>Distributing compute across a cluster of GPU instances to train the model in parallel</li> \n <li>Optimizing model code to run more efficiently on GPUs by utilizing less memory and compute.</li> \n</ol> \n<p>In practice, optimizing machine learning (ML) code is difficult, time-consuming, and a rare skill set to acquire. Data scientists typically write their training code in a Python-based ML framework, such as <a href=\"https://www.tensorflow.org/\">TensorFlow</a> or <a href=\"https://pytorch.org/\">PyTorch</a>, relying on ML frameworks to convert their Python code into mathematical functions that can run on GPUs, commonly known as kernels. However, this translation from the Python code of a user is often inefficient because ML frameworks use pre-built, generic GPU kernels, instead of creating kernels specific to the code and model of the user.</p> \n<p>It can take even the most skilled GPU programmers months to create custom kernels for each new model and optimize them. We built SageMaker Training Compiler to solve this problem.</p> \n<p>Today’s launch lets <span>SageMaker Training Compiler</span> automatically compile your Python training code and generate GPU kernels specifically for your model. Consequently, the training code will use less memory and compute, and therefore train faster. For example, when fine-tuning <a href=\"https://huggingface.co/\">Hugging Face</a>’s GPT-2 model, <span>SageMaker Training Compiler</span> reduced training time from nearly 3 hours to 90 minutes.</p> \n<p><span><strong>Automatically Optimizing Deep Learning Models<br /> </strong></span>So, how have we achieved this acceleration? <span>SageMaker Training Compiler</span> accelerates training jobs by converting DL models from their high-level language representation to hardware-optimized instructions that train faster than jobs with off-the-shelf frameworks. Under the hood, <span>SageMaker Training Compiler</span> makes incremental optimizations beyond what the native PyTorch and TensorFlow frameworks offer to maximize compute and memory utilization on SageMaker GPU instances.</p> \n<p>More specifically, <span>SageMaker Training Compiler</span> uses graph-level optimization (operator fusion, memory planning, and algebraic simplification), data flow-level optimizations (layout transformation, common sub-expression elimination), and back-end optimizations (memory latency hiding, loop oriented optimizations) to produce an optimized model that efficiently uses hardware resources. As a result, training is accelerated by up to 50%, and the returned model is the same as if <span>SageMaker Training Compiler</span> had not been used.</p> \n<p>But how do you use <span>SageMaker Training Compiler</span> with your models? It can be as simple as adding two lines of code!</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/11/23/CodeChange.png\"><img src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/11/23/CodeChange.png\" /></a></p> \n<p>The shortened training times mean that customers gain more time for innovating and deploying their newly-trained models at a reduced cost and a greater ability to experiment with larger models and more data.</p> \n<p><span><strong>Getting the most from <span>SageMaker Training Compiler</span><br /> </strong></span>Although many DL models can benefit from <span>SageMaker Training Compiler</span>, larger models with longer training will realize the greatest time and cost savings. For example, training time and costs fell by 30% on a long-running RoBERTa-base fine-tuning exercise.</p> \n<p>Jorge Lopez Grisman, a Senior Data Scientist at Quantum Health – an organization on a mission to “make healthcare navigation smarter, simpler, and more cost-effective for everyone” – said:</p> \n<p><em>“Iterating with NLP models can be a challenge because of their size: long training times bog down workflows and high costs can discourage our team from trying larger models that might offer better performance. <span>Amazon SageMaker Training Compiler</span> is exciting because it has the potential to alleviate these frictions. Achieving a speedup with SageMaker Training Compiler is a real win for our team that will make us more agile and innovative moving forward.”</em></p> \n<p><strong><span>Further Resources</span><br /> </strong>To learn more about how <span>Amazon SageMaker Training Compiler</span> can benefit you, you can visit <a href=\"https://aws.amazon.com/sagemaker/train\">our page here</a>. And to get started see our <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler.html\">technical documentation here</a>.</p>","author":"Sean M. Tracey","siteTitle":"AWS News Blog","siteHash":"6093e072e4117ec22616e844cb857d03ca62c57a411a8affc77cb5e8b6b15bf6","entryHash":"adadc5edcbba3142bbb39d8601178482d8114ccfc6c5dea5044c8ea2a504c6c7","category":"Tech"}