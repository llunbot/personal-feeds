{"title":"Googleâ€™s PaLM-E is a generalist robot brain that takes commands","link":"https://arstechnica.com/?p=1922315","date":1678230378000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2023/03/palm-e-robot-800x450.png\" alt=\"A robotic arm controlled by PaLM-E reaches for a bag of chips in a demonstration video.\" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2023/03/palm-e-robot.png\">Enlarge</a> <span>/</span> A robotic arm controlled by PaLM-E reaches for a bag of chips in a demonstration video. (credit: Google Research)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>On Monday, a group of AI researchers from Google and the Technical University of Berlin unveiled <a href=\"https://palm-e.github.io/\">PaLM-E</a>, a multimodal embodied visual-language model (VLM) with 562 billion <a href=\"https://ourworldindata.org/grapher/artificial-intelligence-parameter-count\">parameters</a> that integrates vision and language for robotic control. They claim it is the largest VLM ever developed and that it can perform a variety of tasks without the need for retraining.</p>\n\n<p>According to Google, when given a high-level command, such as \"bring me the rice chips from the drawer,\" PaLM-E can generate a plan of action for a mobile robot platform with an arm (developed by Google Robotics) and execute the actions by itself.</p>\n<p>PaLM-E does this by analyzing data from the robot's camera without needing a pre-processed scene representation. This eliminates the need for a human to pre-process or annotate the data and allows for more autonomous robotic control.</p></div><p><a href=\"https://arstechnica.com/?p=1922315#p3\">Read 11 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=1922315&amp;comments=1\">Comments</a></p>","author":"Benj Edwards","siteTitle":"Ars Technica","siteHash":"23e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"c92bec538fc15235fc87108b884a4471ea12be77227926eea04d91","category":"Tech"}