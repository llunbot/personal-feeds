{"title":"Amazon Titan Image Generator, Multimodal Embeddings, and Text models are now available in Amazon Bedrock","link":"https://aws.amazon.com/blogs/aws/amazon-titan-image-generator-multimodal-embeddings-and-text-models-are-now-available-in-amazon-bedrock/","date":1701276657000,"content":"<p>Today, we’re introducing two new Amazon Titan multimodal foundation models (FMs): Amazon Titan Image Generator (preview) and Amazon Titan Multimodal Embeddings. I’m also happy to share that Amazon Titan Text Lite and Amazon Titan Text Express are now generally available in <a href=\"https://aws.amazon.com/bedrock/\">Amazon Bedrock</a>. You can now choose from three available Amazon Titan Text FMs, including Amazon Titan Text Embeddings.</p> \n<p><a href=\"https://aws.amazon.com/bedrock/titan/\">Amazon Titan</a> models incorporate 25 years of <a href=\"https://aws.amazon.com/machine-learning/\">artificial intelligence (AI) and machine learning (ML)</a> innovation at Amazon and offer a range of high-performing image, multimodal, and text model options through a fully managed API. AWS pre-trained these models on large datasets, making them powerful, general-purpose models built to support a variety of use cases while also supporting the responsible use of AI.</p> \n<p>You can use the base models as is, or you can privately customize them with your own data. To enable access to Amazon Titan FMs, navigate to the <a href=\"https://console.aws.amazon.com/bedrock/home\">Amazon Bedrock console</a> and select <strong>Model access</strong> on the bottom left menu. On the model access overview page, choose <strong>Manage model access</strong> and enable access to the Amazon Titan FMs.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/11/20/2023-amazon-titan-models.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/11/20/2023-amazon-titan-models.png\" alt=\"Amazon Titan Models\" width=\"1984\" height=\"1002\" /></a></p> \n<p>Let me give you a quick tour of the new models.</p> \n<p><strong><u>Amazon Titan Image Generator (preview)<br /> </u></strong>As a content creator, you can now use Amazon Titan Image Generator to quickly create and refine images using English natural language prompts. This helps companies in advertising, e-commerce, and media and entertainment to create studio-quality, realistic images in large volumes and at low cost. The model makes it easy to iterate on image concepts by generating multiple image options based on the text descriptions. The model can understand complex prompts with multiple objects and generates relevant images. It is trained on high-quality, diverse data to create more accurate outputs, such as realistic images with inclusive attributes and limited distortions.</p> \n<p>Titan Image Generator’s image editing features include the ability to automatically edit an image with a text prompt using a built-in segmentation model. The model supports inpainting with an image mask and outpainting to extend or change the background of an image. You can also configure image dimensions and specify the number of image variations you want the model to generate.</p> \n<p>In addition, you can customize the model with proprietary data to generate images consistent with your brand guidelines or to generate images in a specific style, for example, by <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/fine-tuning.html\">fine-tuning</a> the model with images from a previous marketing campaign. Titan Image Generator also mitigates harmful content generation to support the <a href=\"https://aws.amazon.com/machine-learning/responsible-ai/\">responsible use of AI</a>. All images generated by Amazon Titan contain an invisible watermark, by default, designed to help reduce the spread of misinformation by providing a discreet mechanism to identify AI-generated images.</p> \n<p><strong>Amazon Titan Image Generator in action<br /> </strong>You can start using the model in the <a href=\"https://console.aws.amazon.com/bedrock/home\">Amazon Bedrock console</a> by submitting either an English natural language prompt to generate images or by uploading an image for editing. In the following example, I show you how to generate an image with Amazon Titan Image Generator using the <a href=\"https://aws.amazon.com/sdk-for-python/\">AWS SDK for Python (Boto3)</a>.</p> \n<p>First, let’s have a look at the configuration options for image generation that you can specify in the body of the inference request. For task type, I choose <code>TEXT_IMAGE</code> to create an image from a natural language prompt.</p> \n<pre><code>import boto3\nimport json\n\nbedrock = boto3.client(service_name=\"bedrock\")\nbedrock_runtime = boto3.client(service_name=\"bedrock-runtime\")\n\n# ImageGenerationConfig Options:\n#   numberOfImages: Number of images to be generated\n#   quality: Quality of generated images, can be standard or premium\n#   height: Height of output image(s)\n#   width: Width of output image(s)\n#   cfgScale: Scale for classifier-free guidance\n#   seed: The seed to use for reproducibility  \n\nbody = json.dumps(\n    {\n        \"taskType\": \"TEXT_IMAGE\",\n        \"textToImageParams\": {\n            \"text\": \"green iguana\",   # Required\n#           \"negativeText\": \"&lt;text&gt;\"  # Optional\n        },\n        \"imageGenerationConfig\": {\n            \"numberOfImages\": 1,   # Range: 1 to 5 \n            \"quality\": \"premium\",  # Options: standard or premium\n            \"height\": 768,         # Supported height list in the docs \n            \"width\": 1280,         # Supported width list in the docs\n            \"cfgScale\": 7.5,       # Range: 1.0 (exclusive) to 10.0\n            \"seed\": 42             # Range: 0 to 214783647\n        }\n    }\n)</code></pre> \n<p>Next, specify the model ID for Amazon Titan Image Generator and use the <code>InvokeModel</code> API to send the inference request.</p> \n<pre><code>response = bedrock_runtime.invoke_model(\n    body=body, \n    modelId=\"amazon.titan-image-generator-v1\" \n    accept=\"application/json\", \n    contentType=\"application/json\"\n)</code></pre> \n<p>Then, parse the response and decode the base64-encoded image.</p> \n<pre><code>import base64\nfrom PIL import Image\nfrom io import BytesIO\n\nresponse_body = json.loads(response.get(\"body\").read())\nimages = [Image.open(BytesIO(base64.b64decode(base64_image))) for base64_image in response_body.get(\"images\")]\n\nfor img in images:\n    display(img)</code></pre> \n<p>Et voilà, here’s the green iguana (one of my favorite animals, actually):</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/11/15/2023-amazon-titan-image-iguana-1.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/11/15/2023-amazon-titan-image-iguana-1.png\" alt=\"Green iguana generated by Amazon Titan Image Generator\" width=\"1280\" height=\"768\" /></a></p> \n<p>To learn more about all the Amazon Titan Image Generator features, visit the <a href=\"https://aws.amazon.com/bedrock/titan/\">Amazon Titan</a> product page. (You’ll see more of the iguana over there.)</p> \n<p>Next, let’s use this image with the new Amazon Titan Multimodal Embeddings model.</p> \n<p><strong><u>Amazon Titan Multimodal Embeddings<br /> </u></strong>Amazon Titan Multimodal Embeddings helps you build more accurate and contextually relevant multimodal search and recommendation experiences for end users. Multimodal refers to a system’s ability to process and generate information using distinct types of data (modalities). With Titan Multimodal Embeddings, you can submit text, image, or a combination of the two as input.</p> \n<p>The model converts images and short English text up to 128 tokens into embeddings, which capture semantic meaning and relationships between your data. You can also fine-tune the model on image-caption pairs. For example, you can combine text and images to describe company-specific manufacturing parts to understand and identify parts more effectively.</p> \n<p>By default, Titan Multimodal Embeddings generates vectors of 1,024 dimensions, which you can use to build search experiences that offer a high degree of accuracy and speed. You can also configure smaller vector dimensions to optimize for speed and price performance. The model provides an asynchronous batch API, and the <a href=\"https://aws.amazon.com/opensearch-service/\">Amazon OpenSearch Service</a> will soon offer a connector that adds Titan Multimodal Embeddings support for <a href=\"https://aws.amazon.com/about-aws/whats-new/2023/11/amazon-opensearch-neural-search/\">neural search</a>.</p> \n<p><strong>Amazon Titan Multimodal Embeddings in action<br /> </strong>For this demo, I create a combined image and text embedding. First, I base64-encode my image, and then I specify either <code>inputText</code>, <code>inputImage</code>, or both in the body of the inference request.</p> \n<pre><code># Maximum image size supported is 2048 x 2048 pixels\nwith open(\"iguana.png\", \"rb\") as image_file:\n    input_image = base64.b64encode(image_file.read()).decode('utf8')\n\n# You can specify either text or image or both\nbody = json.dumps(\n    {\n        \"inputText\": \"Green iguana on tree branch\",\n        \"inputImage\": input_image\n    }\n)</code></pre> \n<p>Next, specify the model ID for Amazon Titan Multimodal Embeddings and use the InvokeModel API to send the inference request.</p> \n<pre><code>response = bedrock_runtime.invoke_model(\n\tbody=body, \n\tmodelId=\"amazon.titan-embed-image-v1\", \n\taccept=\"application/json\", \n\tcontentType=\"application/json\"\n)</code></pre> \n<p>Let’s see the response.</p> \n<pre><code>response_body = json.loads(response.get(\"body\").read())\nprint(response_body.get(\"embedding\"))\n\t\n[-0.015633494, -0.011953583, -0.022617092, -0.012395329, 0.03954641, 0.010079376, 0.08505301, -0.022064181, -0.0037248489, ...]</code></pre> \n<p>I redacted the output for brevity. The distance between multimodal embedding vectors, measured with metrics like cosine similarity or euclidean distance, shows how similar or different the represented information is across modalities. Smaller distances mean more similarity, while larger distances mean more dissimilarity.</p> \n<p>As a next step, you could build an image database by storing and indexing the multimodal embeddings in a vector store or vector database. To implement text-to-image search, query the database with <code>inputText</code>. For image-to-image search, query the database with <code>inputImage</code>. For image+text-to-image search, query the database with both <code>inputImage</code> and <code>inputText</code>.</p> \n<p><strong><u>Amazon Titan Text<br /> </u></strong>Amazon Titan Text Lite and Amazon Titan Text Express are large language models (LLMs) that support a wide range of text-related tasks, including summarization, translation, and conversational chatbot systems. They can also generate code and are optimized to support popular programming languages and text formats like JSON and CSV.</p> \n<p><strong>Titan Text Express</strong> – Titan Text Express has a maximum context length of 8,192 tokens and is ideal for a wide range of tasks, such as open-ended text generation and conversational chat, and support within Retrieval Augmented Generation (RAG) workﬂows.</p> \n<p><strong>Titan Text Lite</strong> – Titan Text Lite has a maximum context length of 4,096 tokens and is a price-performant version that is ideal for English-language tasks. The model is highly customizable and can be fine-tuned for tasks such as article summarization and copywriting.</p> \n<p><strong>Amazon Titan Text in action<br /> </strong>For this demo, I ask Titan Text to write an email to my team members suggesting they organize a live stream: “Compose a short email from Antje, Principal Developer Advocate, encouraging colleagues in the developer relations team to organize a live stream to demo our new Amazon Titan V1 models.”</p> \n<pre><code>body = json.dumps({\n    \"inputText\": prompt, \n    \"textGenerationConfig\":{  \n        \"maxTokenCount\":512,\n        \"stopSequences\":[],\n        \"temperature\":0,\n        \"topP\":0.9\n    }\n})</code></pre> \n<p>Titan Text FMs support <code>temperature</code> and <code>topP</code> inference parameters to control the randomness and diversity of the response, as well as <code>maxTokenCount</code> and <code>stopSequences</code> to control the length of the response.</p> \n<p>Next, choose the model ID for one of the Titan Text models and use the <code>InvokeModel</code> API to send the inference request.</p> \n<pre><code>response = bedrock_runtime.invoke_model(\n    body=body,\n\t# Choose modelID\n\t# Titan Text Express: \"amazon.titan-text-express-v1\"\n\t# Titan Text Lite: \"amazon.titan-text-lite-v1\"\n\tmodelID=\"amazon.titan-text-express-v1\"\n    accept=\"application/json\", \n    contentType=\"application/json\"\n)</code></pre> \n<p>Let’s have a look at the response.</p> \n<pre><code>response_body = json.loads(response.get('body').read())\noutputText = response_body.get('results')[0].get('outputText')\n\ntext = outputText[outputText.index('\\n')+1:]\nemail = text.strip()\nprint(email)</code></pre> \n<blockquote>\n <p>Subject: Demo our new Amazon Titan V1 models live!</p> \n <p>Dear colleagues,</p> \n <p>I hope this email finds you well. I am excited to announce that we have recently launched our new Amazon Titan V1 models, and I believe it would be a great opportunity for us to showcase their capabilities to the wider developer community.</p> \n <p>I suggest that we organize a live stream to demo these models and discuss their features, benefits, and how they can help developers build innovative applications. This live stream could be hosted on our YouTube channel, Twitch, or any other platform that is suitable for our audience.</p> \n <p>I believe that showcasing our new models will not only increase our visibility but also help us build stronger relationships with developers. It will also provide an opportunity for us to receive feedback and improve our products based on the developer’s needs.</p> \n <p>If you are interested in organizing this live stream, please let me know. I am happy to provide any support or guidance you may need. Together, let’s make this live stream a success and showcase the power of Amazon Titan V1 models to the world!</p> \n <p>Best regards,<br /> Antje<br /> Principal Developer Advocate</p>\n</blockquote> \n<p>Nice. I could send this email right away!</p> \n<p><b><u>Availability and pricing<br /> </u></b>Amazon Titan Text FMs are available today in AWS Regions US East (N. Virginia), US West (Oregon), Asia Pacific (Singapore, Tokyo), and Europe (Frankfurt). Amazon Titan Multimodal Embeddings is available today in the AWS Regions US East (N. Virginia) and US West (Oregon). Amazon Titan Image Generator is available in public preview in the AWS Regions US East (N. Virginia) and US West (Oregon). For pricing details, see the Amazon Bedrock Pricing page.</p> \n<p><span><strong>Learn more</strong></span></p> \n<ul> \n <li><a href=\"https://aws.amazon.com/bedrock/\">Amazon Bedrock</a> product page</li> \n <li><a href=\"https://aws.amazon.com/bedrock/titan/\">Amazon Titan</a> product page</li> \n <li><a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html\">Amazon Bedrock User Guide.</a></li> \n</ul> \n<p>Go to the <a href=\"https://console.aws.amazon.com/bedrock/home\">AWS Management Console</a> to start building generative AI applications with Amazon Titan FMs on Amazon Bedrock today!</p> \n<p>— <a href=\"https://www.linkedin.com/in/antje-barth/\" target=\"_blank\">Antje</a></p>","author":"Antje Barth","siteTitle":"AWS News Blog","siteHash":"6093e072e4117ec22616e844cb857d03ca62c57a411a8affc77cb5e8b6b15bf6","entryHash":"ee02a3b743823dc99e643023118396bc249c88f5f3ea5031cbd3dc3ce7f9ad53","category":"Tech"}