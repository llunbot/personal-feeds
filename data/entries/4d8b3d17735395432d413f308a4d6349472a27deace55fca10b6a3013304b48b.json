{"title":"Consistency LLM: converting LLMs to parallel decoders accelerates inference 3.5x","link":"https://hao-ai-lab.github.io/blogs/cllm/","date":1715198107000,"content":"<a href=\"https://news.ycombinator.com/item?id=40302201\">Comments</a>","author":"","siteTitle":"Hacker News","siteHash":"37bb545430005dba450c1e40307450d8e4e791b434e83f3d38915ebad510fd50","entryHash":"4d8b3d17735395432d413f308a4d6349472a27deace55fca10b6a3013304b48b","category":"Tech"}