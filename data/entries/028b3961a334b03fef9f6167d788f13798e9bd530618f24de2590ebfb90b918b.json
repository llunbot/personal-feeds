{"title":"OpenAI training its next major AI model, forms new safety committee","link":"https://arstechnica.com/?p=2027105","date":1716912358000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/05/boulder_header-800x450.jpg\" alt=\"A man rolling a boulder up a hill.\" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2024/05/boulder_header.jpg\">Enlarge</a> (credit: <a href=\"https://www.gettyimages.com/detail/photo/concept-of-hard-work-for-businessman-pushing-rock-royalty-free-image/1247728090\">Getty Images</a>)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>On Monday, OpenAI <a href=\"https://openai.com/index/openai-board-forms-safety-and-security-committee/\">announced</a> the formation of a new \"Safety and Security Committee\" to oversee risk management for its projects and operations. The announcement comes as the company says it has \"recently begun\" training its next frontier model, which it expects to bring the company closer to its goal of achieving artificial general intelligence (AGI), though some critics say AGI is farther off than we might think. It also comes as a reaction to <a href=\"https://arstechnica.com/information-technology/2024/05/openai-on-the-defensive-after-multiple-pr-setbacks-in-one-week/\">two weeks</a> of public setbacks for the company.</p>\n\n<p>Whether the aforementioned new frontier model is intended to be GPT-5 or a step beyond that is currently unknown. In the AI industry, \"frontier model\" is a term for a new AI system designed to push the boundaries of current capabilities. And \"AGI\" refers to a hypothetical AI system with human-level abilities to perform novel, general tasks beyond its training data (unlike narrow AI, which is trained for specific tasks).</p>\n<p>Meanwhile, the new Safety and Security Committee, led by OpenAI directors Bret Taylor (chair), Adam D'Angelo, Nicole Seligman, and Sam Altman (CEO), will be responsible for making recommendations about AI safety to the full company board of directors. In this case, \"safety\" partially means the usual \"we won't let the AI go rogue and <a href=\"https://arstechnica.com/information-technology/2023/03/openai-checked-to-see-whether-gpt-4-could-take-over-the-world/\">take over the world</a>,\" but it also includes a broader set of \"processes and safeguards\" that the company spelled out in a <a href=\"https://openai.com/index/openai-safety-update/\">May 21 safety update</a> related to alignment research, protecting children, upholding election integrity, assessing societal impacts, and implementing security measures.</p></div><p><a href=\"https://arstechnica.com/?p=2027105#p3\">Read 5 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=2027105&amp;comments=1\">Comments</a></p>","author":"Benj Edwards","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"028b3961a334b03fef9f6167d788f13798e9bd530618f24de2590ebfb90b918b","category":"Tech"}