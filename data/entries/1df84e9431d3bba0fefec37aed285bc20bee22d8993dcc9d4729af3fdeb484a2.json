{"title":"Apple releases eight small AI language models aimed at on-device use","link":"https://arstechnica.com/?p=2020032","date":1714078543000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/04/apple_toss_hero_1-800x450.jpg\" alt=\"An illustration of a robot hand tossing an apple to a human hand.\" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2024/04/apple_toss_hero_1.jpg\">Enlarge</a> (credit: Getty Images)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>In the world of AI, what might be called \"small language models\" have been growing in popularity recently because they can be run on a local device instead of requiring data center-grade computers in the cloud. On Wednesday, Apple<a href=\"https://huggingface.co/apple/OpenELM\"> introduced</a> a set of tiny source-available AI language models called OpenELM that are small enough to run directly on a smartphone. They're mostly proof-of-concept research models for now, but they could form the basis of future on-device AI offerings from Apple.</p>\n\n<p>Apple's new AI models, collectively named OpenELM for \"Open-source Efficient Language Models,\" are currently available on the <a href=\"https://huggingface.co/apple/OpenELM\">Hugging Face</a> under an <a href=\"https://huggingface.co/apple/OpenELM/blob/main/LICENSE\">Apple Sample Code License</a>. Since there are some restrictions in the license, it may not fit the <a href=\"https://opensource.org/osd\">commonly accepted definition</a> of \"open source,\" but the source code for OpenELM is available.</p>\n<p>On Tuesday, we covered <a href=\"https://arstechnica.com/information-technology/2024/04/microsofts-phi-3-shows-the-surprising-power-of-small-locally-run-ai-language-models/\">Microsoft's Phi-3 models</a>, which aim to achieve something similar: a useful level of language understanding and processing performance in small AI models that can run locally. Phi-3-mini features 3.8 billion parameters, but some of Apple's OpenELM models are much smaller, ranging from 270 million to 3 billion parameters in eight distinct models.</p></div><p><a href=\"https://arstechnica.com/?p=2020032#p3\">Read 7 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=2020032&amp;comments=1\">Comments</a></p>","author":"Benj Edwards","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"1df84e9431d3bba0fefec37aed285bc20bee22d8993dcc9d4729af3fdeb484a2","category":"Tech"}