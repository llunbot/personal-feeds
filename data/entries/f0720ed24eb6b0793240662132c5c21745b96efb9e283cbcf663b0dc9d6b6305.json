{"title":"Epic’s new motion-capture animation tech has to be seen to be believed","link":"https://arstechnica.com/?p=1926215","date":1679574794000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2023/03/Screen-Shot-2023-03-22-at-5.35.08-PM-800x421.png\" alt=\"Would you believe that creating this performance took only minutes of video processing and no human tweaking?\" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2023/03/Screen-Shot-2023-03-22-at-5.35.08-PM.png\">Enlarge</a> <span>/</span> Would you believe that creating this performance took only minutes of video processing and no human tweaking? (credit: <a href=\"https://www.youtube.com/watch?v=K1qG8pREfkA\">Ninja Theory / Epic</a>)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>SAN FRANCISCO—Every year at the Game Developers Conference, a handful of competing companies show off their latest motion-capture technology, which transforms human performances into 3D animations that can be used on in-game models. Usually, these technical demonstrations involve a lot of specialized hardware for the performance capture and a good deal of computer processing and manual artist tweaking to get the resulting data into a game-ready state.</p>\n<p>Epic's upcoming MetaHuman facial animation tool looks set to revolutionize that kind of labor- and time-intensive workflow. In <a href=\"https://www.youtube.com/watch?v=pnaKyc3mQVk\">an impressive demonstration</a> at <a href=\"https://www.youtube.com/watch?v=teTroOAGZjM\">Wednesday's State of Unreal stage presentation</a>, Epic showed off the new machine-learning-powered system, which needed just a few minutes to generate impressively real, uncanny-valley-leaping facial animation from a simple head-on video taken on an iPhone.</p>\n<p>The potential to get quick, high-end results from that kind of basic input \"has literally changed how [testers] work or the kind of work they can take on,\" Epic VP of Digital Humans Technology Vladimir Mastilovic said in a panel discussion Wednesday afternoon.</p></div><p><a href=\"https://arstechnica.com/?p=1926215#p3\">Read 13 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=1926215&amp;comments=1\">Comments</a></p>","author":"Kyle Orland","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"f0720ed24eb6b0793240662132c5c21745b96efb9e283cbcf663b0dc9d6b6305","category":"Tech"}