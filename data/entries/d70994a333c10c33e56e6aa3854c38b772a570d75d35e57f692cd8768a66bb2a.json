{"title":"Researchers describe how to tell if ChatGPT is confabulating","link":"https://arstechnica.com/?p=2032628","date":1718911944000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/06/ai-uncertainty-800x450.jpg\" alt=\"Researchers describe how to tell if ChatGPT is confabulating\" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2024/06/ai-uncertainty.jpg\">Enlarge</a> (credit: Aurich Lawson | Getty Images)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>It's one of the world's worst-kept secrets that large language models give blatantly false answers to queries and do so with a confidence that's indistinguishable from when they get things right. There are a number of reasons for this. The AI could have been trained on misinformation; the answer could require some extrapolation from facts that the LLM isn't capable of; or some aspect of the LLM's training might have incentivized a falsehood.</p>\n<p>But perhaps the simplest explanation is that an LLM doesn't recognize what constitutes a correct answer but is compelled to provide one. So it simply makes something up, a habit that has <a href=\"https://arstechnica.com/information-technology/2023/04/why-ai-chatbots-are-the-ultimate-bs-machines-and-how-people-hope-to-fix-them/\">been termed confabulation</a>.</p>\n<p>Figuring out when an LLM is making something up would obviously have tremendous value, given how quickly people have started relying on them for everything from college essays to job applications. Now, researchers from the University of Oxford say they've found a relatively simple way to determine when LLMs appear to be confabulating that works with all popular models and across a broad range of subjects. And, in doing so, they develop evidence that most of the alternative facts LLMs provide are a product of confabulation.</p></div><p><a href=\"https://arstechnica.com/?p=2032628#p3\">Read 14 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=2032628&amp;comments=1\">Comments</a></p>","author":"John Timmer","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"d70994a333c10c33e56e6aa3854c38b772a570d75d35e57f692cd8768a66bb2a","category":"Tech"}