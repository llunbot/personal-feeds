{"title":"Running Vault on Nomad, Part 3","link":"https://www.hashicorp.com/blog/running-vault-on-nomad-part-3","date":1715670000000,"content":"<p>This post is the third and final part of a series on how to deploy the underlying HashiCorp Nomad infrastructure and configuration to run HashiCorp Vault as a Nomad job. <a href=\"https://www.hashicorp.com/blog/running-vault-on-hashicorp-nomad-part-1\">Part 1</a> focused on deploying the infrastructure to run Vault on Nomad, while <a href=\"https://www.hashicorp.com/blog/running-vault-on-nomad-part-2\">part 2</a> took a deep dive into running Vault as a Nomad job. This installment looks at automating operational tasks for Vault using Nomad jobs. </p>\n\n<p>Specifically, it covers how to automate:\n1. Unsealing Vault\n2. Taking snapshots of Vault</p>\n\n<h2>What is unsealing Vault?</h2>\n\n<p>As a secrets management platform, Vault stores sensitive and often mission-critical data in its storage backend. This data is encrypted using an encryption key, which is required to decrypt the data in the storage backend. Vault stores this key with the encrypted data and further encrypts it using another key known as the root key.</p>\n\n<p>Unsealing is the process of decrypting the encryption key using the root key and then decrypting the data using the encryption key. Until this process has been completed, you can perform only two operations on the Vault server:</p>\n\n<ol>\n<li>Checking the seal status of Vault</li>\n<li>Unsealing Vault</li>\n</ol>\n\n<p>The root key is normally split into a configurable number of shards, known as unseal keys, using <a href=\"https://developer.hashicorp.com/vault/docs/concepts/seal#shamir-seals\">Shamir's Secret Sharing algorithm</a>, and these are distributed to engineers responsible for unsealing Vault. The unseal process consists of a pre-specified threshold of unseal keys being entered by the key holders.</p>\n\n<h2>Auto unsealing Vault</h2>\n\n<p>When a Vault server is started or restarted, it comes up in a sealed state, which means that only the two operations mentioned above can be performed on it. There are many reasons why Vault might need to be restarted, from OS patching to resource consumption issues. Whatever the reason, unsealing presents a potentially huge management overhead burden for the Vault servers.</p>\n\n<p>To address this, Vault’s <a href=\"https://developer.hashicorp.com/vault/docs/concepts/seal#auto-unseal\">auto-unseal</a> feature delegates the responsibility of unsealing Vault to a service like a cloud key management service (KMS) or a device like a hardware security module (HSM). As the name suggests, using auto-unseal means that the Vault servers will be automatically unsealed when they are started or restarted.</p>\n\n<h2>Vault Unsealer</h2>\n\n<p>There are many reasons why some organizations cannot use auto-unseal. Their security policies may not allow cloud services, or they may not want to pay the high procurement and operational costs of HSMs. <a href=\"https://github.com/devops-rob/vault-unsealer\">Vault Unsealer</a>, is a proof-of-concept tool designed to automate the process of unsealing Vault using the unseal keys.</p>\n\n<h3>How Vault Unsealer works</h3>\n\n<p>Vault Unsealer checks the seal status of each Vault server in the cluster and unseals any servers reporting their status as sealed. Under the hood, Vault Unsealer uses the Vault API to perform these tasks.</p>\n\n<h3>Configuring Vault Unsealer</h3>\n\n<p>In order to use Vault Unsealer, you’ll configure a <code>JSON</code> file  to tell it which servers to manage the unseal state on, the unseal keys to use, how often it should check the seal status, and the log level to output to <code>stdout</code>. Here is an example configuration file:</p>\n<pre><code>{\n \"log_level\": \"debug\",\n \"probe_interval\": 10,\n \"nodes\": [\n   \"http://192.168.1.141:8200\",\n   \"http://192.168.1.142:8200\",\n   \"http://192.168.1.143:8200\"\n ],\n \"unseal_keys\": [\n   \"aa109356340az6f2916894c2e538f7450412056cea4c45b3dd4ae1f9c840befc1a\",\n   \"4948bcfe36834c8e6861f8144672cb804610967c7afb0588cfd03217b4354a8c35\",\n   \"7b5802f21b19s522444e2723a31cb07d5a3de60fbc37d21f918f998018b6e7ce8b\"\n ]\n}</code></pre><p><em><strong>NOTE:</strong> The unseal keys are sensitive pieces of data, so we recommend that the config file is rendered with the unseal keys’ values coming from an encrypted store that you trust.</em></p>\n\n<h2>Deploying Vault Unsealer as a Nomad job</h2>\n\n<p>For this post, the code is located within the <a href=\"https://github.com/devops-rob/vault-on-nomad-demo/tree/main/2-nomad-configuration\">2-nomad-configuration</a> directory.</p>\n\n<p>Writing a Nomad jobspec for Vault Unsealer is similar to the process in <a href=\"https://www.hashicorp.com/blog/running-vault-on-nomad-part-2\">part 2 of the blog series</a> with some subtle differences because the requirements for this job are slightly less than that of the Vault cluster. Here is the <code>vault-unsealer.nomad</code> file:</p>\n<pre><code>job \"vault-unsealer\" {\n namespace   = \"vault-cluster\"\n datacenters = [\"dc1\"]\n type        = \"service\"\n node_pool   = \"vault-servers\"\n\n group \"vault-unsealer\" {\n   count = 1\n\n   constraint {\n     attribute = \"${node.class}\"\n     value     = \"vault-servers\"\n   }\n\n   task \"vault-unsealer\" {\n     driver = \"docker\"\n\n     config {\n       image      = \"devopsrob/vault-unsealer:0.2\"\n\n       command = \"./vault-unsealer\"\n       volumes = [\n         \"local/config:/app/config\"\n       ]\n     }\n\n     template {\n       data = &lt;</code></pre><p>Key points to note about this jobspec include:</p>\n\n<ul>\n<li>Vault Unsealer is deployed as a Docker job</li>\n<li>It runs on the same node pool as the Vault servers</li>\n<li>Only one instance is running</li>\n<li>It renders the configuration file using Nomad's templating engine.\n\n<ul>\n<li>The unseal keys are stored in Nomad variables as seen in <a href=\"https://www.hashicorp.com/blog/running-vault-on-nomad-part-2\">part 2</a> of this blog series. The template renders the values in the configuration file.</li>\n<li>The list of Vault servers to manage their respective seal statuses are populated from Nomad's built-in service registry</li>\n</ul></li>\n</ul>\n\n<p>Vault Unsealer can be deployed using Terraform, similar to how the Vault cluster was deployed. Here is the code used to deploy the job to Nomad:</p>\n<pre><code>resource \"nomad_job\" \"vault-unsealer\" {\n jobspec = file(\"vault-unsealer.nomad\")\n depends_on = [\n   nomad_namespace.vault,\n   nomad_variable.unseal,\n   nomad_job.vault\n ]\n}</code></pre><p>This Terraform code specifies some explicit dependencies, all of which were explained in part 2 of this blog series. This job ensures the Vault servers are all unsealed and ready to accept requests.</p>\n\n<h2>Automated snapshots of Vault</h2>\n\n<p>The storage backend of the Vault cluster, (Raft integrated storage) replicates its data across the Vault servers to create a highly available cluster. This may improve availability and redundancy; however it does not provide disaster recovery if the storage for all three Vault servers is lost irreparably.</p>\n\n<p>This is where snapshots come into play. Snapshots take a point-in-time backup of Vault's data which, in the event of a total loss, will allow a new cluster to be provisioned and the data from the snapshot can be used to restore Vault. Vault provides an <a href=\"https://developer.hashicorp.com/vault/api-docs/system/storage/raft#take-a-snapshot-of-the-raft-cluster\">API endpoint</a> to take snapshots.</p>\n\n<p>Best practice dictates that snapshots should be stored away from the things they are there to protect. <a href=\"https://www.hashicorp.com/blog/running-vault-on-hashicorp-nomad-part-1\">Part 1</a> of this blog series showed how to provision a Vault backup server to accommodate this best practice.</p>\n\n<p>You create a Nomad job to take regular backups of the Vault cluster. The first step is to create a Vault policy that allows the Nomad job to perform this task. This code snippet writes a policy to Vault named <code>snapshot_policy</code>:</p>\n<pre><code>resource \"terracurl_request\" \"snapshot_policy\" {\n method         = \"POST\"\n name           = \"snapshot_policy\"\n response_codes = [201, 204]\n url            = \"http://${data.terraform_remote_state.tfc.outputs.nomad_clients_public_ips[0]}:8200/v1/sys/policy/snapshot_policy\"\n\n headers = {\n   X-Vault-Token = jsondecode(terracurl_request.init.response).root_token\n }\n\n request_body = &lt;</code></pre><p>The contents of the policy file are written as the value of a JSON key/value pair, so the quotation marks have been escaped. Here is the resulting policy in Vault:</p>\n<pre><code>path \"sys/storage/raft/snapshot\" {capabilities = [\"read\"]}</code></pre><p>Now that the policy has been written to Vault, the next step is to create a role under the JWT auth method that allows the Nomad job to take snapshots. A Vault role is a set of parameters to define the actions authorized by specific entities. The role will specify the claims required within a JWT and the Vault permissions to assign to the resulting Vault token as part of the authentication process. In this case, if the JWT has the following claims, it will issue a Vault token with <code>snapshot_policy</code> assigned to it:</p>\n\n<ul>\n<li> <code>nomad_job_id</code> must be <code>vault-backup</code>: This will prevent other jobs from obtaining a Vault token via this role.</li>\n<li> <code>nomad_namespace</code> must be <code>vault-cluster</code>: This will prevent a job running in the wrong namespace from obtaining a Vault token via this role.</li>\n<li> <code>nomad_task</code> must be <code>vault-backup</code>: This will prevent any other tasks within the job group from obtaining a Vault token via this role.</li>\n</ul>\n\n<p>The code snippet uses TerraCurl to create a snapshot role in Vault that will be used by the Nomad job:</p>\n<pre><code>resource \"terracurl_request\" \"snapshot_role\" {\n method = \"POST\"\n name   = \"snapshot_role\"\n\n response_codes = [\n   204\n ]\n\n url = \"http://${data.terraform_remote_state.tfc.outputs.nomad_clients_public_ips[0]}:8200/v1/auth/jwt/role/snapshot\"\n\n headers = {\n   X-Vault-Token = jsondecode(terracurl_request.init.response).root_token\n }\n\n request_body = &lt;</code></pre><p>The final piece of the puzzle is to write and deploy the <code>vault-backup</code> job. Here is the Nomad jobspec written for <code>vault-backup</code>:</p>\n<pre><code>job \"vault-backup\" {\n namespace   = \"vault-cluster\"\n datacenters = [\"dc1\"]\n type        = \"batch\"\n node_pool   = \"vault-backup\"\n\n periodic {\n\n   crons = [\n     \"@daily\"\n   ]\n\n   prohibit_overlap = true\n }\n\n group \"vault-backup\" {\n   count = 1\n\n   constraint {\n     attribute = \"${node.class}\"\n     value     = \"vault-backup\"\n   }\n\n   volume \"vault_data\" {\n     type      = \"host\"\n     source    = \"vault_vol\"\n     read_only = false\n   }\n\n   task \"vault-backup\" {\n     driver = \"docker\"\n\n     volume_mount {\n       volume      = \"vault_data\"\n       destination = \"/vault/file\"\n       read_only   = false\n     }\n\n     config {\n       image   = \"shipyardrun/tools\"\n       command = \"./scripts/backup.sh\"\n       volumes = [\n         \"local/scripts:/scripts\"\n       ]\n     }\n\n     template {\n       data = &lt; dev/null\n\n# Find the cluster leader\nleader_address=$(curl \\\n   ${vault_addr}/v1/sys/leader | \\\n   jq -r '.leader_address')\n\n# Take snapshot\ndate=$(date -I)\n\nvault operator raft snapshot save \\\n -address $leader_address \\\n \"/vault/file/${date}.snap\"\n\nEOH\n\n       destination = \"local/scripts/backup.sh\"\n       change_mode = \"noop\"\n       perms       = \"777\"\n     }\n\n     resources {\n       cpu    = 100\n       memory = 512\n\n     }\n\n     affinity {\n       attribute = \"${meta.node_id}\"\n       value     = \"${NOMAD_ALLOC_ID}\"\n       weight    = 100\n     }\n\n     identity {\n       env         = true\n     }\n\n     env {\n       JWT = \"${NOMAD_TOKEN}\"\n     }\n   }\n }\n}</code></pre><p>Note these key points:\n- This is a periodic job, a batch job that runs on a predefined schedule. This job is scheduled to run daily at midnight.\n- The job will be run within the <code>vault-backup</code> node pool to ensure the backup is stored away from the Vault cluster.\n- Host volumes are used here to allow the job to store the snapshot.\n- This job uses the Docker task driver.\n- The container image is <a href=\"https://hub.docker.com/r/shipyardrun/tools\"><code>shipyardrun/tools</code></a>, which is a community image containing the binaries for most HashiCorp tools as well as other useful packages, such as <a href=\"https://jqlang.github.io/jq/\"><code>jq</code></a>.\n- The workload identity JWT is exposed to the job via an environment variable.\n- The workload identity environment variable is exposed to the container.\n- The template within the task renders a shell script that:\n  - Reads the workload identity environment variable and uses this to login and obtain a Vault token.\n  - Checks the cluster leader address.\n  - Takes a snapshot of the cluster via the cluster leader.</p>\n\n<p>This jobspec is then deployed using Terraform with this code snippet:</p>\n<pre><code>resource \"nomad_job\" \"snapshot\" {\n jobspec          = file(\"nomad-jobs/snapshot.nomad\")\n purge_on_destroy = true\n\n depends_on = [\n   terracurl_request.snapshot_role\n ]\n}</code></pre><p>This shows how to automate the process of taking snapshots of Vault using workload identity to authenticate to Vault. Any Nomad job that needs a secret from Vault can use a similar process for Vault-aware workloads. For example, if a job needs to use the transit secrets engine, it will need to make a call to Vault within the application code. In these cases, authentication via workload identity is a good pattern to implement.</p>\n\n<h2>Summary</h2>\n\n<p><a href=\"https://www.hashicorp.com/blog/running-vault-on-hashicorp-nomad-part-1\">Part 1</a> of this series explored the infrastructure needed to deploy Nomad servers and clients, how to configure them for workload identity, how to spin up host volumes for stateful workloads, and how to configure the Docker plugin to permit the required Linux capabilities. It also covered how to enable and bootstrap the ACL system to secure the Nomad deployment. </p>\n\n<p><a href=\"https://www.hashicorp.com/blog/running-vault-on-nomad-part-2\">Part 2</a> took a deep dive into jobspecs and constructing a job to run Vault, as well as the Nomad templating engine and built-in service registry. It also looked at the initialization process and how Vault's seal mechanism works. </p>\n\n<p>This third and final installment of the blog series showed how to use Vault Unsealer to provide auto-unseal capabilities without external dependencies using Nomad variables. It also looked at the process of automating Vault backups using periodic jobs and how to authenticate to Vault using workload identity.</p>\n\n<p>In summary, running Vault on Nomad has a lot of operational benefits that can reduce management overhead. This is a good approach for smaller organizations with limited resources. The alternative approach is to use the <a href=\"https://developer.hashicorp.com/nomad/docs/integrations/vault\">Vault integration within Nomad</a>, which is helpful for authenticating to Vault and obtaining secrets for workloads without them being aware of Vault. This could be a better fit for organizations with dedicated teams managing Vault.</p>\n","author":"Rob Barnes","siteTitle":"HashiCorp Blog","siteHash":"219aa6310b3388f2335eba49871f4df9581f2c58eaeb5e498363b54e835b7001","entryHash":"4e9ce56e1b134cd60d6566db6db1edb8c9d97851d422886f17c970c83fa0e795","category":"Tech"}