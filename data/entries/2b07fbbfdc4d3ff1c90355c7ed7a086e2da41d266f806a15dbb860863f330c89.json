{"title":"New Seventh-Generation General Purpose Amazon EC2 Instances (M7i-Flex and M7i)","link":"https://aws.amazon.com/blogs/aws/new-seventh-generation-general-purpose-amazon-ec2-instances-m7i-flex-and-m7i/","date":1691010788000,"content":"<p>Today we are launching <a href=\"https://aws.amazon.com/ec2/\">Amazon Elastic Compute Cloud (Amazon EC2)</a> M7i-Flex and M7i instances powered by custom 4th generation Intel Xeon Scalable processors available only on AWS, that offer the best performance among comparable Intel processors in the cloud – up to 15% faster than Intel processors utilized by other cloud providers. M7i-Flex instances are available in the five most common sizes, and are designed to give you up to 19% better price/performance than M6i instances for many workloads. The M7i instances are available in nine sizes (with two size of bare metal instances in the works), and offer 15% better price/performance than the previous generation of Intel-powered instances.</p> \n<p><span><strong>M7i-Flex Instances</strong></span><br /> The M7i-Flex instances are a lower-cost variant of the M7i instances, with 5% better price/performance and 5% lower prices. They are great for applications that don’t fully utilize all compute resources. The M7i-Flex instances deliver a baseline of 40% CPU performance, and can scale up to full CPU performance 95% of the time. M7i-Flex instances are ideal for running general purpose workloads such as web and application servers, virtual desktops, batch processing, micro-services, databases and enterprise applications. If you are currently using earlier generations of general-purposes instances, you can adopt M7i-Flex instances without having to make changes to your application or your workload.</p> \n<p>Here are the specs for the M7i-Flex instances:</p> \n<table> \n <tbody> \n  <tr> \n   <td><strong>Instance Name</strong></td> \n   <td><strong>vCPUs<br /> </strong></td> \n   <td><strong>Memory<br /> </strong></td> \n   <td><strong>Network Bandwidth<br /> </strong></td> \n   <td><strong>EBS Bandwidth<br /> </strong></td> \n  </tr> \n  <tr> \n   <td><strong>m7i-flex.large</strong></td> \n   <td>2</td> \n   <td>8 GiB</td> \n   <td>up to 12.5 Gbps</td> \n   <td>up to 10 Gbps</td> \n  </tr> \n  <tr> \n   <td><strong>m7i-flex.xlarge</strong></td> \n   <td>4</td> \n   <td>16 GiB</td> \n   <td>up to 12.5 Gbps</td> \n   <td>up to 10 Gbps</td> \n  </tr> \n  <tr> \n   <td><strong>m7i-flex.2xlarge</strong></td> \n   <td>8</td> \n   <td>32 GiB</td> \n   <td>up to 12.5 Gbps</td> \n   <td>up to 10 Gbps</td> \n  </tr> \n  <tr> \n   <td><strong>m7i-flex.4xlarge</strong></td> \n   <td>16</td> \n   <td>64 GiB</td> \n   <td>up to 12.5 Gbps</td> \n   <td>up to 10 Gbps</td> \n  </tr> \n  <tr> \n   <td><strong>m7i-flex.8xlarge</strong></td> \n   <td>32</td> \n   <td>128 GiB</td> \n   <td>up to 12.5 Gbps</td> \n   <td>up to 10 Gbps</td> \n  </tr> \n </tbody> \n</table> \n<p><span><strong>M7i Instances</strong></span><br /> For workloads such as large application servers and databases, gaming servers, CPU based machine learning, and video streaming that need the largest instance sizes or high CPU continuously, you can get price/performance benefits by using M7i instances.</p> \n<p>Here are the specs for the M7i instances:</p> \n<table> \n <tbody> \n  <tr> \n   <td><strong>Instance Name</strong></td> \n   <td><strong>vCPUs<br /> </strong></td> \n   <td><strong>Memory<br /> </strong></td> \n   <td><strong>Network Bandwidth<br /> </strong></td> \n   <td><strong>EBS Bandwidth<br /> </strong></td> \n  </tr> \n  <tr> \n   <td><strong>m7i.large</strong></td> \n   <td>2</td> \n   <td>8 GiB</td> \n   <td>up to 12.5 Gbps</td> \n   <td>up to 10 Gbps</td> \n  </tr> \n  <tr> \n   <td><strong>m7i.xlarge</strong></td> \n   <td>4</td> \n   <td>16 GiB</td> \n   <td>up to 12.5 Gbps</td> \n   <td>up to 10 Gbps</td> \n  </tr> \n  <tr> \n   <td><strong>m7i.2xlarge</strong></td> \n   <td>8</td> \n   <td>32 GiB</td> \n   <td>up to 12.5 Gbps</td> \n   <td>up to 10 Gbps</td> \n  </tr> \n  <tr> \n   <td><strong>m7i.4xlarge</strong></td> \n   <td>16</td> \n   <td>64 GiB</td> \n   <td>up to 12.5 Gbps</td> \n   <td>up to 10 Gbps</td> \n  </tr> \n  <tr> \n   <td><strong>m7i.8xlarge</strong></td> \n   <td>32</td> \n   <td>128 GiB</td> \n   <td>12.5 Gbps</td> \n   <td>10 Gbps</td> \n  </tr> \n  <tr> \n   <td><strong>m7i.12xlarge</strong></td> \n   <td>48</td> \n   <td>192 GiB</td> \n   <td>18.75 Gbps</td> \n   <td>15 Gbps</td> \n  </tr> \n  <tr> \n   <td><strong>m7i.16xlarge</strong></td> \n   <td>64</td> \n   <td>256 GiB</td> \n   <td>25.0 Gbps</td> \n   <td>20 Gbps</td> \n  </tr> \n  <tr> \n   <td><strong>m7i.24xlarge</strong></td> \n   <td>96</td> \n   <td>384 GiB</td> \n   <td>37.5 Gbps</td> \n   <td>30 Gbps</td> \n  </tr> \n  <tr> \n   <td><strong>m7i.48xlarge</strong></td> \n   <td>192</td> \n   <td>768 GiB</td> \n   <td>50 Gbps</td> \n   <td>40 Gbps</td> \n  </tr> \n </tbody> \n</table> \n<p>You can attach up to 128 EBS volumes to each M7i instance; by way of comparison, the M6i instances allow you to attach up to 28 volumes.</p> \n<p>We are also getting ready to launch two sizes of bare metal M7i instances:</p> \n<table> \n <tbody> \n  <tr> \n   <td><strong>Instance Name</strong></td> \n   <td><strong>vCPUs<br /> </strong></td> \n   <td><strong>Memory<br /> </strong></td> \n   <td><strong>Network Bandwidth<br /> </strong></td> \n   <td><strong>EBS Bandwidth<br /> </strong></td> \n  </tr> \n  <tr> \n   <td><strong>m7i.metal-24xl</strong></td> \n   <td>96</td> \n   <td>384 GiB</td> \n   <td>37.5 Gbps</td> \n   <td>30 Gbps</td> \n  </tr> \n  <tr> \n   <td><strong>m7i.metal-48xl</strong></td> \n   <td>192</td> \n   <td>768 GiB</td> \n   <td>50.0 Gbps</td> \n   <td>40 Gbps</td> \n  </tr> \n </tbody> \n</table> \n<p><span><strong>Built-In Accelerators</strong></span><br /> The Sapphire Rapids processors include four built-in accelerators, each providing hardware acceleration for a specific workload:</p> \n<ul> \n <li><strong>Advanced Matrix Extensions (AMX) </strong>– This set of extensions to the x86 instruction set improve deep learning and inferencing, and support workloads such as natural language processing, recommendation systems, and image recognition. The extensions provide high-speed multiplication operations on 2-dimensional matrices of INT8 or BF16 values. To learn more, read Chapter 3 of the <a href=\"https://www.intel.com/content/dam/develop/external/us/en/documents/architecture-instruction-set-extensions-programming-reference.pdf\">Intel AMX Instruction Set Reference</a>.</li> \n <li><strong>Intel Data Streaming Accelerator (DSA)</strong> – This accelerator drives high performance for storage, networking, and data-intensive workloads by offloading common data movement tasks between CPU, memory, caches, network devices, and storage devices, improving streaming data movement and transformation operations. Read <a href=\"https://www.intel.com/content/www/us/en/content-details/759709/intel-data-streaming-accelerator-user-guide.html\">Introducing the Intel Data Streaming Accelerator (Intel DSA)</a> to learn more.</li> \n <li><strong>Intel In-Memory Analytics Accelerator (IAA)</strong> – This accelerator runs database and analytic workloads faster, with the potential for greater power efficiency. In-memory compression, decompression, and encryption at very high throughput, and a suite of analytics primitives support in-memory databases, open source database, and data stores like <a href=\"https://rocksdb.org/\">RocksDB</a> and <a href=\"https://clickhouse.com\">ClickHouse</a>. To learn more, read the <a href=\"https://cdrdv2-public.intel.com/721858/60941-in-memory-analytics-accelerator.pdf\">Intel In-Memory Analytics Accelerator (Intel IAA) Architecture Specification</a>.</li> \n <li><strong>Intel QuickAssist Technology (QAT)</strong> -This accelerator offloads encryption, decryption, and compression, freeing up processor cores and reducing power consumption. It also supports merged compression and encryption in a single data flow. To learn more start at the <a href=\"https://www.intel.com/content/www/us/en/developer/topic-technology/open/quick-assist-technology/overview.html\">Intel QuickAssist Technology (Intel QAT) Overview</a>.</li> \n</ul> \n<p>Some of these accelerators require the use of specific kernel versions, drivers, and/or compilers.</p> \n<p>The Advanced Matrix Extensions are available on all sizes of M7i and M7i-Flex instances. The Intel QAT, Intel IAA, and Intel DSA accelerators will be available on the <strong>m7i.metal-24xl</strong> and <strong>m7i.metal-48xl</strong> instances.</p> \n<p><strong><span>Details</span></strong><br /> Here are a couple of things to keep in mind about the M7i-Flex and M7i instances:</p> \n<p><strong>Regions</strong> – The new instances are available in the US East (Ohio, N. Virginia), US West (Oregon), and Europe (Ireland) AWS Regions, and we plan to expand to additional regions throughout the rest of 2023.</p> \n<p><strong>Purchasing Options</strong> – M7i-Flex amd M7i instances are available in On-Demand, Reserved Instance, Savings Plan, and Spot form. M7i instances are also available in Dedicated Host and Dedicated Instance form.</p> \n<p>— <a href=\"https://twitter.com/jeffbarr\">Jeff</a>;</p>","author":"Jeff Barr","siteTitle":"AWS News Blog","siteHash":"6093e072e4117ec22616e844cb857d03ca62c57a411a8affc77cb5e8b6b15bf6","entryHash":"2b07fbbfdc4d3ff1c90355c7ed7a086e2da41d266f806a15dbb860863f330c89","category":"Tech"}