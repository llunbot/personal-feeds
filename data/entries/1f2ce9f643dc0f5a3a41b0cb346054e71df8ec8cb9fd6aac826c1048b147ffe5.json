{"title":"Microsoftâ€™s Phi-3 shows the surprising power of small, locally run AI language models","link":"https://arstechnica.com/?p=2019278","date":1713905252000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/04/llm_compression-800x450.jpg\" alt=\"An illustration of lots of information being compressed into a smartphone with a funnel.\" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2024/04/llm_compression.jpg\">Enlarge</a> (credit: Getty Images)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>On Tuesday, Microsoft <a href=\"https://arxiv.org/abs/2404.14219\">announced</a> a new, freely available lightweight AI language model named Phi-3-mini, which is simpler and less expensive to operate than traditional large language models (LLMs) like OpenAI's <a href=\"https://arstechnica.com/information-technology/2023/11/openai-introduces-gpt-4-turbo-larger-memory-lower-cost-new-knowledge/\">GPT-4 Turbo</a>. Its small size is ideal for running locally, which could bring an AI model of similar capability to the free version of <a href=\"https://arstechnica.com/information-technology/2023/11/chatgpt-was-the-spark-that-lit-the-fire-under-generative-ai-one-year-ago-today/\">ChatGPT</a> to a smartphone without needing an Internet connection to run it.</p>\n\n<p>The AI field typically measures AI language model size by parameter count. Parameters are numerical values in a neural network that determine how the language model processes and generates text. They are learned during training on large datasets and essentially encode the model's knowledge into quantified form. More parameters generally allow the model to capture more nuanced and complex language-generation capabilities but also require more computational resources to train and run.</p>\n<p>Some of the largest language models today, like <a href=\"https://arstechnica.com/information-technology/2023/05/googles-top-ai-model-palm-2-hopes-to-upstage-gpt-4-in-generative-mastery/\">Google's PaLM 2</a>, have hundreds of billions of parameters. OpenAI's GPT-4 is <a href=\"https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/\">rumored to have</a> over a trillion parameters but spread over eight 220-billion parameter models in a mixture-of-experts configuration. Both models require heavy-duty data center GPUs (and supporting systems) to run properly.</p></div><p><a href=\"https://arstechnica.com/?p=2019278#p3\">Read 8 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=2019278&amp;comments=1\">Comments</a></p>","author":"Benj Edwards","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"1f2ce9f643dc0f5a3a41b0cb346054e71df8ec8cb9fd6aac826c1048b147ffe5","category":"Tech"}