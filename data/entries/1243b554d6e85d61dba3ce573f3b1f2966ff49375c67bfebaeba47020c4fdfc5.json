{"title":"Amazon EC2 Inf2 Instances for Low-Cost, High-Performance Generative AI Inference are Now Generally Available","link":"https://aws.amazon.com/blogs/aws/amazon-ec2-inf2-instances-for-low-cost-high-performance-generative-ai-inference-are-now-generally-available/","date":1681389507000,"content":"<p>Innovations in deep learning (DL), especially the rapid growth of large language models (LLMs), have taken the industry by storm. DL models have grown from millions to billions of parameters and are demonstrating exciting new capabilities. They are fueling new applications such as generative AI or advanced research in healthcare and life sciences. AWS has been innovating across chips, servers, data center connectivity, and software to accelerate such DL workloads at scale.</p> \n<p>At AWS re:Invent 2022, we announced the preview of Amazon EC2 <a href=\"https://aws.amazon.com/ec2/instance-types/inf2/\">Inf2 instances</a> powered by <a href=\"https://aws.amazon.com/machine-learning/inferentia/\">AWS Inferentia2</a>, the latest AWS-designed ML chip. Inf2 instances are designed to run high-performance DL inference applications at scale globally. They are the most cost-effective and energy-efficient option on Amazon EC2 for deploying the latest innovations in generative AI, such as <a href=\"https://huggingface.co/docs/transformers/model_doc/gptj\">GPT-J</a> or <a href=\"https://arxiv.org/abs/2205.01068\">Open Pre-trained Transformer</a> (OPT) language models.</p> \n<p>Today, I’m excited to announce that Amazon EC2 Inf2 instances are now generally available!</p> \n<p>Inf2 instances are the first inference-optimized instances in Amazon EC2 to support scale-out distributed inference with ultra-high-speed connectivity between accelerators. You can now efficiently deploy models with hundreds of billions of parameters across multiple accelerators on Inf2 instances. Compared to Amazon EC2 Inf1 instances, Inf2 instances deliver up to 4x higher throughput and up to 10x lower latency. Here’s an infographic that highlights the key performance improvements that we have made available with the new Inf2 instances:</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/04/02/inf1_vs_inf2_updated2.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/04/02/inf1_vs_inf2_updated2.png\" alt=\"Performance improvements with Amazon EC2 Inf2\" width=\"1634\" height=\"812\" /></a></p> \n<p><strong><u>New Inf2 Instance Highlights<br /> </u></strong>Inf2 instances are available today in four sizes and are powered by up to 12 AWS Inferentia2 chips with 192 vCPUs. They offer a combined compute power of 2.3 petaFLOPS at BF16 or FP16 data types and feature an ultra-high-speed NeuronLink interconnect between chips. NeuronLink scales large models across multiple Inferentia2 chips, avoids communication bottlenecks, and enables higher-performance inference.</p> \n<p>Inf2 instances offer up to 384 GB of shared accelerator memory, with 32 GB high-bandwidth memory (HBM) in every Inferentia2 chip and 9.8 TB/s of total memory bandwidth. This type of bandwidth is particularly important to support inference for large language models that are memory bound.</p> \n<p>Since the underlying AWS Inferentia2 chips are purpose-built for DL workloads, Inf2 instances offer up to 50 percent better performance per watt than other comparable Amazon EC2 instances. I’ll cover the AWS Inferentia2 silicon innovations in more detail later in this blog post.</p> \n<p>The following table lists the sizes and specs of Inf2 instances in detail.</p> \n<table> \n <tbody> \n  <tr> \n   <td><strong>Instance Name<br /> </strong></td> \n   <td><strong>vCPUs</strong></td> \n   <td><strong>AWS Inferentia2 Chips</strong></td> \n   <td><strong>Accelerator Memory</strong></td> \n   <td><strong>NeuronLink</strong></td> \n   <td><strong>Instance Memory</strong></td> \n   <td><strong>Instance Networking</strong></td> \n  </tr> \n  <tr> \n   <td><strong>inf2.xlarge</strong></td> \n   <td>4</td> \n   <td>1</td> \n   <td>32 GB</td> \n   <td>N/A</td> \n   <td>16 GB</td> \n   <td>Up to 15 Gbps</td> \n  </tr> \n  <tr> \n   <td><strong>inf2.8xlarge</strong></td> \n   <td>32</td> \n   <td>1</td> \n   <td>32 GB</td> \n   <td>N/A</td> \n   <td>128 GB</td> \n   <td>Up to 25 Gbps</td> \n  </tr> \n  <tr> \n   <td><strong>inf2.24xlarge</strong></td> \n   <td>96</td> \n   <td>6</td> \n   <td>192 GB</td> \n   <td>Yes</td> \n   <td>384 GB</td> \n   <td>50 Gbps</td> \n  </tr> \n  <tr> \n   <td><strong>inf2.48xlarge</strong></td> \n   <td>192</td> \n   <td>12</td> \n   <td>384 GB</td> \n   <td>Yes</td> \n   <td>768 GB</td> \n   <td>100 Gbps</td> \n  </tr> \n </tbody> \n</table> \n<p><strong><u>AWS Inferentia2 Innovation<br /> </u></strong>Similar to <a href=\"https://aws.amazon.com/machine-learning/trainium/\">AWS Trainium</a> chips, each AWS Inferentia2 chip has two improved <a href=\"https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/neuroncores-arch.html#neuroncores-v2-arch\">NeuronCore-v2</a> engines, HBM stacks, and dedicated collective compute engines to parallelize computation and communication operations when performing multi-accelerator inference.</p> \n<p>Each NeuronCore-v2 has dedicated scalar, vector, and tensor engines that are purpose-built for DL algorithms. The tensor engine is optimized for matrix operations. The scalar engine is optimized for element-wise operations like <a href=\"https://d2l.ai/chapter_multilayer-perceptrons/mlp.html#relu-function\">ReLU</a> (rectified linear unit) functions. The vector engine is optimized for non-element-wise vector operations, including <a href=\"https://d2l.ai/chapter_convolutional-modern/batch-norm.html#batch-normalization\">batch normalization</a> or <a href=\"https://d2l.ai/chapter_convolutional-neural-networks/pooling.html#pooling\">pooling</a>.</p> \n<p>Here is a short summary of additional AWS Inferentia2 chip and server hardware innovations:</p> \n<ul> \n <li><strong>Data Types</strong> – AWS Inferentia2 supports a wide range of data types, including FP32, TF32, BF16, FP16, and UINT8, so you can choose the most suitable data type for your workloads. It also supports the new configurable FP8 (cFP8) data type, which is especially relevant for large models because it reduces the memory footprint and I/O requirements of the model. The following image compares the supported data types.<a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/04/02/inf2_data_types_updated.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/04/02/inf2_data_types_updated.png\" alt=\"AWS Inferentia2 Supported Data Types\" width=\"1485\" height=\"634\" /></a></li> \n <li><strong>Dynamic Execution, Dynamic Input Shapes</strong> – AWS Inferentia2 has embedded general-purpose digital signal processors (DSPs) that enable dynamic execution, so control flow operators don’t need to be unrolled or executed on the host. AWS Inferentia2 also supports dynamic input shapes that are key for models with unknown input tensor sizes, such as models processing text.</li> \n <li><strong>Custom Operators</strong> – AWS Inferentia2 supports <a href=\"https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-customops/index.html#neuron-c-customops\">custom operators</a> written in C++. Neuron Custom C++ Operators enable you to write C++ custom operators that natively run on NeuronCores. You can use standard PyTorch custom operator programming interfaces to migrate CPU custom operators to Neuron and implement new experimental operators, all without any intimate knowledge of the NeuronCore hardware.</li> \n <li><strong>NeuronLink v2</strong> – Inf2 instances are the first inference-optimized instance on Amazon EC2 to support distributed inference with direct ultra-high-speed connectivity—NeuronLink v2—between chips. NeuronLink v2 uses collective communications (CC) operators such as all-reduce to run high-performance inference pipelines across all chips.</li> \n</ul> \n<p>The following Inf2 distributed inference benchmarks show throughput and cost improvements for <a href=\"https://huggingface.co/facebook/opt-30b\">OPT-30B</a> and <a href=\"https://huggingface.co/facebook/opt-66b\">OPT-66B</a> models over comparable inference-optimized Amazon EC2 instances.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/04/02/inf2_opt_benchmarks_final.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/04/02/inf2_opt_benchmarks_final.png\" alt=\"Amazon EC2 Inf2 Benchmarks\" width=\"2732\" height=\"1390\" /></a></p> \n<p>Now, let me show you how to get started with Amazon EC2 Inf2 instances.</p> \n<p><strong><u>Get Started with Inf2 Instances<br /> </u></strong>The <a href=\"https://github.com/aws/aws-neuron-sdk\">AWS Neuron SDK</a> integrates AWS Inferentia2 into popular machine learning (ML) frameworks like PyTorch. The Neuron SDK includes a compiler, runtime, and profiling tools and is constantly being updated with new features and performance optimizations.</p> \n<p>In this example, I will compile and deploy a pre-trained <a href=\"https://huggingface.co/bert-base-cased-finetuned-mrpc\">BERT model</a> from <a href=\"https://huggingface.co/\">Hugging Face</a> on an EC2 Inf2 instance using the available PyTorch Neuron packages. PyTorch Neuron is based on the <a href=\"https://pytorch.org/xla\">PyTorch XLA</a> software package and enables the conversion of PyTorch operations to AWS Inferentia2 instructions.</p> \n<p>SSH into your Inf2 instance and activate a Python virtual environment that includes the PyTorch Neuron packages. If you’re using a Neuron-provided AMI, you can activate the preinstalled environment by running the following command:</p> \n<pre><code>source aws_neuron_venv_pytorch_p37/bin/activate</code></pre> \n<p>Now, with only a few changes to your code, you can compile your PyTorch model into an AWS Neuron-optimized TorchScript. Let’s start with importing <code>torch</code>, the PyTorch Neuron package <code>torch_neuronx</code>, and the Hugging Face <code>transformers</code> library.</p> \n<pre><code>import torch\n<strong>import torch_neuronx </strong>from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport transformers\n...</code></pre> \n<p>Next, let’s build the tokenizer and model.</p> \n<pre><code>name = \"bert-base-cased-finetuned-mrpc\"\ntokenizer = AutoTokenizer.from_pretrained(name)\nmodel = AutoModelForSequenceClassification.from_pretrained(name, torchscript=True)</code></pre> \n<p>We can test the model with example inputs. The model expects two sentences as input, and its output is whether or not those sentences are a paraphrase of each other.</p> \n<pre><code>def encode(tokenizer, *inputs, max_length=128, batch_size=1):\n    tokens = tokenizer.encode_plus(\n        *inputs,\n        max_length=max_length,\n        padding='max_length',\n        truncation=True,\n        return_tensors=\"pt\"\n    )\n    return (\n        torch.repeat_interleave(tokens['input_ids'], batch_size, 0),\n        torch.repeat_interleave(tokens['attention_mask'], batch_size, 0),\n        torch.repeat_interleave(tokens['token_type_ids'], batch_size, 0),\n    )\n\n# Example inputs\nsequence_0 = \"The company Hugging Face is based in New York City\"\nsequence_1 = \"Apples are especially bad for your health\"\nsequence_2 = \"Hugging Face's headquarters are situated in Manhattan\"\n\nparaphrase = encode(tokenizer, sequence_0, sequence_2)\nnot_paraphrase = encode(tokenizer, sequence_0, sequence_1)\n\n# Run the original PyTorch model on examples\nparaphrase_reference_logits = model(*paraphrase)[0]\nnot_paraphrase_reference_logits = model(*not_paraphrase)[0]\n\nprint('Paraphrase Reference Logits: ', paraphrase_reference_logits.detach().numpy())\nprint('Not-Paraphrase Reference Logits:', not_paraphrase_reference_logits.detach().numpy())</code></pre> \n<p>The output should look similar to this:</p> \n<pre><code>Paraphrase Reference Logits:     [[-0.34945598  1.9003887 ]]\nNot-Paraphrase Reference Logits: [[ 0.5386365 -2.2197142]]</code></pre> \n<p>Now, the <code>torch_neuronx.trace()</code> method sends operations to the Neuron Compiler (neuron-cc) for compilation and embeds the compiled artifacts in a TorchScript graph. The method expects the model and a tuple of example inputs as arguments.</p> \n<pre><code><strong>neuron_model = torch_neuronx.trace(model, paraphrase)</strong></code></pre> \n<p>Let’s test the Neuron-compiled model with our example inputs:</p> \n<pre><code>paraphrase_neuron_logits = neuron_model(*paraphrase)[0]\nnot_paraphrase_neuron_logits = neuron_model(*not_paraphrase)[0]\n\nprint('Paraphrase Neuron Logits: ', paraphrase_neuron_logits.detach().numpy())\nprint('Not-Paraphrase Neuron Logits: ', not_paraphrase_neuron_logits.detach().numpy())</code></pre> \n<p>The output should look similar to this:</p> \n<pre><code>Paraphrase Neuron Logits: [[-0.34915772 1.8981738 ]]\nNot-Paraphrase Neuron Logits: [[ 0.5374032 -2.2180378]]</code></pre> \n<p>That’s it. With just a few lines of code changes, we compiled and ran a PyTorch model on an Amazon EC2 Inf2 instance. To learn more about which DL model architectures are a good fit for AWS Inferentia2 and the current model support matrix, <a href=\"https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/model-architecture-fit.html\">visit the AWS Neuron Documentation</a>.</p> \n<p><strong><u>Available Now<br /> </u></strong>You can launch Inf2 instances today in the AWS US East (Ohio) and US East (N. Virginia) Regions as <a href=\"https://aws.amazon.com/ec2/pricing/on-demand/\">On-Demand</a>, <a href=\"https://aws.amazon.com/ec2/pricing/reserved-instances/\">Reserved</a>, and <a href=\"https://aws.amazon.com/ec2/spot/\">Spot Instances</a> or as part of a <a href=\"https://aws.amazon.com/savingsplans/\">Savings Plan</a>. As usual with Amazon EC2, you pay only for what you use. For more information, see <a href=\"https://aws.amazon.com/ec2/pricing/\">Amazon EC2 pricing</a>.</p> \n<p>Inf2 instances can be deployed using <a href=\"https://aws.amazon.com/machine-learning/amis/\">AWS Deep Learning AMIs</a>, and container images are available via managed services such as <a href=\"https://aws.amazon.com/sagemaker/\">Amazon SageMaker</a>, <a href=\"https://aws.amazon.com/eks/\">Amazon Elastic Kubernetes Service</a> (Amazon EKS), <a href=\"https://aws.amazon.com/ecs/\">Amazon Elastic Container Service</a> (Amazon ECS), and <a href=\"https://aws.amazon.com/hpc/parallelcluster/\">AWS ParallelCluster</a>.</p> \n<p>To learn more, visit our <a href=\"https://aws.amazon.com/ec2/instance-types/inf2/\">Amazon EC2 Inf2 instances page</a>, and please send feedback to <a href=\"https://repost.aws/tags/TAO-wqN9fYRoyrpdULLa5y7g/amazon-ec-2\">AWS re:Post for EC2</a> or through your usual AWS Support contacts.</p> \n<p>— <a href=\"https://twitter.com/anbarth\" target=\"_blank\">Antje</a></p>","author":"Antje Barth","siteTitle":"AWS News Blog","siteHash":"6093e072e4117ec22616e844cb857d03ca62c57a411a8affc77cb5e8b6b15bf6","entryHash":"1243b554d6e85d61dba3ce573f3b1f2966ff49375c67bfebaeba47020c4fdfc5","category":"Tech"}