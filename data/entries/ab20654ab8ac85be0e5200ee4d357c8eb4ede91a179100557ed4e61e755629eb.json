{"title":"Can you do better than top-level AI models on these basic vision tests?","link":"https://arstechnica.com/?p=2036427","date":1720715734000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/07/GettyImages-1283438928-800x374.jpg\" alt=\"Whatever you do, don't ask the AI how many horizontal lines are in this image.\" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2024/07/GettyImages-1283438928.jpg\">Enlarge</a> <span>/</span> Whatever you do, don't ask the AI how many horizontal lines are in this image. (credit: Getty Images)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>In the last couple of years, we've seen <a href=\"https://arstechnica.com/information-technology/2023/03/microsoft-unveils-kosmos-1-an-ai-language-model-with-visual-perception-abilities/\">amazing</a> <a href=\"https://arstechnica.com/information-technology/2023/09/chatgpt-goes-multimodal-with-image-recognition-and-speech-synthesis/\">advancements</a> in AI systems when it comes to <a href=\"https://arstechnica.com/information-technology/2023/07/report-openai-holding-back-gpt-4-image-features-on-fears-of-privacy-issues/\">recognizing</a> and <a href=\"https://arstechnica.com/information-technology/2023/03/microsoft-unveils-kosmos-1-an-ai-language-model-with-visual-perception-abilities/\">analyzing</a> the contents of complicated images. But a new paper highlights how many state-of-the-art \"vision learning Models\" (VLMs) often fail at simple, low-level visual analysis tasks that are trivially easy for a human.</p>\n<p>In the provocatively titled pre-print paper <a href=\"https://arxiv.org/abs/2407.06581\">\"Vision language models are <em>blind</em>\"</a> (which has <a href=\"https://arxiv.org/pdf/2407.06581\">a PDF version</a> that includes a dark sunglasses emoji <em>in the title</em>), researchers from Auburn University and the University of Alberta create eight simple visual acuity tests with objectively correct answers. These range from identifying how often two colored lines intersect to identifying which letter in a long word has been circled to counting how many nested shapes exist in an image (representative examples and results can be <a href=\"https://vlmsareblind.github.io/\">viewed on the research team's webpage</a>).</p>\n  <ul>\n          <li>\n        <div>\n          <div><img src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/07/Screenshot-2024-07-11-115243-980x270.png\" /></div>\n          <p>\n            If you can solve these kinds of puzzles, you may have better visual reasoning than state-of-the-art AIs.                          [credit:\n                              <a href=\"https://arxiv.org/abs/2407.06581\">Rahmanzadehgervi, Bolton, Taesiri, Nguyen.</a>\n                            ]\n                      </p>\n        </div>\n      </li>\n      </ul>\n\n<p>Crucially, these tests are <a href=\"https://github.com/anguyen8/vision-llms-are-blind\">generated by custom code</a> and don't rely on pre-existing images or tests that could be found on the public Internet, thereby \"minimiz[ing] the chance that VLMs can solve by memorization,\" according to the researchers. The tests also \"require minimal to zero world knowledge\" beyond basic 2D shapes, making it difficult for the answer to be inferred from \"textual question and choices alone\" (which has been <a href=\"https://arxiv.org/abs/2403.20330\">identified as an issue for some other visual AI benchmarks</a>).</p></div><p><a href=\"https://arstechnica.com/?p=2036427#p3\">Read 5 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=2036427&amp;comments=1\">Comments</a></p>","author":"Kyle Orland","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"ab20654ab8ac85be0e5200ee4d357c8eb4ede91a179100557ed4e61e755629eb","category":"Tech"}