{"title":"AI firms working on “constitutions” to keep AI from spewing toxic content","link":"https://arstechnica.com/?p=1973907","date":1696599776000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2023/10/ai-list-800x450.jpg\" alt=\"montage of AI company logos\" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2023/10/ai-list.jpg\">Enlarge</a> (credit: <a href=\"https://www.ft.com/content/f23e59a2-4cad-43a5-aed9-3aea6426d0f2\">FT montage/Dreamstime</a>)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>Two of the world’s biggest artificial intelligence companies announced major advances in consumer AI products last week.</p>\n<p>Microsoft-backed OpenAI said that its ChatGPT software could now “see, hear, and speak,” conversing using voice alone and responding to user queries in both pictures and words. Meanwhile, Facebook owner Meta announced that an AI assistant and multiple celebrity chatbot personalities would be available for billions of WhatsApp and Instagram users to talk with.</p>\n<p>But as these groups race to commercialize AI, the so-called “guardrails” that prevent these systems going awry—such as generating toxic speech and misinformation, or helping commit crimes—are struggling to evolve in tandem, according to AI leaders and researchers.</p></div><p><a href=\"https://arstechnica.com/?p=1973907#p3\">Read 22 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=1973907&amp;comments=1\">Comments</a></p>","author":"Financial Times","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"e11ac75e837e56041cd54e1de913b941dca7109048e56da8f7216f1eca2c97ee","category":"Tech"}