{"title":"Nvidia unveils Blackwell B200, the “world’s most powerful chip” designed for AI","link":"https://arstechnica.com/?p=2011117","date":1710862053000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/03/GB200_hero_3-800x450.jpg\" alt=\"The GB200 &quot;superchip&quot; covered with a fanciful blue explosion that suggests computational power bursting forth from within. The chip does not actually glow blue in reality.\" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2024/03/GB200_hero_3.jpg\">Enlarge</a> <span>/</span> The GB200 \"superchip\" covered with a fanciful blue explosion that suggests computational power bursting forth from within. The chip does not actually glow blue in reality. (credit: Nvidia / Benj Edwards)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>On Monday, Nvidia <a href=\"https://nvidianews.nvidia.com/news/nvidia-blackwell-platform-arrives-to-power-a-new-era-of-computing\">unveiled</a> the Blackwell B200 tensor core chip—the company's most powerful single-chip GPU, with 208 billion transistors—which Nvidia claims can reduce AI inference operating costs (such as running <a href=\"https://arstechnica.com/information-technology/2023/11/chatgpt-was-the-spark-that-lit-the-fire-under-generative-ai-one-year-ago-today/\">ChatGPT</a>) and energy consumption by up to 25 times compared to the <a href=\"https://arstechnica.com/information-technology/2022/09/hopper-time-nvidias-most-powerful-ai-chip-yet-ships-in-october/\">H100</a>. The company also unveiled the GB200, a \"superchip\" that combines two B200 chips and a Grace CPU for even more performance.</p>\n\n<p>The news came as part of Nvidia's annual GTC conference, which is taking place this week at the San Jose Convention Center. Nvidia CEO Jensen Huang delivered the <a href=\"https://blogs.nvidia.com/blog/2024-gtc-keynote/\">keynote</a> Monday afternoon. \"We need bigger GPUs,\" Huang said during his keynote. The Blackwell platform will allow the training of trillion-parameter AI models that will make today's generative AI models look rudimentary in comparison, he said. For reference, OpenAI's GPT-3, launched in 2020, included 175 billion parameters. Parameter count is a rough indicator of AI model complexity.</p>\n<p>Nvidia named the Blackwell architecture after <a href=\"https://en.wikipedia.org/wiki/David_Blackwell\">David Harold Blackwell</a>, a mathematician who specialized in game theory and statistics and was the first Black scholar inducted into the National Academy of Sciences. The platform introduces six technologies for accelerated computing, including a second-generation Transformer Engine, fifth-generation NVLink, RAS Engine, secure AI capabilities, and a decompression engine for accelerated database queries.</p></div><p><a href=\"https://arstechnica.com/?p=2011117#p3\">Read 8 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=2011117&amp;comments=1\">Comments</a></p>","author":"Benj Edwards","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"e62f8ddd1fe2a884dea13bebf1b6e2c8f0b543caaa808510d1099ee7e398dc0d","category":"Tech"}