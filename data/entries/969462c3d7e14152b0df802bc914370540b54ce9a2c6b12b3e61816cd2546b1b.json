{"title":"แก้ไขปัญหา Ollama ใน VS Code + Github Copilot Chat","link":"https://www.somkiat.cc/workaround-ollama-in-vscode-and-github-copilot-chat/","date":1771597144000,"content":"<p><img width=\"150\" height=\"150\" src=\"https://www.somkiat.cc/wp-content/uploads/2026/02/ollama-01-150x150.jpg\" alt=\"\" loading=\"lazy\" srcset=\"https://www.somkiat.cc/wp-content/uploads/2026/02/ollama-01-150x150.jpg 150w, https://www.somkiat.cc/wp-content/uploads/2026/02/ollama-01-75x75.jpg 75w\" /></p>\n<figure><a href=\"https://www.somkiat.cc/wp-content/uploads/2026/02/ollama-01.jpg\"><img src=\"https://www.somkiat.cc/wp-content/uploads/2026/02/ollama-01.jpg\" alt=\"\" /></a></figure>\n\n\n\n<p>ปัญหาที่เจอคือ ใน VS Code + GitHub Copilot Chat นั้น<br />ไม่สามารถใช้งาน LLM Model ผ่าน Ollama server ได้<br />ทั้ง ๆ ที่เพิ่มเข้าไปแล้ว แต่ไม่ list ของ model ที่ใช้ได้กลับไม่ขึ้นมา ?<br />โดยปัญหานี้เจอใน <a href=\"https://github.com/ollama/ollama/issues/14135\" target=\"_blank\">Ollama issue 14135</a><br />มาดูแนวทางในการแก้ไขเพื่อให้ใช้งานได้ก่อน (workaround)</p>\n\n\n\n<span></span>\n\n\n\n<figure><a href=\"https://www.somkiat.cc/wp-content/uploads/2026/02/Screenshot-2569-02-20-at-9.10.49%E2%80%AFPM.png\"><img src=\"https://www.somkiat.cc/wp-content/uploads/2026/02/Screenshot-2569-02-20-at-9.10.49%E2%80%AFPM-1024x588.png\" alt=\"\" width=\"438\" height=\"251\" /></a></figure>\n\n\n\n<p>ซึ่งเป็น issue ของ Ollama ที่ทำการเปลี่ยน API endpoint ใหม่<br />แต่ใน VS Code + GitHub Copilot Chat ยังไม่ได้แก้ไข<br />จึงต้องแก้ไขแบบ workaround ให้ใช้งานได้ก่อน !!</p>\n\n\n\n<p><strong>โดยการติดตั้ง extension ใน VS Code ชื่อว่า <a href=\"https://marketplace.visualstudio.com/items?itemName=johnny-zhao.oai-compatible-copilot\" target=\"_blank\">OAI Compatible Provider for Copilot</a></strong></p>\n\n\n\n<figure><a href=\"https://www.somkiat.cc/wp-content/uploads/2026/02/ollama-02.jpg\"><img src=\"https://www.somkiat.cc/wp-content/uploads/2026/02/ollama-02.jpg\" alt=\"\" width=\"536\" height=\"239\" /></a></figure>\n\n\n\n<p>จากนั้นทำการ config Model ใน VS Code ได้เลย</p>\n\n\n\n<ul>\n<li>ทำการ config ใน settings แล้วค้นหา OAI จะเจอ Base URL ให้ชี้ไปที่ Ollama Server API</li>\n</ul>\n\n\n\n<p>จากนั้นทำการดูใน List model ดังนี้</p>\n\n\n\n<figure><a href=\"https://www.somkiat.cc/wp-content/uploads/2026/02/ollama-03.jpg\"><img src=\"https://www.somkiat.cc/wp-content/uploads/2026/02/ollama-03-1024x256.jpg\" alt=\"\" width=\"534\" height=\"134\" /></a></figure>\n\n\n\n<p>เพียงเท่านี้ก็สามารถใช้งาน LLM model จาก Ollama Server ได้แล้ว !!</p>\n","author":"somkiat","siteTitle":"cc :: somkiat","siteHash":"3a23a5a4389e1e40c6fbb16520a8cc20df5b3591c25145ce72aaa18b19e48201","entryHash":"969462c3d7e14152b0df802bc914370540b54ce9a2c6b12b3e61816cd2546b1b","category":"Thai"}