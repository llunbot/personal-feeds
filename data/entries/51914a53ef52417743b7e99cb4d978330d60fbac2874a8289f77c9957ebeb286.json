{"title":"Gemini 2.5 Pro is here with bigger numbers and great vibes","link":"https://arstechnica.com/ai/2025/03/google-says-the-new-gemini-2-5-pro-model-is-its-smartest-ai-yet/","date":1743003399000,"content":"<p>Just a few months after releasing its first <a href=\"https://arstechnica.com/information-technology/2024/12/google-goes-agentic-with-gemini-2-0s-ambitious-ai-agent-features/\">Gemini 2.0 AI models</a>, Google is upgrading again. <a href=\"https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/\">The company says</a> the new Gemini 2.5 Pro Experimental is its \"most intelligent\" model yet, offering a massive context window, multimodality, and reasoning capabilities. Google points to a raft of benchmarks that show the new Gemini clobbering other large language models (LLMs), and our testing seems to back that up—Gemini 2.5 Pro is one of the most impressive generative AI models we've seen.</p>\n<p>Gemini 2.5, like all Google's models going forward, has reasoning built in. The AI essentially fact-checks itself along the way to generating an output. We like to call this \"<a href=\"https://arstechnica.com/information-technology/2024/12/openai-announces-o3-and-o3-mini-its-next-simulated-reasoning-models/\">simulated reasoning</a>,\" as there's no evidence that this process is akin to human reasoning. However, it can go a long way to improving LLM outputs. Google specifically cites the model's \"agentic\" coding capabilities as a beneficiary of this process. Gemini 2.5 Pro Experimental can, for example, generate a full working video game from a single prompt. We've tested this, and it works with the publicly available version of the model.</p>\n<div><div></div><div>\n    <div></div>\n    <div>\n      Gemini 2.5 Pro builds a game in one step.\n\n          </div>\n  </div>\n</div>\n<p>Google says a lot of things about Gemini 2.5 Pro; it's smarter, it's context-aware, it <em>thinks</em>—but it's hard to quantify what constitutes improvement in generative AI bots. There are some clear technical upsides, though. Gemini 2.5 Pro comes with a 1 million token context window, which is common for the big Gemini models but massive compared to competing models like OpenAI GPT or <a href=\"https://arstechnica.com/ai/2025/02/claude-3-7-sonnet-debuts-with-extended-thinking-to-tackle-complex-problems/\">Anthropic Claude</a>. You could feed multiple very long books to Gemini 2.5 Pro in a single prompt, and the output maxes out at 64,000 tokens. That's the same as Flash 2.0, but it's still objectively a lot of tokens compared to other LLMs.</p><p><a href=\"https://arstechnica.com/ai/2025/03/google-says-the-new-gemini-2-5-pro-model-is-its-smartest-ai-yet/\">Read full article</a></p>\n<p><a href=\"https://arstechnica.com/ai/2025/03/google-says-the-new-gemini-2-5-pro-model-is-its-smartest-ai-yet/#comments\">Comments</a></p>","author":"Ryan Whitwam","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"51914a53ef52417743b7e99cb4d978330d60fbac2874a8289f77c9957ebeb286","category":"Tech"}