{"title":"Groq บริการ LLM โชว์ผลทดสอบรัน Llama 2 70B เร็วสุด 240token/s","link":"https://www.blognone.com/node/138283","date":1708409070000,"content":"<div><div><div><p>Groq สตาร์ตอัพผู้พัฒนาชิป GroqChip 1 สำหรับการรันโมเดลปัญญาประดิษฐ์ LLM และผู้ให้บริการ LLM แบบคลาวด์ระบุถึงผลทดสอบของ ArtificialAnalysis.ai ที่แสดงให้เห็นว่า Groq เป็นผู้ให้บริการที่สามารถประมวลผล LLM ได้เร็วที่สุดในตลาด</p>\n<p>โมเดลที่ใช้ทดสอบเป็นโมเดล Llama 2 70B ที่มีคลาวด์หลายเจ้าให้บริการกัน รวมถึงคลาวด์รายใหญ่อย่าง Amazon Bedrock และ Azure  แต่จุดที่ Groq นำมาเน้นคือความเร็วในการตอบ ที่ระยะเวลาจนถึงการตอบ 100 token แรกนั้นกินเวลาเพียง 0.7 วินาที และอัตราการตอบรวมได้เร็วกว่า 240 token ต่อวินาที นับว่าเร็วกว่าคู่แข่งอันดับสองแบบห่างไกล (Lepton รันได้สูงกว่า 120 token ต่อวินาทีไปเล็กน้อย)</p>\n<p>ชิป GroqChip 1 นั้นทาง Groq เรียกว่าเป็น LPU หรือ language processing unit จุดแตกต่างคือในชิปมี SRAM มากถึง 230MB สำหรับการรัน AI โดยเฉพาะ สถาปัตยกรรมโดยรวมเรียบง่ายกว่าชิปกราฟิก</p>\n<p>ตอนนี้โมเดลที่ดีที่สุดที่ Groq ให้บริการคือ Mixtral 8x7B 32k สามารถรันได้ที่ระดับ 500 token ต่อวินาที และ<a href=\"https://groq.com\">เว็บ Groq เปิดให้ทุกคนทดสอบได้โดยไม่ต้องสมัครสมาชิก</a></p>\n<p>ที่มา - <a href=\"https://wow.groq.com/news_press/groq-lpu-inference-engine-leads-in-first-independent-llm-benchmark/\">Groq</a></p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/05710f830f352c508af7be35ca34f284.png\" /><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/1092e45305b9b02cf388f4b2672094b7.png\" /></p>\n</div></div></div><div><div>Topics: </div><div><div><a href=\"/topics/llm\">LLM</a></div><div><a href=\"/topics/artificial-intelligence\">Artificial Intelligence</a></div></div></div>","author":"lew","siteTitle":"Blognone","siteHash":"ededadcf18490b3937e7dd89ebe8c00dc129addbdf1ebe4aff1f458146693da0","entryHash":"21f9f391f223a02371c1f614d0b3bde8060a4c7fc62dc3546110ccc92055eb9f","category":"Thai"}