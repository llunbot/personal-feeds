{"title":"รู้จัก Alpaca และ Koala โมเดล LLM ที่พัฒนาต่อจาก LLaMA ของ Meta, ขนาดเล็กกว่า GPT แต่แข่งขันได้","link":"https://www.blognone.com/node/133324","date":1680757181000,"content":"<div><div><div><p>ถึงแม้ <a href=\"https://www.blognone.com/node/133075\">OpenAI เปลี่ยนมาใช้แนวทางปิด ไม่เปิดเผยรายละเอียดของโมเดล GPT-4</a> และฝั่งกูเกิลเองก็<a href=\"https://www.blognone.com/node/133261\">ยังค่อนข้างระมัดระวังในการปล่อย Bard ทีละนิด</a> แต่โลกเราก็ยังมีโมเดลภาษาขนาดใหญ่ (Large Language Model หรือ LLM) ตัวอื่นให้ใช้งาน โดยเฉพาะ <a href=\"https://www.blognone.com/node/132773\">LLaMA ของ Meta ที่เปิดตัวในเดือนกุมภาพันธ์ 2023</a> ซึ่งเป็นโอเพนซอร์ส ใช้สัญญาอนุญาตแบบ GPLv3 และเปิดทางให้หน่วยงานวิจัยมาขอชุดข้อมูลที่ใช้เทรนไปศึกษาได้</p>\n<p>ตัวอย่างก่อนหน้านี้คือ <a href=\"https://www.blognone.com/node/132809\">Nebuly AI สร้าง ChatLLaMA แบบโอเพนซอร์ส ใช้โมเดล LLaMA ของ Meta เป็นฐาน</a></p>\n<p>ส่วนมหาวิทยาลัยที่นำ LLaMa ไปใช้งานคือ Stanford มีโมเดลชื่อ <a href=\"https://crfm.stanford.edu/2023/03/13/alpaca.html\">Alpaca</a> (ทุกคนล้วนรักสัตว์) เป็นการนำเอาโมเดล LLaMA ขนาดเล็ก 7 พันล้านพารามิเตอร์ มาเทรนปรับแต่ง (fine-tuning) อีกรอบด้วย<a href=\"https://arxiv.org/abs/2212.10560\">ข้อมูลการถาม-ตอบจาก OpenAI GPT</a> (ฐานข้อมูล text-davinci-003) ช่วยให้โมเดล LLaMA ที่ขนาดเล็กกว่า GPT มาก สามารถตอบคำถามได้ไกล้เคียงกับ GPT มากขึ้น</p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/c96ff39246ab67f593ca26e7e55c9f49.jpg\" /></p>\n<p>ล่าสุด BAIR หรือ Berkeley Artificial Intelligence Research ห้องวิจัยปัญญาประดิษฐ์ของมหาวิทยาลัย UC Berkeley เปิดตัวโมเดลแชทบ็อต <a href=\"https://bair.berkeley.edu/blog/2023/04/03/koala/\">Koala</a> ซึ่งเป็นการดัดแปลง LLaMA ให้ตอบคำถามได้แม่นยำขึ้น โดยไม่ต้องใช้จำนวนพารามิเตอร์มากเท่ากับ GPT-4</p>\n<p>Koala เป็นการนำโมเดล LLaMA ขนาดใหญ่ 1.3 หมื่นล้านพารามิเตอร์ มาเทรนเพิ่ม (fine-tune) โดยใช้ข้อมูลที่หาได้จากสาธารณะ ทั้งจากบนเว็บทั่วไปและจากชุดข้อมูลเทรนที่เปิดสาธารณะอยู่แล้ว (บางส่วนเป็นชุดข้อมูลคำถามคำตอบจาก ShareGPT รวมถึงข้อมูลจาก Alpaca ด้วย) ได้ออกมาเป็นโมเดล Koala-13B ขนาดเท่าๆ เดิมกับต้นฉบับ แต่มีความแม่นยำในการตอบคำถามมากขึ้น</p>\n<p>ผลการทดสอบโดยนำคำตอบจาก Koala มาเทียบกับโมเดลอื่นๆ ทั้ง ChatGPT และ Alpaca แล้วให้มนุษย์ 100 คนตัดสินแบบ blind test ว่าอันไหนดีกว่า (180 คำถามทดสอบ) พบว่าโมเดล Koala สามารถเอาชนะ Alpaca ได้แล้ว แต่ยังแพ้ ChatGPT อยู่ แต่ก็ถือว่าทำได้ดี แพ้ไม่ขาด ด้วยขนาดพารามิเตอร์ที่เล็กกว่ามาก</p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/ff450784133a2d8367a769f3762d5da2.png\" /></p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/a88bc1b589ce82bc047efb3d15571abb.png\" /></p>\n<p>BAIR บอกว่าโมเดล Koala ที่พัฒนาต่อจาก LLaMA มีจุดอ่อนร่วมเหมือนโมเดลตระกูล LLM อื่นๆ คือ อาจเพ้อเจ้อ (hallucinate) และตอบคำถามแบบมั่วๆ อย่างมั่นใจ ซึ่งในแง่การวิจัยก็ต้องหาวิธีพัฒนาปิดจุดอ่อนตรงนี้กันต่อไป</p>\n<p>ทั้งโมเดล Alpaca และ Koala เปิดตัวโมเดลและชุดข้อมูลเป็นโอเพนซอร์ส รายละเอียดอ่านได้จากลิงก์ของทั้งสองโครงการ</p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/6c460034f481fa6315b1ee71202ba97b.png\" /></p>\n</div></div></div><div><div>Topics: </div><div><div><a href=\"/topics/artificial-intelligence\">Artificial Intelligence</a></div><div><a href=\"/topics/open-source\">Open Source</a></div><div><a href=\"/topics/meta\">Meta</a></div><div><a href=\"/topics/research\">Research</a></div><div><a href=\"/topics/stanford\">Stanford</a></div><div><a href=\"/topics/berkeley\">Berkeley</a></div></div></div>","author":"mk","siteTitle":"Blognone","siteHash":"ededadcf18490b3937e7dd89ebe8c00dc129addbdf1ebe4aff1f458146693da0","entryHash":"7bb25c44a49848abe804b86b45101beed921bee5cf46872f834cceae4204a469","category":"Thai"}