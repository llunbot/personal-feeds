{"title":"Amazon Bedrock now provides access to Meta’s Llama 2 Chat 13B model","link":"https://aws.amazon.com/blogs/aws/amazon-bedrock-now-provides-access-to-llama-2-chat-13b-model/","date":1699906512000,"content":"<p>Today, we’re announcing the availability of <a href=\"https://aws.amazon.com/bedrock/llama-2/\">Meta’s Llama 2 Chat 13B large language model</a> (LLM) on <a href=\"https://aws.amazon.com/bedrock/\">Amazon Bedrock</a>. With this launch, Amazon Bedrock becomes the first public cloud service to offer a fully managed API for Llama 2, Meta’s next-generation LLM. Now, organizations of all sizes can access Llama 2 Chat models on Amazon Bedrock without having to manage the underlying infrastructure. This is a step change in accessibility.</p> \n<p>Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies, including <a href=\"https://aws.amazon.com/bedrock/jurassic/\">AI21 Labs</a>, <a href=\"https://aws.amazon.com/bedrock/claude/\">Anthropic</a>, <a href=\"https://aws.amazon.com/bedrock/cohere-command/\">Cohere</a>, <a href=\"https://aws.amazon.com/bedrock/stable-diffusion/\">Stability AI</a>, <a href=\"https://aws.amazon.com/bedrock/titan/\">Amazon</a>, and now <a href=\"https://aws.amazon.com/bedrock/llama-2/\">Meta</a>, along with a broad set of capabilities to build <a href=\"https://aws.amazon.com/generative-ai/\">generative AI</a> applications, simplifying the development while maintaining privacy and security. <a href=\"https://aws.amazon.com/blogs/aws/amazon-bedrock-is-now-generally-available-build-and-scale-generative-ai-applications-with-foundation-models/\">You can read more about Amazon Bedrock in Antje’s post here</a>.</p> \n<p>Llama 2 is a family of publicly available LLMs by Meta. The Llama 2 base model was pre-trained on 2 trillion tokens from online public data sources. <a href=\"https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md\">According to Meta</a>, the training of Llama 2 13B consumed 184,320 GPU/hour. That’s the equivalent of 21.04 years of a single GPU, not accounting for bissextile years.</p> \n<p>Built on top of the base model, the Llama 2 Chat model is optimized for dialog use cases. It is fine-tuned with over 1 million human annotations (a technique known as <a href=\"https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback\">reinforcement learning from human feedback</a> or RLHF) and has undergone testing by Meta to identify performance gaps and mitigate potentially problematic responses in chat use cases, such as offensive or inappropriate responses.</p> \n<p>To promote a responsible, collaborative AI innovation ecosystem, Meta established a range of resources for all who use Llama 2: individuals, creators, developers, researchers, academics, and businesses of any size. In particular, I like the <a href=\"https://ai.meta.com/llama/responsible-use-guide/\">Meta Responsible Use Guide</a>, a resource for developers that provides best practices and considerations for building products powered by LLMs in a responsible manner, covering various stages of development from inception to deployment. This guide fits well in the set of <a href=\"https://aws.amazon.com/machine-learning/responsible-ai/resources/\">AWS tools and resources</a> to build AI responsibly.</p> \n<p>You can now integrate the LLama 2 Chat model in your applications written in any programming language by calling the Amazon Bedrock API or using the <a href=\"https://aws.amazon.com/tools/\">AWS SDKs</a> or the <a href=\"https://aws.amazon.com/cli/\">AWS Command Line Interface (AWS CLI)</a>.</p> \n<p><span><strong>Llama 2 Chat in action<br /> </strong></span>Those of you who read the AWS News blog regularly know we like to show you the technologies we write about. So let’s write code to interact with Llama2.</p> \n<p>I was lucky enough to talk at the <a href=\"https://conf.ugperu.cloud/\">AWS UG Perú Conf</a> a few weeks ago. <a href=\"https://www.linkedin.com/posts/jeffbarr_aws-activity-7116878128857182208-QvST?utm_source=share&amp;utm_medium=member_desktop\">Jeff</a> and <a href=\"https://www.linkedin.com/posts/marciavillalba_empieza-el-keynote-de-aws-conf-peru-activity-7111350370609602562-ePuK?utm_source=share&amp;utm_medium=member_desktop\">Marcia</a> were there too. Jeff opened the conference with an inspiring talk about generative AI, and he used a wall of <a href=\"https://www.linkedin.com/feed/update/urn:li:activity:7111345922562740224?updateEntityUrn=urn%3Ali%3Afs_feedUpdate%3A%28V2%2Curn%3Ali%3Aactivity%3A7111345922562740224%29\">generated images of llamas</a>, the emblematic animal from Perú. So what better subject to talk about with Llama 2 Chat than llamas?</p> \n<p>(And before writing code, I can’t resist sharing two photos of llamas I took during my visit to <a href=\"https://en.wikipedia.org/wiki/Machu_Picchu\">Machu Picchu</a>)</p> \n<table> \n <tbody> \n  <tr> \n   <td><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/10/21/6I3A1224-1188218354-x1920-2.jpeg\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/10/21/6I3A1224-1188218354-x1920-2-scaled.jpeg\" alt=\"A white llama at Machu Picchu\" width=\"400\" height=\"267\" /></a></td> \n   <td><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/10/21/6I3A1233-1188218354-x1920.jpeg\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/10/21/6I3A1233-1188218354-x1920-scaled.jpeg\" alt=\"A brown llama at Machu Picchu\" width=\"400\" height=\"267\" /></a></td> \n  </tr> \n </tbody> \n</table> \n<p>To get started with a new model on Bedrock, I first navigate to <a href=\"https://console.aws.amazon.com/bedrock/\">Amazon Bedrock on the console</a>. I select <strong>Model access</strong> on the bottom left pane, then select the <strong>Edit</strong> button on the top right side, and enable access to the Llama 2 Chat model.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/11/04/2023-11-04_16-28-10.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/11/04/2023-11-04_16-28-10.png\" alt=\"Bedrock Llama2 Grant Access\" width=\"800\" height=\"398\" /></a></p> \n<p>In the left navigation bar, under <strong>Playgrounds</strong>, I select <strong>Chat</strong> to interact with the model without writing any code.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/10/26/Screenshot-2023-10-26-at-10.23.01-AM.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/10/26/Screenshot-2023-10-26-at-10.23.01-AM.png\" alt=\"Bedrock chat playground for llama2\" width=\"800\" height=\"488\" /></a></p> \n<p>Now that I know I can access the model, I open a code editor on my laptop. I assume you have the <a href=\"https://aws.amazon.com/cli/\">AWS Command Line Interface (AWS CLI)</a> configured, which will allow the AWS SDK to locate your AWS credentials. I use Python for this demo, but I want to show that Bedrock can be called from any language. I also <a href=\"https://gist.github.com/sebsto/9ef3b9bdaed7b6493b7921a0482fdf19\">share a public gist</a> with the same code sample written in the <a href=\"https://www.swift.org/\">Swift programming language</a>.</p> \n<p>Returning to Python, I first run the <a href=\"https://docs.aws.amazon.com/bedrock/latest/APIReference/API_ListFoundationModels.html\">ListFoundationModels</a> API call to discover the <code>modelId</code> for Llama 2 Chat 13B.</p> \n<pre><code>import boto3\n\nbedrock = boto3.client(service_name='bedrock', region_name='us-east-1')\nlistModels = bedrock.list_foundation_models(byProvider='meta')\nprint(\"\\n\".join(list(map(lambda x: f\"{x['modelName']} : { x['modelId'] }\", listModels['modelSummaries']))))\n</code></pre> \n<p>Running this code produces the list:</p> \n<pre><code>Llama 2 Chat 13B : meta.llama2-13b-chat-v1\n</code></pre> \n<p>I select <code>meta-llama2-chat-13b</code> model ID and write the code to send a prompt to the LLama 2 Chat 13B model.</p> \n<pre><code>import boto3\nimport json\n\nllamaModelId = 'meta.llama2-13b-chat-v1' \nprompt = \"What is the difference between a llama and an alpaca?\"\n\nllamaPayload = json.dumps({ \n\t'prompt': prompt,\n    'max_gen_len': 512,\n\t'top_p': 0.9,\n\t'temperature': 0.2\n})\n\nbedrock_runtime = boto3.client(\n    service_name='bedrock-runtime', \n    region_name='us-east-1'\n)\nresponse = bedrock_runtime.invoke_model(\n    body=llamaPayload, \n    modelId=llamaModelId, \n    accept='application/json', \n    contentType='application/json'\n)\nbody = response.get('body').read().decode('utf-8')\nresponse_body = json.loads(body)\nprint(response_body['generation'].strip())</code></pre> \n<p>The response is printed:</p> \n<pre><code>Llamas and alpacas are both members of the camelid family, \nbut they are different species with distinct physical and behavioral characteristics. \nHere are some of the main differences: (...)</code></pre> \n<p>I redacted the answer for brevity.</p> \n<p>You can tune the different request parameters to see how they influence the response.</p> \n<p><span><strong>Optimizing your prompts<br /> </strong></span>To get the best results out of the Llama 2 Chat model, you can experiment with a couple of <a href=\"https://aws.amazon.com/what-is/prompt-engineering/\">prompt engineering techniques</a>, such as using roles or chain-of-thought prompts.</p> \n<p>Using roles, you tell the model how to behave by giving it a role and instructions before asking your question. For example:</p> \n<pre><code>Prompt: You are a teacher. You explain things for six-year old kids.\nWhat is the difference between a llama and an alpaca?\n\nAnswer: Great question! Both llamas and alpacas are animals that live in South America, and they look a little bit similar. But there are some differences that we can talk about.\nFirst, let's talk about their fur. Llamas have longer fur than alpacas. It's like they have a big, fluffy coat that helps them stay warm in the cold weather. Alpacas, on the other hand, have shorter fur that's a little bit smoother.\nAnother difference is their ears. Llamas have banana-shaped ears that are really big, while alpacas have small, triangular ears.\nFinally, let's talk about their size. Llamas are generally bigger than alpacas. They can grow up to 6 feet tall and weigh a lot, while alpacas are usually smaller, only growing up to 3 feet tall.\nSo, those are some of the main differences between llamas and alpacas. They're both really cool animals, and it's fun to learn about them!</code></pre> \n<p>With chain-of-thought prompts, you ask the model to explain each step that led to the answer. For example:</p> \n<pre><code>Prompt: You are a a very intelligent bot with exceptional critical thinking.\nI went to the market and bought 10 apples. I gave 2 apples to your friend and 2 to the helper. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\nLet's think step by step.\n\nAnswer:\n1. You went to the market and bought 10 apples.\n2. You gave 2 apples to your friend.\n3. You gave 2 apples to the helper.\n4. You went back to the market and bought 5 more apples.\n5. You ate 1 apple.\nNow, let's count how many apples you have:\n10 apples (from the first trip to the market) - 2 apples (given to your friend) - 2 apples (given to the helper) = 6 apples\n6 apples + 5 apples (from the second trip to the market) = 11 apples\n11 apples - 1 apple (that you ate) = 10 apples\nTherefore, you remain with 10 apples.\nI hope this helps! Do you have any more questions or would you like to play again?</code></pre> \n<p>Meta shared more examples in section 4.2 of their <a href=\"https://github.com/facebookresearch/llama-recipes/blob/main/examples/Getting_to_know_Llama.ipynb\">getting to know Llama notebook</a>.</p> \n<p><span><strong>Availability<br /> </strong></span>The Llama 2 Chat model is available today for all AWS customers in two of the <a href=\"https://aws.amazon.com/about-aws/global-infrastructure/regions_az/\">AWS Regions</a> where Bedrock is available: US East (N. Virginia) and US West (Oregon).</p> \n<p>You will be charged for model inference. You can choose to be charged on a pay-as-you-go basis, with no upfront or recurring fees; AWS charges per processed input and output tokens. Or you can provision sufficient throughput to meet your application’s performance requirements in exchange for a time-based term commitment. <a href=\"https://aws.amazon.com/bedrock/pricing/\">The Bedrock pricing page has the details</a>.</p> \n<p>With this information, you’re prepared to start using Amazon Bedrock and the Llama 2 Chat model in your applications.</p> \n<p><a href=\"https://console.aws.amazon.com/bedrock\">Go build!</a></p> \n<a href=\"https://twitter.com/sebsto\">-- seb</a>","author":"Sébastien Stormacq","siteTitle":"AWS News Blog","siteHash":"6093e072e4117ec22616e844cb857d03ca62c57a411a8affc77cb5e8b6b15bf6","entryHash":"c69333193022d8939a9eb8564f8b49f853c5ac021bf68da38ee794a0e17146f9","category":"Tech"}