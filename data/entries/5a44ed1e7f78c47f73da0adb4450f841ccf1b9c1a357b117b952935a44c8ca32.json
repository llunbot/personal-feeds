{"title":"Apple’s controversial iCloud Photos CSAM scanning scrubbed from site","link":"https://www.macworld.com/article/559731/apple-csam-icloud-photo-scanning-removed.html","date":null,"content":"<div>\n<section><div></div></section><p>Earlier this year, <a href=\"https://www.macworld.com/article/352875/ios-15-csam-scanning-icloud-photos-messages-siri-search-faq.html\">Apple announced a new system</a> designed to catch potential CSAM (Child Sexual Abuse Material) by scanning iPhone users’ photos. After an instant uproar, <a href=\"https://www.macworld.com/article/355366/pple-to-postpone-controversial-csam-rollout.html\">Apple delayed the system</a> until later in 2021, and now it seems like it might not arrive for a while longer, if at all.</p>\n\n\n\n<p>Just days after releasing the Messages component of its multi-pronged child-safety approach in <a href=\"https://www.macworld.com/article/547488/ios-15-2-features-security-update-releases-how-to-install.html\">iOS 15.2</a>, Apple has removed all references to the CSAM scanning tech on Apple.com. As <a href=\"https://go.redirectingat.com/?id=111346X1569486&amp;url=https://www.macrumors.com/2021/12/15/apple-nixes-csam-references-website/&amp;xcust=1-1-559731-1-0-0&amp;sref=https://www.macworld.com/feed\">spotted by Macrumors</a>, the previous Child Safety webpage now leads to <a href=\"https://apple.sjv.io/c/321564/435031/7613?u=https://www.apple.com/child-safety/&amp;subid1=1-1-559731-1-0-0\">a support page</a> for the Communication safety in Messages feature. Apple says the feature is still “delayed” and <a href=\"https://go.redirectingat.com/?id=111346X1569486&amp;url=https://www.theverge.com/2021/12/15/22837631/apple-csam-detection-child-safety-feature-webpage-removal-delay&amp;xcust=1-1-559731-1-0-0&amp;sref=https://www.macworld.com/feed\">not canceled</a>, though it will clearly miss its self imposed the 2021 deadline.</p>\n\n\n\n<p>Apple’s CSAM detection announcement generated controversy almost as soon as it announced. The system as described scans users’ iPhones for images for recognizable hashes in the National Center for Missing and Exploited Children’s database, which are then checked on a list of known CSAM hashes. If a match is made, the photo is reviewed by a person at Apple after it is uploaded to iCloud, and if it indeed contains CSAM, the person who uploaded it would then be referred to the appropriate authorities.</p>\n\n\n\n<p>Arguments made against this feature are mainly centered around the idea that it could be implemented for other uses. For example, a government could demand that Apple create a similar process to check for images deemed determinantal to the government’s policies. Some also were concerned that the scanning is being done on the iPhone itself, though results aren’t delivered until photos are uploaded to iCloud.  </p>\n\n\n\n<p>Apple launched a new Messages feature in iOS 15.2 that can warn children and parents when receiving or sending photos that contain nudity, which was part of the original announcement. Unlike the proposed CSAM scanning, the feature is off by default and parents need to explicitly opt in as part of Family Sharing. Siri and search will also warn people when attempting to find potential CSAM.</p>\n\n\n\n<p><strong>Update 12:10pm ET:</strong> <em>Apple says the feature is still delayed but not canceled.</em></p>\n</div>","author":"","siteTitle":"Macworld","siteHash":"37e84dd5a21fa961d6d6630e269546024dbb7741b2e2fadbe74f47383c70dfbb","entryHash":"5a44ed1e7f78c47f73da0adb4450f841ccf1b9c1a357b117b952935a44c8ca32","category":"Apple"}