{"title":"Matthew Panzarino Interviews Erik Neuenschwander, Apple’s Head of Privacy","link":"https://techcrunch.com/2021/08/10/interview-apples-head-of-privacy-details-child-abuse-detection-and-messages-safety-features/","date":1628723863000,"content":"\n<p>Terrific interview — great questions, asked in the right order:</p>\n\n<blockquote>\n  <p><strong>Panzarino:</strong> <em>One of the bigger queries about this system is\nthat Apple has said that it will just refuse action if it is asked\nby a government or other agency to compromise by adding things\nthat are not CSAM to the database to check for them on-device.\nThere are some examples where Apple has had to comply with local\nlaw at the highest levels if it wants to operate there, China\nbeing an example. So how do we trust that Apple is going to hew to\nthis rejection of interference if pressured or asked by a\ngovernment to compromise the system?</em></p>\n\n<p><strong>Neuenschwander:</strong> Well first, that is launching only for U.S.,\niCloud accounts, and so the hypotheticals seem to bring up generic\ncountries or other countries that aren’t the U.S. when they speak\nin that way, and the therefore it seems to be the case that people\nagree U.S. law doesn’t offer these kinds of capabilities to our\ngovernment.</p>\n\n<p>But even in the case where we’re talking about some attempt to\nchange the system, it has a number of protections built in that\nmake it not very useful for trying to identify individuals holding\nspecifically objectionable images. The hash list is built into the\noperating system, we have one global operating system and don’t\nhave the ability to target updates to individual users and so hash\nlists will be shared by all users when the system is enabled. And\nsecondly, the system requires the threshold of images to be\nexceeded so trying to seek out even a single image from a person’s\ndevice or set of people’s devices won’t work because the system\nsimply does not provide any knowledge to Apple for single photos\nstored in our service. And then, thirdly, the system has built\ninto it a stage of manual review where, if an account is flagged\nwith a collection of illegal CSAM material, an Apple team will\nreview that to make sure that it is a correct match of illegal\nCSAM material prior to making any referral to any external entity.\nAnd so the hypothetical requires jumping over a lot of hoops,\nincluding having Apple change its internal process to refer\nmaterial that is not illegal, like known CSAM and that we don’t\nbelieve that there’s a basis on which people will be able to make\nthat request in the U.S. And the last point that I would just add\nis that it does still preserve user choice, if a user does not\nlike this kind of functionality, they can choose not to use iCloud\nPhotos and if iCloud Photos is not enabled no part of the system\nis functional.</p>\n</blockquote>\n\n<p>Neuenschwander also confirms that if you’re not using iCloud Photos, none of the system operates:</p>\n\n<blockquote>\n  <p>If users are not using iCloud Photos, NeuralHash will not run and\nwill not generate any vouchers. CSAM detection is a neural hash\nbeing compared against a database of the known CSAM hashes that\nare part of the operating system image. None of that piece, nor\nany of the additional parts including the creation of the safety\nvouchers or the uploading of vouchers to iCloud Photos, is\nfunctioning if you’re not using iCloud Photos.</p>\n</blockquote>\n\n<div>\n<a href=\"https://daringfireball.net/linked/2021/08/11/panzarino-neuenschwander-interview\"> ★ </a>\n</div>\n\n\t","author":"John Gruber","siteTitle":"Daring Fireball","siteHash":"fc569638025dadf22a867470f8215f38855cf50e975782a6c989909474292a36","entryHash":"24def89f9db13df3571b364e3d92dfd2bc8a95a9650302ec2cb6fd4b02479601","category":"Tech"}