{"title":"Package and deploy models faster with new tools and guided workflows in Amazon SageMaker","link":"https://aws.amazon.com/blogs/aws/package-and-deploy-models-faster-with-new-tools-and-guided-workflows-in-amazon-sagemaker/","date":1701285285000,"content":"<p>I’m happy to share that <a href=\"https://aws.amazon.com/sagemaker/deploy/\">Amazon SageMaker</a> now comes with an improved model deployment experience to help you deploy traditional machine learning (ML) models and foundation models (FMs) faster.</p> \n<p>As a data scientist or ML practitioner, you can now use the new <code>ModelBuilder</code> class in the <a href=\"https://sagemaker.readthedocs.io/en/stable/\">SageMaker Python SDK</a> to package models, perform local inference to validate runtime errors, and deploy to SageMaker from your local IDE or SageMaker Studio notebooks.</p> \n<p>In <a href=\"https://aws.amazon.com/sagemaker/studio/\">SageMaker Studio</a>, new interactive model deployment workflows give you step-by-step guidance on which instance type to choose to find the most optimal endpoint configuration. SageMaker Studio also provides additional interfaces to add models, test inference, and enable auto scaling policies on the deployed endpoints.</p> \n<p><strong><u>New tools in SageMaker Python SDK<br /> </u></strong>The SageMaker Python SDK has been updated with new tools, including <code>ModelBuilder</code> and <code>SchemaBuilder</code> classes that unify the experience of converting models into SageMaker deployable models across ML frameworks and model servers. Model builder automates the model deployment by selecting a compatible SageMaker container and capturing dependencies from your development environment. Schema builder helps to manage serialization and deserialization tasks of model inputs and outputs. You can use the tools to deploy the model in your local development environment to experiment with it, fix any runtime errors, and when ready, transition from local testing to deploy the model on SageMaker with a single line of code.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/11/07/2023-sm-model-builder.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/11/07/2023-sm-model-builder.png\" alt=\"Amazon SageMaker ModelBuilder\" width=\"2710\" height=\"1036\" /></a></p> \n<p>Let me show you how this works. In the following example, I choose the <a href=\"https://huggingface.co/tiiuae/falcon-7b\">Falcon-7B</a> model from the <a href=\"https://huggingface.co/models\">Hugging Face model hub</a>. I first deploy the model locally, run a sample inference, perform local benchmarking to find the optimal configuration, and finally deploy the model with the suggested configuration to SageMaker.</p> \n<p>First, import the updated SageMaker Python SDK and define a sample model input and output that matches the prompt format for the selected model.</p> \n<pre><code>import sagemaker\nfrom sagemaker.serve.builder.model_builder import ModelBuilder\nfrom sagemaker.serve.builder.schema_builder import SchemaBuilder\nfrom sagemaker.serve import Mode\n\nprompt = \"Falcons are\"\nresponse = \"Falcons are small to medium-sized birds of prey related to hawks and eagles.\"\n\nsample_input = {\n    \"inputs\": prompt,\n    \"parameters\": {\"max_new_tokens\": 32}\n}\n\nsample_output = [{\"generated_text\": response}]</code></pre> \n<p>Then, create a <code>ModelBuilder</code> instance with the Hugging Face model ID, a <code>SchemaBuilder</code> instance with the sample model input and output, define a local model path, and set the mode to <code>LOCAL_CONTAINER</code> to deploy the model locally. The schema builder generates the required functions for serializing and deserializing the model inputs and outputs.</p> \n<pre><code>model_builder = ModelBuilder(\n    model=\"tiiuae/falcon-7b\",\n    schema_builder=SchemaBuilder(sample_input, sample_output),\n    model_path=\"/path/to/falcon-7b\",\n    mode=Mode.LOCAL_CONTAINER,\n\tenv_vars={\"HF_TRUST_REMOTE_CODE\": \"True\"}\n)</code></pre> \n<p>Next, call <code>build()</code> to convert the PyTorch model into a SageMaker deployable model. The build function generates the required artifacts for the model server, including the <code>inferency.py</code> and <code>serving.properties</code> files.</p> \n<pre><code>local_mode_model = model_builder.build()</code></pre> \n<p>For FMs, such as Falcon, you can optionally run <code>tune()</code> in local container mode that performs local benchmarking to find the optimal model serving configuration. This includes the tensor parallel degree that specifies the number of GPUs to use if your environment has multiple GPUs available. Once ready, call <code>deploy()</code> to deploy the model in your local development environment.</p> \n<pre><code>tuned_model = local_mode_model.tune()\ntuned_model.deploy()</code></pre> \n<p>Let’s test the model.</p> \n<pre><code>updated_sample_input = model_builder.schema_builder.sample_input\nprint(updated_sample_input)\n\n{'inputs': 'Falcons are',\n 'parameters': {'max_new_tokens': 32}}\n \nlocal_tuned_predictor.predict(updated_sample_input)[0][\"generated_text\"]</code></pre> \n<p>In my demo, the model returns the following response:</p> \n<blockquote>\n <p>a type of bird that are known for their sharp talons and powerful beaks. They are also known for their ability to fly at high speeds […]</p>\n</blockquote> \n<p>When you’re ready to deploy the model on SageMaker, call <code>deploy()</code> again, set the mode to <code>SAGEMAKLER_ENDPOINT</code>, and provide an <a href=\"https://aws.amazon.com/iam/\">AWS Identity and Access Management (IAM)</a> role with appropriate permissions.</p> \n<pre><code>sm_predictor = tuned_model.deploy(\n    mode=Mode.SAGEMAKER_ENDPOINT, \n\trole=\"arn:aws:iam::012345678910:role/role_name\"\n)</code></pre> \n<p>This starts deploying your model on a SageMaker endpoint. Once the endpoint is ready, you can run predictions.</p> \n<pre><code>new_input = {'inputs': 'Eagles are','parameters': {'max_new_tokens': 32}}\nsm_predictor.predict(new_input)[0][\"generated_text\"])</code></pre> \n<p><strong><u>New SageMaker Studio model deployment experience<br /> </u></strong>You can start the new interactive model deployment workflows by selecting one or more models to deploy from the models landing page or <a href=\"https://aws.amazon.com/sagemaker/jumpstart/\">SageMaker JumpStart</a> model details page or by creating a new endpoint from the endpoints details page.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/11/21/2023-sm-model-deploy-13.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/11/21/2023-sm-model-deploy-13.png\" alt=\"Amazon SageMaker - New Model Deployment Experience\" width=\"2097\" height=\"889\" /></a></p> \n<p>The new workflows help you quickly deploy the selected model(s) with minimal inputs. If you used <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.html\">SageMaker Inference Recommender</a> to benchmark your model, the dropdown will show instance recommendations from that benchmarking.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/11/21/2023-sm-model-deploy-12.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/11/21/2023-sm-model-deploy-12.png\" alt=\"Model deployment experience in SageMaker Studio\" width=\"1658\" height=\"909\" /></a></p> \n<p>Without benchmarking your model, the dropdown will display prospective instances that SageMaker predicts could be a good fit based on its own heuristics. For some of the most popular SageMaker JumpStart models, you’ll see an AWS pretested optimal instance type. For other models, you’ll see generally recommended instance types. For example, if I select the Falcon 40B Instruct model in SageMaker JumpStart, I can see the recommended instance types.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/11/21/2023-sm-model-deploy-08.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/11/21/2023-sm-model-deploy-08.png\" alt=\"Model deployment experience in SageMaker Studio\" width=\"1427\" height=\"800\" /></a></p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/11/21/2023-sm-model-deploy-06.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/11/21/2023-sm-model-deploy-06.png\" alt=\"Model deployment experience in SageMaker Studio\" width=\"1430\" height=\"811\" /></a></p> \n<p>However, if I want to optimize the deployment for cost or performance to meet my specific use cases, I could open the <strong>Alternate configurations</strong> panel to view more options based on data from before benchmarking.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/11/21/2023-sm-model-deploy-10.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/11/21/2023-sm-model-deploy-10.png\" alt=\"Model deployment experience in SageMaker Studio\" width=\"1355\" height=\"863\" /></a></p> \n<p>Once deployed, you can test inference or manage auto scaling policies.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/11/21/2023-sm-model-deploy-11.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/11/21/2023-sm-model-deploy-11.png\" alt=\"Model deployment experience in SageMaker Studio\" width=\"1674\" height=\"1600\" /></a></p> \n<p><strong><u>Things to know</u></strong><br /> Here are a couple of important things to know:</p> \n<p><strong>Supported ML models and frameworks – </strong>At launch, the new SageMaker Python SDK tools support model deployment for XGBoost and PyTorch models. You can deploy FMs by specifying the Hugging Face model ID or SageMaker JumpStart model ID using the <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-dlc.html\">SageMaker LMI container</a> or <a href=\"https://huggingface.co/blog/sagemaker-huggingface-llm\">Hugging Face TGI-based container</a>. You can also bring your own container (BYOC) or deploy models using the Triton model server in ONNX format.</p> \n<p><b><u>Now available<br /> </u></b>The new set of tools is available today in all AWS Regions where Amazon SageMaker real-time inference is available. There is no cost to use the new set of tools; you pay only for any underlying SageMaker resources that get created.</p> \n<p><span><strong>Learn more</strong></span></p> \n<ul> \n <li><a href=\"https://aws.amazon.com/sagemaker/deploy/\">Amazon SageMaker Model Deployment</a></li> \n <li><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html\">SageMaker Developer Guide</a></li> \n</ul> \n<p><strong><span>Get started</span><br /> </strong>Explore the new SageMaker model deployment experience in the <a href=\"https://console.aws.amazon.com/sagemaker/home\">AWS Management Console</a> today!</p> \n<p>— <a href=\"https://www.linkedin.com/in/antje-barth/\" target=\"_blank\">Antje</a></p>","author":"Antje Barth","siteTitle":"AWS News Blog","siteHash":"6093e072e4117ec22616e844cb857d03ca62c57a411a8affc77cb5e8b6b15bf6","entryHash":"b720fa38cadedacb57f0da4ea545c86b47975d2c9e232a3f91fb86aab341f801","category":"Tech"}