{"title":"Announcing AWS Parallel Computing Service to run HPC workloads at virtually any scale","link":"https://aws.amazon.com/blogs/aws/announcing-aws-parallel-computing-service-to-run-hpc-workloads-at-virtually-any-scale/","date":1724867639000,"content":"<p>Today we are announcing <a href=\"https://aws.amazon.com/pcs\">AWS Parallel Computing Service (AWS PCS)</a>, a new managed service that helps customers set up and manage <a href=\"https://aws.amazon.com/hpc/\">high performance computing (HPC)</a> clusters so they seamlessly run their simulations at virtually any scale on AWS. Using the <a href=\"https://slurm.schedmd.com/man_index.html\">Slurm</a> scheduler, they can work in a familiar HPC environment, accelerating their time to results instead of worrying about infrastructure.</p> \n<p>In November 2018, we introduced <a href=\"https://aws.amazon.com/about-aws/whats-new/2018/11/AWSParallelCluster/\">AWS ParallelCluster</a>, an AWS supported open-source cluster management tool that helps you to deploy and manage HPC clusters in the AWS Cloud. With AWS ParallelCluster, customers can also quickly build and deploy proof of concept and production HPC compute environments. They can use <a href=\"https://docs.aws.amazon.com/parallelcluster/latest/ug/commands-v3.html\">AWS ParallelCluster Command-Line interface</a>, <a href=\"https://docs.aws.amazon.com/parallelcluster/latest/ug/api-ref-v3.html\">API</a>, <a href=\"https://docs.aws.amazon.com/parallelcluster/latest/ug/pc-py-library-v3.html\">Python library</a>, and the user interface installed from open source packages. They are responsible for updates, which can include tearing down and redeploying clusters. Many customers, though, have asked us for a fully managed AWS service to eliminate operational jobs in building and operating HPC environments.</p> \n<p>AWS PCS simplifies HPC environments managed by AWS and is accessible through the <a href=\"https://console.aws.amazon.com/pcs\">AWS Management Console</a>, AWS SDK, and <a href=\"https://aws.amazon.com/cli\">AWS Command-Line Interface (AWS CLI)</a>. Your system administrators can create managed Slurm clusters that use their compute and storage configurations, identity, and job allocation preferences. AWS PCS uses Slurm, a highly scalable, fault-tolerant job scheduler used across a wide range of HPC customers, for scheduling and orchestrating simulations. End users such as scientists, researchers, and engineers can log in to AWS PCS clusters to run and manage HPC jobs, use interactive software on virtual desktops, and access data. You can bring their workloads to AWS PCS quickly, without significant effort to port code.</p> \n<p>You can use fully managed <a href=\"https://aws.amazon.com/hpc/dcv/\">NICE DCV</a> remote desktops for remote visualization, and access job telemetry or application logs to enable specialists to manage your HPC workflows in one place.</p> \n<p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/08/21/2024-aws-pcs-1-diagram.png\" width=\"1984\" height=\"1114\" /></p> \n<p>AWS PCS is designed for a wide range of traditional and emerging, compute or data-intensive, engineering and scientific workloads across areas such as computational fluid dynamics, weather modeling, finite element analysis, electronic design automation, and reservoir simulations using familiar ways of preparing, executing, and analyzing simulations and computations.</p> \n<p><strong><u>Getting started with AWS Parallel Computing Service</u></strong><br /> To try out AWS PCS, you can use our <a href=\"https://docs.aws.amazon.com/pcs/latest/userguide/getting-started.html\">tutorial for creating a simple cluster</a> in the AWS documentation. First, you create a virtual private cloud (VPC) with an AWS CloudFormation template and shared storage in <a href=\"https://aws.amazon.com/efs\">Amazon Elastic File System (Amazon EFS)</a> within your account for the AWS Region where you will try AWS PCS. To learn more, visit <a href=\"https://docs.aws.amazon.com/pcs/latest/userguide/getting-started_create-vpc.html\">Create a VPC</a> and <a href=\"https://docs.aws.amazon.com/pcs/latest/userguide/getting-started_create-efs.html\">Create shared storage</a> in the AWS documentation.</p> \n<p><strong>1. Create a cluster</strong><br /> In the <a href=\"https://console.aws.amazon.com/pcs/home\">AWS PCS console</a>, choose <strong>Create cluster</strong>, a persistent resource for managing resources and running workloads.</p> \n<p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/08/21/2024-aws-pcs-2-cluster-list.png\" width=\"2258\" height=\"626\" /></p> \n<p>Next, enter your cluster name and choose the controller size of your Slurm scheduler. You can choose <strong>Small</strong> (up to 32 nodes and 256 jobs), <strong>Medium</strong> (up to 512 nodes and 8,192 jobs), or <strong>Large</strong> (up to 2,048 nodes and 16,384 jobs) for the limits of cluster workloads. In the <strong>Networking</strong> section, choose your created VPC, subnet to launch the cluster, and security group applied to your cluster.</p> \n<p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/08/21/2024-aws-pcs-3-create-a-cluster.png\" width=\"1693\" height=\"2540\" /></p> \n<p>Optionally, you can set the Slurm configuration such as an idle time before compute nodes will scale down, a Prolog and Epilog scripts directory on launched compute nodes, and a resource selection algorithm parameter used by Slurm.</p> \n<p>Choose <strong>Create cluster</strong>. It takes some time for the cluster to be provisioned.</p> \n<p><strong>2. Create compute node groups</strong><br /> After creating your cluster, you can create compute node groups, a virtual collection of <a href=\"https://aws.amazon.com/ec2\">Amazon Elastic Compute Cloud (Amazon EC2)</a> instances that AWS PCS uses to provide interactive access to a cluster or run jobs in a cluster. When you define a compute node group, you specify common traits such as EC2 instance types, minimum and maximum instance count, target VPC subnets, <a href=\"https://docs.aws.amazon.com/pcs/latest/userguide/working-with_ami.html\">Amazon Machine Image (AMI)</a>, purchase option, and custom launch configuration. Compute node groups require an instance profile to pass an <a href=\"https://aws.amazon.com/iam\">AWS Identity and Access Management (IAM)</a> role to an EC2 instance and an EC2 launch template that AWS PCS uses to configure EC2 instances it launches. To learn more, visit <a href=\"https://docs.aws.amazon.com/pcs/latest/userguide/getting-started_create-cng_launch-templates.html\">Create a launch template</a> And <a href=\"https://docs.aws.amazon.com/pcs/latest/userguide/getting-started_create-cng_instance-profile.html\">Create an instance profile</a> in the AWS documentation.</p> \n<p>To create a compute node group in the console, go to your cluster and choose the <strong>Compute node groups</strong> tab and the <strong>Create compute node group</strong> button.</p> \n<p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/08/21/2024-aws-pcs-4-compute-node-groups.png\" width=\"1855\" height=\"1445\" /></p> \n<p>You can create two compute node groups: a login node group to be accessed by end users and a job node group to run HPC jobs.</p> \n<p>To create a compute node group running HPC jobs, enter a compute node name and select a previously-created EC2 launch template, IAM instance profile, and subnets to launch compute nodes in your cluster VPC.</p> \n<p>Next, choose your preferred EC2 instance types to use when launching compute nodes and the minimum and maximum instance count for scaling. I chose the <code>hpc6a.48xlarge</code> instance type and scale limit up to eight instances. For a login node, you can choose a smaller instance, such as one <code>c6i.xlarge</code> instance. You can also choose either the <strong>On-demand</strong> or <strong>Spot</strong> EC2 purchase option if the instance type supports. Optionally, you can choose a specific AMI.</p> \n<p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/08/21/2024-aws-pcs-5-create-compute-note-group.png\" width=\"1678\" height=\"3112\" /></p> \n<p>Choose <strong>Create</strong>. It takes some time for the compute node group to be provisioned. To learn more, visit <a href=\"https://docs.aws.amazon.com/pcs/latest/userguide/getting-started_create-cng_workers.html\">Create a compute node group to run jobs</a> and <a href=\"https://docs.aws.amazon.com/pcs/latest/userguide/getting-started_create-cng_login-nodes.html\">Create a compute node group for login nodes</a> in the AWS documentation.</p> \n<p><strong>3. Create and run your HPC jobs</strong><br /> After creating your compute node groups, you submit a job to a queue to run it. The job remains in the queue until AWS PCS schedules it to run on a compute node group, based on available provisioned capacity. Each queue is associated with one or more compute node groups, which provide the necessary EC2 instances to do the processing.</p> \n<p>To create a queue in the console, go to your cluster and choose the <strong>Queues</strong> tab and the <strong>Create queue</strong> button.</p> \n<p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/08/21/2024-aws-pcs-create-a-queue.png\" width=\"1838\" height=\"586\" /></p> \n<p>Enter your queue name and choose your compute node groups assigned to your queue.</p> \n<p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/08/21/2024-aws-pcs-create-a-queue-detail.png\" width=\"1680\" height=\"1324\" /></p> \n<p>Choose <strong>Create</strong> and wait while the queue is being created.</p> \n<p>When the login compute node group is active, you can use <a href=\"https://aws.amazon.com/systems-manager/\">AWS Systems Manager</a> to connect to the EC2 instance it created. Go to the <a href=\"https://console.aws.amazon.com/ec2/\">Amazon EC2 console</a> and choose your EC2 instance of the login compute node group. To learn more, visit <a href=\"https://docs.aws.amazon.com/pcs/latest/userguide/getting-started_create-queue.html\">Create a queue to submit and manage jobs</a> and <a href=\"https://docs.aws.amazon.com/pcs/latest/userguide/getting-started_connect.html\">Connect to your cluster</a> in the AWS documentation.</p> \n<p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/08/21/2024-aws-pcs-login-instance-1.png\" width=\"2170\" height=\"674\" /></p> \n<p>To run a job using Slurm, you prepare a submission script that specifies the job requirements and submit it to a queue with the <code>sbatch</code> command. Typically, this is done from a shared directory so the login and compute nodes have a common space for accessing files.</p> \n<p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/08/21/2024-aws-pcs-run-jobs.png\" width=\"1656\" height=\"1119\" /></p> \n<p>You can also run a message passing interface (MPI) job in AWS PCS using Slurm. To learn more, visit <a href=\"https://docs.aws.amazon.com/pcs/latest/userguide/getting-started_run-job.html\">Run a single node job with Slurm</a> or <a href=\"https://docs.aws.amazon.com/pcs/latest/userguide/getting-started_run-mpi-job.html\">Run a multi-node MPI job with Slurm</a> in the AWS documentation.</p> \n<p>You can connect a fully-managed NICE DCV remote desktop for visualization. To get started, use the CloudFormation template from <a href=\"https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/pcs/cfd_cluster\">HPC Recipes for AWS GitHub repository</a>.</p> \n<p>In this example, I used the <a href=\"https://www.openfoam.com/\">OpenFOAM</a> <a href=\"https://develop.openfoam.com/Development/openfoam/-/tree/master/tutorials/incompressible/simpleFoam/motorBike\">motorBike simulation</a> to calculate the steady flow around a motorcycle and rider. This simulation was run with 288 cores of three hpc6a instances. The output can be visualized in the <a href=\"https://www.paraview.org/\">ParaView</a> session after logging in to the web interface of DCV instance.</p> \n<p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/08/21/2024-aws-pcs-nice-dcv-paraview.png\" width=\"2466\" height=\"1486\" /></p> \n<p>Finally, after you are done HPC jobs with the cluster and node groups that you created, you should delete the resources that you created to avoid unnecessary charges. To learn more, visit <a href=\"https://docs.aws.amazon.com/pcs/latest/userguide/getting-started_delete.html\">Delete your AWS resources</a> in the AWS documentation.</p> \n<p><strong><u>Things to know</u></strong><br /> Here are a couple of things that you should know about this feature:</p> \n<ul> \n <li><strong>Slurm versions</strong> – AWS PCS initially supports Slurm 23.11 and oﬀers mechanisms designed to enable customers to upgrade their Slurm major versions once new versions are added. Additionally, AWS PCS is designed to automatically update the Slurm controller with patch versions. To learn more, visit <a href=\"https://docs.aws.amazon.com/pcs/latest/userguide/slurm-versions.html\">Slurm versions</a> in the AWS documentation.</li> \n <li><strong>Capacity Reservations</strong> – You can reserve EC2 capacity in a specific Availability Zone and for a specific duration using On-Demand Capacity Reservations to make sure that you have the necessary compute capacity available when you need it. To learn more, visit <a href=\"https://docs.aws.amazon.com/pcs/latest/userguide/working-with_capacity-reservations.html\">Capacity Reservations</a> in the AWS documentation.</li> \n <li><strong>Network file systems</strong> – You can attach network storage volumes where data and files can be written and accessed, including <a href=\"https://aws.amazon.com/fsx/netapp-ontap/\">Amazon FSx for NetApp ONTAP</a>, <a href=\"https://aws.amazon.com/fsx/openzfs/\">Amazon FSx for OpenZFS</a>, and <a href=\"https://aws.amazon.com/filecache/\">Amazon File Cache</a> as well as <a href=\"https://aws.amazon.com/efs/\">Amazon EFS</a> and <a href=\"https://aws.amazon.com/fsx/lustre/\">Amazon FSx for Lustre</a>. You can also use self-managed volumes, such as NFS servers. To learn more, visit <a href=\"https://docs.aws.amazon.com/pcs/latest/userguide/working-with_file-systems.html\">Network file systems</a> in the AWS documentation.</li> \n</ul> \n<p><strong><u>Now available</u></strong><br /> <a href=\"https://aws.amazon.com/pcs\">AWS Parallel Computing Service</a> is now available in the US East (N. Virginia), AWS US East (Ohio), US West (Oregon), Asia Pacific (Singapore), Asia Pacific (Sydney), Asia Pacific (Tokyo), Europe (Frankfurt), Europe (Ireland), Europe (Stockholm) Regions.</p> \n<p>AWS PCS launches all resources in your AWS account. You will be billed appropriately for those resources. For more information, see the <a href=\"https://aws.amazon.com/pcs/pricing/\">AWS PCS Pricing page</a>.</p> \n<p>Give it a try and send feedback to <a href=\"https://repost.aws/\">AWS re:Post</a> or through your usual AWS Support contacts.</p> \n<p>— <a href=\"https://twitter.com/channyun\">Channy</a></p> \n<p><em>P.S. Special thanks to <a href=\"https://www.linkedin.com/in/mattdotvaughn/\">Matthew Vaughn</a>, a principal developer advocate at AWS for his contribution in creating a HPC testing environment.</em></p>","author":"Channy Yun (윤석찬)","siteTitle":"AWS News Blog","siteHash":"6093e072e4117ec22616e844cb857d03ca62c57a411a8affc77cb5e8b6b15bf6","entryHash":"8ed16f4a2e8cc99df16266f2dfb5871fb09e279d8b4b3642249a926b8018547a","category":"Tech"}