{"title":"Hugging Face เปิดซอฟต์แวร์ HUGS ใช้รันโมเดลบนเครื่องเซิร์ฟเวอร์หลากหลาย ให้คนอื่นเช่าใช้งาน","link":"https://www.blognone.com/node/142743","date":1729759763000,"content":"<div><div><div><p>คนในแวดวง AI คงรู้จักบริษัท Hugging Face ในฐานะศูนย์รวมคลังโมเดลขนาดใหญ่ (<a href=\"https://www.blognone.com/node/142238\">มีโมเดลเกิน 1 ล้านตัวแล้ว</a>) วิธีการหารายได้ของ Hugging Face ที่ผ่านมาคือการให้เช่าเซิร์ฟเวอร์สำหรับเทรน-ปรับแต่ง-รันโมเดลเหล่านี้ได้ทันที</p>\n<p>คราวนี้ Hugging Face ขยายโมเดลธุรกิจของตัวเอง โดยนำซอฟต์แวร์ที่ใช้รันโมเดลข้างต้น เปิดให้ลูกค้าเช่าใช้งานซอฟต์แวร์นี้บนระบบไอทีของตัวเองได้ด้วย ใช้ชื่อว่า <strong>Hugging Face Generative AI Services</strong> หรือตัวย่อ <strong>HUGS</strong></p>\n<p>HUGS เป็นซอฟต์แวร์ microservice ที่ใช้รันโมเดล AI บนฮาร์ดแวร์เซิร์ฟเวอร์ที่หลากหลาย ทั้งบนจีพียูหลายค่ายและบนตัวเร่งความเร็ว AI เฉพาะทาง แถมยังปรับแต่งประสิทธิภาพของโมเดลบนฮาร์ดแวร์เหล่านี้มาให้เรียบร้อย (เพราะ Hugging Face ต้องให้บริการรันโมเดลจำนวนมากอยู่แล้ว) ช่วยประหยัดแรงของลูกค้าฝั่งองค์กรที่ต้องการรันโมเดลเองไปได้มาก</p>\n<p>ตัวอย่างโมเดลที่รันบน HUGS เน้นไปที่โมเดลแบบเปิด เช่น Llama 3.1, Mixtral, Gemma 2, Qwen 2.5 เป็นต้น</p>\n<p>Hugging Face บอกว่า HUGS เหมาะสำหรับองค์กรที่ต้องการใช้งานโมเดล AI ในระบบเซิร์ฟเวอร์ของตัวเอง (ไม่ว่าจะอยู่บนคลาวด์หรือไม่) และไม่มีกำลังคน-ความเชี่ยวชาญมากพอในการปรับแต่งประสิทธิภาพของการรัน ตัวซอฟต์แวร์ยังรันอยู่บนซอฟต์แวร์ที่เป็นมาตรฐานอย่าง Kubernetes และเรียกใช้ API ที่เข้ากันได้กับ OpenAI API เพิ่มความสะดวกในการย้ายงานโมเดลมารันบน HUGS</p>\n<p>ตอนนี้ HUGS เปิดให้ใช้งานแล้วบนคลาวด์รายใหญ่ๆ อย่าง AWS, Google Cloud, Microsoft Azure, DigitalOcean ส่วนการใช้งานกับเครื่องในองค์กรสามารถติดต่อไปยังฝ่ายขายของ Hugging Face ได้โดยตรง วิธีคิด</p>\n<p>คู่แข่งที่ใกล้เคียงที่สุดของ HUGS น่าจะเป็น <a href=\"https://www.blognone.com/node/138769\">NVIDIA NIM</a> ที่เป็นซอฟต์แวร์คั่นกลางระหว่างตัวโมเดลกับฮาร์ดแวร์ที่ใช้ประมวลผล เพียงแต่ NIM รันได้บนแพลตฟอร์ม NVIDIA CUDA เท่านั้น ในขณะที่ HUGS รองรับจีพียู AMD ด้วย และประกาศว่าจะรองรับตัวเร่งประสิทธิภาพ AI อื่นๆ เช่น <a href=\"https://www.blognone.com/node/119609\">AWS Inferentia</a> และ Google TPU ในอนาคต</p>\n<p>ที่มา - <a href=\"https://huggingface.co/blog/hugs\">HUGS</a></p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/6525ae146cfe4f5660b278e6c0ce3975.jpg\" /></p>\n</div></div></div><div><div>Topics: </div><div><div><a href=\"/topics/hugging-face\">Hugging Face</a></div><div><a href=\"/topics/artificial-intelligence\">Artificial Intelligence</a></div><div><a href=\"/topics/cloud-computing\">Cloud Computing</a></div><div><a href=\"/topics/llm\">LLM</a></div></div></div>","author":"mk","siteTitle":"Blognone","siteHash":"ededadcf18490b3937e7dd89ebe8c00dc129addbdf1ebe4aff1f458146693da0","entryHash":"5012f60065e7b0bcd0b96f96aaa3ba9709485b5a7df0d9146068a98d8a5b8bdd","category":"Thai"}