{"title":"Tech giants form AI group to counter Nvidia with new interconnect standard","link":"https://arstechnica.com/?p=2027773","date":1717101779000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/05/datacenter_image-800x450.jpg\" alt=\"Abstract image of data center with flowchart.\" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2024/05/datacenter_image.jpg\">Enlarge</a> (credit: <a href=\"https://www.gettyimages.com/detail/photo/data-center-and-flowchart-royalty-free-image/1725517301\">Getty Images</a>)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>On Thursday, several major tech companies, including Google, Intel, Microsoft, Meta, AMD, Hewlett Packard Enterprise, Cisco, and Broadcom, announced the formation of the Ultra Accelerator Link (UALink) Promoter Group to develop a new interconnect standard for AI accelerator chips in data centers. The group aims to create an alternative to Nvidia's proprietary <a href=\"https://www.nvidia.com/en-us/design-visualization/nvlink-bridges/\">NVLink</a> interconnect technology, which links together multiple servers that power today's AI applications like <a href=\"https://arstechnica.com/information-technology/2024/05/chatgpt-4o-lets-you-have-real-time-audio-video-conversations-with-emotional-chatbot/\">ChatGPT</a>.</p>\n\n<p>The beating heart of AI these days lies in <a href=\"https://arstechnica.com/information-technology/2024/03/nvidia-unveils-blackwell-b200-the-worlds-most-powerful-chip-designed-for-ai/\">GPUs</a>, which can perform massive numbers of matrix multiplications—necessary for running neural network architecture—in parallel. But one GPU often isn't enough for complex AI systems. NVLink can connect multiple AI accelerator chips within a server or across multiple servers. These interconnects enable faster data transfer and communication between the accelerators, allowing them to work together more efficiently on complex tasks like training large AI models.</p>\n<p>This linkage is a key part of any modern AI data center system, and whoever controls the link standard can effectively dictate which hardware the tech companies will use. Along those lines, the UALink group seeks to establish an open standard that allows multiple companies to contribute and develop AI hardware advancements instead of being locked into Nvidia's proprietary ecosystem. This approach is similar to other open standards, such as <a href=\"https://en.wikipedia.org/wiki/Compute_Express_Link\">Compute Express Link</a> (CXL)—created by Intel in 2019—which provides high-speed, high-capacity connections between CPUs and devices or memory in data centers.</p></div><p><a href=\"https://arstechnica.com/?p=2027773#p3\">Read 5 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=2027773&amp;comments=1\">Comments</a></p>","author":"Benj Edwards","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"ba72dfbe1fb0f2038c19e4d5fe111440e88c929aae0f02ec09e5577fb06c3011","category":"Tech"}