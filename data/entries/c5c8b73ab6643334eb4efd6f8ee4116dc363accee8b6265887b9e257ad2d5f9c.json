{"title":"I abandoned OpenLiteSpeed and went back to good ol’ Nginx","link":"https://arstechnica.com/?p=1998514","date":1706282944000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/01/GettyImages-200414013-001-800x471.jpg\" alt=\"Ish is on fire, yo.\" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2024/01/GettyImages-200414013-001.jpg\">Enlarge</a> <span>/</span> Ish is on fire, yo. (credit: <a href=\"https://www.gettyimages.com/detail/photo/computer-terminal-on-fire-on-table-royalty-free-image/200414013-001\">Tim Macpherson / Getty Images</a>)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>Since 2017, in what spare time I have (ha!), I help my colleague Eric Berger host his Houston-area weather forecasting site, <a href=\"https://spacecityweather.com/\">Space City Weather</a>. It’s an interesting hosting challenge—on a typical day, SCW does maybe 20,000–30,000 page views to 10,000–15,000 unique visitors, which is a relatively easy load to handle with minimal work. But when severe weather events happen—especially in the summer, when hurricanes lurk in the Gulf of Mexico—the site’s traffic can spike to more than a million page views in 12 hours. That level of traffic requires a bit more prep to handle.</p>\n<div><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2024/01/Screenshot-2024-01-24-at-9.02.05%E2%80%AFAM.jpg\"><img alt=\"Hey, it's &lt;a href=&quot;https://spacecityweather.com&quot;&gt;Space City Weather&lt;/a&gt;!\" src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/01/Screenshot-2024-01-24-at-9.02.05%E2%80%AFAM.jpg\" srcset=\"https://cdn.arstechnica.net/wp-content/uploads/2024/01/Screenshot-2024-01-24-at-9.02.05 AM.jpg 2x\" /></a><p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2024/01/Screenshot-2024-01-24-at-9.02.05%E2%80%AFAM.jpg\">Hey, it's </a><a href=\"https://spacecityweather.com\">Space City Weather</a>! (credit: Lee Hutchinson)</p></div>\n<p>For a very long time, I ran SCW on a backend stack made up of <a href=\"https://www.haproxy.org/\">HAProxy</a> for SSL termination, <a href=\"https://varnish-cache.org/\">Varnish Cache</a> for on-box caching, and <a href=\"https://nginx.org/\">Nginx</a> for the actual web server application—all fronted by <a href=\"https://www.cloudflare.com/\">Cloudflare</a> to absorb the majority of the load. (<a href=\"https://arstechnica.com/information-technology/2017/09/how-to-hurricane-proof-a-web-server/\">I wrote about this setup at length</a> on Ars a few years ago for folks who want some more in-depth details.) This stack was fully battle-tested and ready to devour whatever traffic we threw at it, but it was also annoyingly complex, with multiple cache layers to contend with, and that complexity made troubleshooting issues more difficult than I would have liked.</p>\n<p>So during some winter downtime two years ago, I took the opportunity to jettison some complexity and reduce the hosting stack down to a single monolithic web server application: <a href=\"https://openlitespeed.org/\">OpenLiteSpeed</a>.</p></div><p><a href=\"https://arstechnica.com/?p=1998514#p3\">Read 32 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=1998514&amp;comments=1\">Comments</a></p>","author":"Lee Hutchinson","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"c5c8b73ab6643334eb4efd6f8ee4116dc363accee8b6265887b9e257ad2d5f9c","category":"Tech"}