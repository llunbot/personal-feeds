{"title":"NVIDIA เปิดตัว Chat with RTX แช็ทบ็อท AI แบบโลคัล รันบน GeForce RTX ในเครื่องพีซี","link":"https://www.blognone.com/node/138187","date":1707866287000,"content":"<div><div><div><p>NVIDIA เปิดตัว <strong>Chat with RTX</strong> เป็นไคลเอนต์สำหรับรันแช็ทบ็อท Generative AI บนเครื่องพีซีของเราเอง ไม่ต้องพึ่งพาเซิร์ฟเวอร์ภายนอก</p>\n<p>Chat with RTX เป็นแพลตฟอร์มที่ใช้รันโมเดลภาษาโอเพนซอร์ส (ตอนนี้รองรับ Llama 2 และ Mistral สองตัว) มารันบน Tensor Core ของจีพียู GeForce RTX ซีรีส์ 30 ขึ้นไป (เบื้องหลังของมันคือ <a href=\"https://blogs.nvidia.com/blog/ignite-rtx-ai-tensorrt-llm-chat-api/\">TensorRT-LLM</a> และ <a href=\"https://github.com/NVIDIA/trt-llm-rag-windows\">RAG on Windows</a>) ตอนนี้ยังรองรับเฉพาะบนพีซีวินโดวส์เท่านั้น</p>\n<p>จุดเด่นของ Chat with RTX คือทั้งตัวโมเดล LLM และตัวข้อมูลที่ให้โมเดลอ่านจะอยู่ในเครื่องพีซีเท่านั้น เช่น อ่านจากไฟล์ txt, pdf, doc/docx, xml ในเครื่องเฉพาะโฟลเดอร์ที่กำหนด ป้องกันข้อมูลรั่วไหล แต่ในอีกทางก็ยังสามารถอ้างอิงข้อมูลบนอินเทอร์เน็ต (เช่น ให้ AI ชมวิดีโอบน YouTube ที่ระบุ) ได้เช่นกัน</p>\n<p>NVIDIA บอกว่า Chat with RTX มีสถานะเป็น tech demo และยังไม่ระบุว่าจะผลักดันต่อในระยะยาวหรือไม่ แต่อย่างน้อยก็เป็นตัวอย่างให้เห็นว่า แช็ทบ็อท AI ที่รันแบบโลคัลนั้นสามารถทำได้แล้ว</p>\n<p>ที่มา - <a href=\"https://blogs.nvidia.com/blog/chat-with-rtx-available-now/\">NVIDIA</a></p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/f907cefb07a5eeb6647326230a6030b2.jpeg\" /></p>\n</div></div></div><div><div>Topics: </div><div><div><a href=\"/topics/nvidia\">NVIDIA</a></div><div><a href=\"/topics/chat-bot\">Chat Bot</a></div><div><a href=\"/topics/artificial-intelligence\">Artificial Intelligence</a></div><div><a href=\"/topics/geforce\">GeForce</a></div></div></div>","author":"mk","siteTitle":"Blognone","siteHash":"ededadcf18490b3937e7dd89ebe8c00dc129addbdf1ebe4aff1f458146693da0","entryHash":"469b12aa1f6532493603be0c8a2d8f24cf088a1842f8b6e74f1385292446b290","category":"Thai"}