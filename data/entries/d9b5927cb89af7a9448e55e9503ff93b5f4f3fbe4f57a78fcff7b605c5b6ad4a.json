{"title":"Researchers figure out how to make AI misbehave, serve up prohibited content","link":"https://arstechnica.com/?p=1958270","date":1690982544000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2023/08/chatbot-list-800x533.jpg\" alt=\"pixelated word balloon\" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2023/08/chatbot-list.jpg\">Enlarge</a> (credit: MirageC/Getty Images)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>ChatGPT and its artificially intelligent siblings have been tweaked over and over to prevent troublemakers from getting them to spit out undesirable messages such as hate speech, personal information, or step-by-step instructions for building an improvised bomb. But researchers at Carnegie Mellon University last week <a href=\"https://llm-attacks.org/\">showed</a> that adding a simple incantation to a prompt—a string text that might look like gobbledygook to you or me but which carries subtle significance to an AI model trained on huge quantities of web data—can defy all of these defenses in several popular chatbots at once.</p>\n<p>The work suggests that the propensity for the cleverest AI chatbots to go off the rails isn’t just a quirk that can be papered over with a few simple rules. Instead, it represents a more fundamental weakness that will complicate efforts to deploy the most advanced AI.</p>\n<p></p></div><p><a href=\"https://arstechnica.com/?p=1958270#p3\">Read 19 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=1958270&amp;comments=1\">Comments</a></p>","author":"WIRED","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"d9b5927cb89af7a9448e55e9503ff93b5f4f3fbe4f57a78fcff7b605c5b6ad4a","category":"Tech"}