{"title":"Apple Study Reveals Critical Flaws in AI's Logical Reasoning Abilities","link":"https://www.macrumors.com/2024/10/14/apple-study-reveals-flaws-in-ai-reasoning/","date":1728916280000,"content":"Apple's AI research team has uncovered significant weaknesses in the reasoning abilities of large language models, according to a newly published study.\r<br />\n\r<br />\n<img src=\"https://images.macrumors.com/article-new/2024/04/Apple-Silicon-AI-Optimized-Feature-Siri-1.jpg\" width=\"2500\" height=\"1406\" />\r<br />\nThe study, <a href=\"https://arxiv.org/pdf/2410.05229\">published on arXiv</a>, outlines Apple's evaluation of a range of leading language models, including those from OpenAI, Meta, and other prominent developers, to determine how well these models could handle mathematical reasoning tasks. The findings reveal that even slight changes in the phrasing of questions can cause major discrepancies in model performance that can undermine their reliability in scenarios requiring logical consistency.\r<br />\n\r<br />\nApple draws attention to a persistent problem in language models: their reliance on pattern matching rather than genuine logical reasoning. In several tests, the researchers demonstrated that adding irrelevant information to a question—details that should not affect the mathematical outcome—can lead to vastly different answers from the models.\r<br />\n\r<br />\nOne example given in the paper involves a simple math problem asking how many kiwis a person collected over several days. When irrelevant details about the size of some kiwis were introduced, models such as OpenAI's o1 and Meta's Llama incorrectly adjusted the final total, despite the extra information having no bearing on the solution.\r<br />\n\r<br />\n<blockquote>We found no evidence of formal reasoning in language models. Their behavior is better explained by sophisticated pattern matching—so fragile, in fact, that changing names can alter results by ~10%.</blockquote>\r<br />\n\r<br />\nThis fragility in reasoning prompted the researchers to conclude that the models do not use real logic to solve problems but instead rely on sophisticated pattern recognition learned during training. They found that \"simply changing names can alter results,\" a potentially troubling sign for the future of AI applications that require consistent, accurate reasoning in real-world contexts.\r<br />\n\r<br />\nAccording to the study, all models tested, from smaller open-source versions like Llama to proprietary models like OpenAI's GPT-4o, showed significant performance degradation when faced with seemingly inconsequential variations in the input data. Apple suggests that AI might need to combine neural networks with traditional, symbol-based reasoning called <a href=\"https://en.wikipedia.org/wiki/Neuro-symbolic_AI\">neurosymbolic AI</a> to obtain more accurate decision-making and problem-solving abilities.<div>Tags: <a href=\"https://www.macrumors.com/guide/apple-research/\">Apple Research</a>, <a href=\"https://www.macrumors.com/guide/artificial-intelligence/\">Artificial Intelligence</a></div><br />This article, \"<a href=\"https://www.macrumors.com/2024/10/14/apple-study-reveals-flaws-in-ai-reasoning/\">Apple Study Reveals Critical Flaws in AI's Logical Reasoning Abilities</a>\" first appeared on <a href=\"https://www.macrumors.com\">MacRumors.com</a><br /><br /><a href=\"https://forums.macrumors.com/threads/apple-study-reveals-critical-flaws-in-ais-logical-reasoning-abilities.2440029/\">Discuss this article</a> in our forums<br /><br />","author":"Hartley Charlton","siteTitle":"MacRumors: Mac News and Rumors - All Stories","siteHash":"4c0f1b1ecc2ed084c9f5be50f1058e33a55cdf9b904dadc33a2071fc2d63e8c1","entryHash":"dbfeb4b8d1915b95eb62ff5ab82ea2757964fc4c4f51c0344cd4df1cda21f99c","category":"Apple"}