{"title":"Washington Post Analysis of the Content That Trained Google’s ‘C4’ LLM Data Set","link":"https://www.washingtonpost.com/technology/interactive/2023/ai-chatbot-learning/","date":1681945760000,"content":"\n<p>The Washington Post:</p>\n\n<blockquote>\n  <p>Tech companies have grown secretive about what they feed the AI.\nSo The Washington Post set out to analyze one of these data sets\nto fully reveal the types of proprietary, personal, and often\noffensive websites that go into an AI’s training data.</p>\n\n<p>To look inside this black box, we analyzed <a href=\"https://www.semanticscholar.org/paper/Documenting-the-English-Colossal-Clean-Crawled-Dodge-Sap/40c3327a6ddb0603b6892344509c7f428ab43d81\">Google’s C4 data\nset</a>, a massive snapshot of the contents of 15 million\nwebsites that have been used to instruct some high-profile\nEnglish-language AIs, called large language models, including\nGoogle’s T5 and Facebook’s LLaMA. (OpenAI does not disclose what\ndatasets it uses to train the models backing its popular chatbot,\nChatGPT).</p>\n</blockquote>\n\n<p>Scroll the bottom and they have a tool that lets you search for the ranking of a particular website. <a href=\"https://daringfireball.net/misc/2023/04/wapo-c4-dataset-daring.png\">Daring Fireball is #24,293</a>; Kottke.org is right behind at #25,310; Six Colors is #38,783; Stratechery is #57,283. MacRumors is way up at #761. <a href=\"https://daringfireball.net/misc/2023/04/wapo-c4-dataset-forums.png\">MacRumors’s forums are at #45</a>, and Apple Insider’s forums are at #211. The New York Times is #4, and The Washington Post itself #11.</p>\n\n<div>\n<a href=\"https://daringfireball.net/linked/2023/04/19/wapo-google-c4\"> ★ </a>\n</div>\n\n\t","author":"John Gruber","siteTitle":"Daring Fireball","siteHash":"fc569638025dadf22a867470f8215f38855cf50e975782a6c989909474292a36","entryHash":"220820dc7a6c5fc5397300a6127ff816a00a9af5fdabc0268764f1ace18082d9","category":"Tech"}