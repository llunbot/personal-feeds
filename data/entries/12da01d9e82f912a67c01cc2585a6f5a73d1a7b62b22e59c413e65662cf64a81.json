{"title":"Embedding in AI","link":"https://markpeak.net/embedding-in-ai/","date":1680790709000,"content":"<p>อ่านเปเปอร์เรื่อง AI แล้วพบคำศัพท์ว่า Embedding เลยไปอ่านข้อมูลเพิ่ม เจอ<a href=\"https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture\">คอร์สของ Google Machine Learning</a> สรุปไว้ค่อนข้างดี (มีคลิปด้วย)</p>\n<p>พื้นฐานของการทำ prediction นั้นเป็นการคำนวณทางคณิตศาสตร์ โดยหาแพทเทิร์นความสัมพันธ์ของข้อมูลชนิกเวกเตอร์ ซึ่งการหาความสัมพันธ์ให้ได้ดีและแม่นยำ เวกเตอร์ต้องมีการเกาะกลุ่มกัน (dense vector)</p>\n<p>แต่ข้อมูลที่เรานำเข้ามาทำนาย มักเป็นเวกเตอร์ที่กระจัดกระจาย (sparse vector) ซึ่งหาความสัมพันธ์ได้ยาก และหากข้อมูลมีขนาดใหญ่ ยิ่งสิ้นเปลืองพลังประมวลผล ในวงการ machine learning จึงมีเทคนิค “ตบ” ข้อมูลให้จำกัดวงแคบลง เทคนิคอันนี้ล่ะเรียกว่า embedding</p>\n<p>ฟังแล้วเข้าใจยาก ดูตัวอย่างดีกว่า</p>\n<p>หากเราเป็น Netflix และต้องการพยากรณ์หนังที่ผู้ใช้ของเราชอบ เราจะมีข้อมูลขนาดใหญ่มากคือ ตารางที่ประกอบด้วยผู้ใช้แต่ละคน x หนังทั้งหมดที่ผู้ใช้แต่ละคนดู หากเรามีผู้ใช้ 1,000,000 คน x หนัง 500,000 เรื่อง เวลาจับคูณความเป็นไปได้แล้ว ตารางมันจะขนาดใหญ่มหาศาล</p>\n<p><img src=\"https://markpeak.net/wp-content/uploads/2023/04/InputRepresentationWithValues.png\" alt width=\"506\" height=\"320\" srcset=\"https://markpeak.net/wp-content/uploads/2023/04/InputRepresentationWithValues.png 506w, https://markpeak.net/wp-content/uploads/2023/04/InputRepresentationWithValues-300x190.png 300w\" /></p>\n<p>การเอาข้อมูลดิบของหนัง 500,000 เรื่องมาคำนวณไปทีละชุด หาความสัมพันธ์ไปเรื่อยๆ มันเยอะเกินไป</p>\n<p>เรามาหาความสัมพันธ์ของหนังแต่ละเรื่องกันเองก่อนว่ามันมีความคล้ายกันหรือเปล่า ตรงนี้คือการสร้างมิติความสัมพันธ์ (dimension space) เพิ่มเข้ามาอีกก้อน ซึ่งจำนวนมิติจะเป็น 1, 2, 3 ก็ได้แล้วแต่กรณี แต่เพื่อความง่ายก็ควรใช้มิติน้อยๆ (low-dimension)</p>\n<p>ตัวอย่างความสัมพันธ์ของหนังแบบ 1 มิติ หนังที่อยู่ใกล้กันแปลว่าคล้ายกัน</p>\n<p><img loading=\"lazy\" src=\"https://markpeak.net/wp-content/uploads/2023/04/one-dimension.png\" alt width=\"1026\" height=\"154\" srcset=\"https://markpeak.net/wp-content/uploads/2023/04/one-dimension.png 1026w, https://markpeak.net/wp-content/uploads/2023/04/one-dimension-300x45.png 300w, https://markpeak.net/wp-content/uploads/2023/04/one-dimension-1024x154.png 1024w, https://markpeak.net/wp-content/uploads/2023/04/one-dimension-768x115.png 768w, https://markpeak.net/wp-content/uploads/2023/04/one-dimension-700x105.png 700w\" /></p>\n<p>ตัวอย่างความสัมพันธ์ของหนังแบบ 2 มิติ เพิ่มแกนของอายุผู้ชม (เด็ก-ผู้ใหญ่) และสไตล์ของหนัง (แมส-อาร์ท) เข้ามา</p>\n<p><img loading=\"lazy\" src=\"https://markpeak.net/wp-content/uploads/2023/04/two-dimension.png\" alt width=\"985\" height=\"655\" srcset=\"https://markpeak.net/wp-content/uploads/2023/04/two-dimension.png 985w, https://markpeak.net/wp-content/uploads/2023/04/two-dimension-300x199.png 300w, https://markpeak.net/wp-content/uploads/2023/04/two-dimension-768x511.png 768w, https://markpeak.net/wp-content/uploads/2023/04/two-dimension-700x465.png 700w\" /></p>\n<p>การสร้างมิติความสัมพันธ์แบบนี้ (ที่เรียกว่า embedding space) จำเป็นต้องเพิ่มข้อมูลภายนอก (นอกจากอินพุตหลักของ prediction) ซึ่งจะใช้วิธีใดก็แล้วแต่ อย่างเคสเรื่องรูปแบบของหนัง เราอาจใช้เรตอายุของหนัง + การประเมินเองว่าเป็นหนังแมส/อาร์ท แล้วพล็อตจุดในกราฟ 2D เองได้ หรือถ้าเป็นข้อมูลรูปแบบที่ใช้บ่อยๆ ก็มีตัวช่วย เช่น <a href=\"https://venturebeat.com/ai/what-is-an-embedding-for-ai/\">Word2Vec</a></p>\n<p>สิ่งที่เราต้องการจากกราฟข้างบนคือ ข้อมูลตัวเลขว่าหนังเรื่องนั้นมีพิกัดเท่าไร (เช่น Shrek คือ -1.0, 0.95) เพื่อนำตัวเลขนี้ไปคำนวณตอนเทรนโมเดล มันจะกลายเป็นตารางข้อมูลที่เรียกว่า embedding tables</p>\n<p>ในสถาปัตยกรรม deep learning network (DNN) ตามภาพด้านล่าง เราจะเห็นการเพิ่มชั้นสีเขียว (embedding) เข้ามาคั่นกลางระหว่างรายการหนังทั้งหมด (สีฟ้า) เพื่อ “คัดกรอง” หาความสัมพันธ์ของหนังก่อนไป</p>\n<p>การที่ชั้นของ embedding มีจำนวนมิติไม่มากนัก (1-3 มิติ) ความเป็นไปได้ของการคำนวณจึงลดลงมากในโมเดลชั้นถัดไป</p>\n<p><img loading=\"lazy\" src=\"https://markpeak.net/wp-content/uploads/2023/04/dnn.png\" alt width=\"1076\" height=\"504\" srcset=\"https://markpeak.net/wp-content/uploads/2023/04/dnn.png 1076w, https://markpeak.net/wp-content/uploads/2023/04/dnn-300x141.png 300w, https://markpeak.net/wp-content/uploads/2023/04/dnn-1024x480.png 1024w, https://markpeak.net/wp-content/uploads/2023/04/dnn-768x360.png 768w, https://markpeak.net/wp-content/uploads/2023/04/dnn-700x328.png 700w\" /></p>\n<p>เทคนิค Embedding ถือเป็นมาตรฐานสำหรับโมเดลแบบ Deep Learning Recommendation Models (DLRM) ที่ใช้พยากรณ์เรื่องต่างๆ ไปแล้ว โดยมันจะอยู่ในชั้นแรก (first layer) ที่ใช้กรองข้อมูลในโมเดลก่อน</p>\n<p>ตัวตาราง embedding tables มีจำนวนมิติน้อยแน่นอน แต่ในแต่ละมิติ อาจมีขนาดใหญ่มากๆ ได้ การค้นหาข้อมูลในตาราง (lookup) อาจต้องโหลดตารางขนาดใหญ่เข้ามาในหน่วยความจำ ซึ่งในทางปฏิบัติก็มีเทคนิคจำพวก sharding แบ่งส่วนตารางเพื่อลดขนาดของปัญหาลงได้เช่นกัน ซึ่งกูเกิลก็พัฒนาฮาร์ดแวร์ที่เรียกว่า SparseCore เข้ามาช่วยในเรื่องนี้ (<a href=\"https://arxiv.org/ftp/arxiv/papers/2304/2304.01433.pdf\">เปเปอร์</a>)</p>The post <a href=\"https://markpeak.net/embedding-in-ai/\">Embedding in AI</a> first appeared on <a href=\"https://markpeak.net\">markpeak.net</a>.","author":"Isriya Paireepairit","siteTitle":"markpeak.net","siteHash":"174209a41ef21fd794de2993285c799df6ec31048fd82206fb5c8fe38898acfe","entryHash":"12da01d9e82f912a67c01cc2585a6f5a73d1a7b62b22e59c413e65662cf64a81","category":"Thai"}