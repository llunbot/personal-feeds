{"title":"Researchers astonished by tool’s apparent success at revealing AI’s hidden motives","link":"https://arstechnica.com/ai/2025/03/researchers-astonished-by-tools-apparent-success-at-revealing-ais-hidden-motives/","date":1741982621000,"content":"<p>In a <a href=\"https://www.anthropic.com/research/auditing-hidden-objectives\">new paper</a> published Thursday titled \"<a href=\"https://assets.anthropic.com/m/317564659027fb33/original/Auditing-Language-Models-for-Hidden-Objectives.pdf\">Auditing language models for hidden objectives</a>,\" Anthropic researchers described how models trained to deliberately conceal certain motives from evaluators could still inadvertently reveal secrets, thanks to their ability to adopt different contextual roles or \"personas.\" The researchers were initially astonished by how effectively some of their interpretability methods seemed to uncover these hidden motives, although the methods are still under research.</p>\n<p>While the research involved models trained specifically to conceal motives from automated software evaluators called <a href=\"https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback\">reward models</a> (RMs), the broader purpose of studying hidden objectives is to prevent future scenarios where powerful AI systems might intentionally deceive or manipulate human users.</p>\n<p>While training a language model using reinforcement learning from human feedback (RLHF), reward models are typically tuned to score AI responses according to how well they align with human preferences. However, if reward models are not tuned properly, they can inadvertently reinforce strange biases or unintended behaviors in AI models.</p><p><a href=\"https://arstechnica.com/ai/2025/03/researchers-astonished-by-tools-apparent-success-at-revealing-ais-hidden-motives/\">Read full article</a></p>\n<p><a href=\"https://arstechnica.com/ai/2025/03/researchers-astonished-by-tools-apparent-success-at-revealing-ais-hidden-motives/#comments\">Comments</a></p>","author":"Benj Edwards","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"a520613c0f63415e29a80bf05d9579defc134ae0360172fdd98cda140e19fca2","category":"Tech"}