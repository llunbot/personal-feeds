{"title":"A jargon-free explanation of how AI large language models work","link":"https://arstechnica.com/?p=1956916","date":1690801212000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2023/07/LLM-cat-vector-space-1-800x450.jpg\" alt=\"An illustration of words connected by lines.\" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2023/07/LLM-cat-vector-space-1.jpg\">Enlarge</a> (credit: Aurich Lawson / Ars Technica.)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>When ChatGPT was introduced last fall, it sent shockwaves through the technology industry and the larger world. Machine learning researchers had been experimenting with large language models (LLMs) for a few years by that point, but the general public had not been paying close attention and didn’t realize how powerful they had become.</p>\n<p>Today, almost everyone has heard about LLMs, and tens of millions of people have tried them out. But not very many people understand how they work.</p>\n<p>If you know anything about this subject, you’ve probably heard that LLMs are trained to “predict the next word” and that they require huge amounts of text to do this. But that tends to be where the explanation stops. The details of how they predict the next word is often treated as a deep mystery.</p></div><p><a href=\"https://arstechnica.com/?p=1956916#p3\">Read 107 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=1956916&amp;comments=1\">Comments</a></p>","author":"Timothy B. Lee","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"ea449a6ddac6b33222e5f6186a7a3b2e24728392610db9a5577e76f304a381c7","category":"Tech"}