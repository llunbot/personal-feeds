{"title":"AWS announces Pixtral Large 25.02 model in Amazon Bedrock serverless","link":"https://aws.amazon.com/blogs/aws/aws-announces-pixtral-large-25-02-model-in-amazon-bedrock-serverless/","date":1744147755000,"content":"<p>Today, we announce that the <a href=\"https://aws.amazon.com/bedrock/mistral/?trk=4b29643c-e00f-4ab6-ab9c-b1fb47aa1708&amp;sc_channel=blog\">Pixtral Large 25.02 model</a> is now available in <a href=\"https://aws.amazon.com/bedrock/\">Amazon Bedrock</a> as a fully managed, serverless offering. AWS is the first major cloud provider to deliver Pixtral Large as a fully managed, serverless model.</p> \n<p>Working with <a href=\"https://aws.amazon.com/what-is/large-language-model/?trk=4b29643c-e00f-4ab6-ab9c-b1fb47aa1708&amp;sc_channel=blog\">large foundation models (FMs)</a> often requires significant infrastructure planning, specialized expertise, and ongoing optimization to handle the computational demands effectively. Many customers find themselves managing complex environments or making trade-offs between performance and cost when deploying these sophisticated models.</p> \n<p>The Pixtral Large model, developed by <a href=\"https://mistral.ai\">Mistral AI</a>, represents their first multimodal model that combines advanced vision capabilities with powerful language understanding. A 128K context window makes it ideal for complex visual reasoning tasks. The model delivers exceptional performance on key benchmarks including <a href=\"https://mathvista.github.io/\">MathVista</a>, <a href=\"https://www.docvqa.org/\">DocVQA</a>, and <a href=\"https://visualqa.org/\">VQAv2</a>, demonstrating its effectiveness across document analysis, chart interpretation, and natural image understanding.</p> \n<p>One of the most powerful aspects of Pixtral Large is its multilingual capability. The model supports dozens of languages including English, French, German, Spanish, Italian, Chinese, Japanese, Korean, Portuguese, Dutch, and Polish, making it accessible to global teams and applications. It‚Äôs also trained on more than 80 programming languages including Python, Java, C, C++, JavaScript, Bash, Swift, and Fortran, providing robust code generation and interpretation capabilities.</p> \n<p>Developers will appreciate the model‚Äôs agent-centric design with built-in function calling and JSON output formatting, which simplifies integration with existing systems. Its strong system prompt adherence improves reliability when working with <a href=\"https://aws.amazon.com/what-is/retrieval-augmented-generation/?trk=4b29643c-e00f-4ab6-ab9c-b1fb47aa1708&amp;sc_channel=blog\">Retrieval Augmented Generation (RAG)</a> applications and large context scenarios.</p> \n<p>With Pixtral Large in Amazon Bedrock, you can now access this advanced model without having to provision or manage any infrastructure. The serverless approach lets you scale usage based on actual demand without upfront commitments or capacity planning. You pay only for what you use, with no idle resources.</p> \n<p><span><strong>Cross-Region inference<br /> </strong></span>Pixtral Large is now available in Amazon Bedrock across multiple <a href=\"https://docs.aws.amazon.com/glossary/latest/reference/glos-chap.html#region?trk=4b29643c-e00f-4ab6-ab9c-b1fb47aa1708&amp;sc_channel=blog\">AWS Regions</a> through cross-Region inference.</p> \n<p><a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/cross-region-inference.html?trk=4b29643c-e00f-4ab6-ab9c-b1fb47aa1708&amp;sc_channel=blog\">With Amazon Bedrock cross-Region inference</a>, you can access a single FM across multiple geographic Regions while maintaining high availability and low latency for global applications. For example, when a model is deployed in both European and US Regions, you can access it through Region-specific API endpoints using distinct prefixes: <code>eu.model-id</code> for European Regions and <code>us.model-id</code> for US Regions . This approach enables Amazon Bedrock to route inference requests to the geographically closest endpoint, reducing latency while helping to meet regulatory compliance by keeping data processing within desired geographic boundaries. The system automatically handles traffic routing and load balancing across these Regional deployments, providing seamless scalability and redundancy without requiring you to keep track of individual Regions where the model is actually deployed.</p> \n<p><span><strong>See it in action<br /> </strong></span>As a developer advocate, I‚Äôm constantly exploring how our newest capabilities can solve real problems. Recently, I had a perfect opportunity to test the new multimodal capabilities in the <a href=\"https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html?trk=4b29643c-e00f-4ab6-ab9c-b1fb47aa1708&amp;sc_channel=blog\">Amazon Bedrock Converse API</a> when my daughter asked for help with her physics exam preparation.</p> \n<p>Last weekend, my kitchen table was covered with practice exams full of complex diagrams, force vectors, and equations. My daughter was struggling with conceptualizing how to approach these problems. That‚Äôs when I realized this was the perfect use case for the multimodal capabilities we‚Äôd just launched. I snapped photos of a particularly challenging problem sheet containing several graphs and mathematical notation, then used the Converse API to create a simple application that could analyze the images. Together, we uploaded the physics exam materials and asked the model to explain the solution approach.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2025/03/28/problem2-1.png\"><img src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2025/03/28/problem2-1.png\" alt=\"Physics problem in french\" width=\"800\" height=\"567\" /></a></p> \n<p>What happened next impressed both of us‚Äîthe model interpreted the diagrams, recognized the french language and the mathematical notation, and provided a step-by-step explanation of how to solve each problem. As we asked follow-up questions about specific concepts, the model maintained context across our entire conversation, creating a tutoring experience that felt remarkably natural.</p> \n<pre><code># Effet Doppler avec une Source Sonore en Rotation\n\n## Analyse du probl√®me\n\nCe probl√®me concerne l'effet Doppler produit par une source sonore en rotation. Une source √©mettant un son √† 1500 Hz tourne sur une table tournante dans le sens antihoraire, et nous devons d√©terminer comment les fr√©quences sont per√ßues par un microphone fixe.\n\n## Concepts cl√©s\n\nL'effet Doppler se produit lorsqu'il y a un mouvement relatif entre une source sonore et un observateur. La fr√©quence per√ßue d√©pend de si la source se rapproche ou s'√©loigne de l'observateur.\n\nLa formule de l'effet Doppler est:\n- Pour une source qui se rapproche: $f_{observ√©e} = f_{source} \\cdot \\frac{v_{son}}{v_{son} - v_{source}}$\n- Pour une source qui s'√©loigne: $f_{observ√©e} = f_{source} \\cdot \\frac{v_{son}}{v_{son} + v_{source}}$\n\n## Solution √©tape par √©tape\n\n### √âtape 1: Analyser le mouvement de la source\nLa source tourne sur un cercle et le microphone est fixe √† 40 cm du centre de rotation. Aux diff√©rentes positions (a, b, c), la source a des vitesses radiales diff√©rentes par rapport au microphone:\n\n- Position a: La source se d√©place perpendiculairement √† la ligne source-microphone (vitesse radiale nulle)\n- Position b: La source s'√©loigne du microphone (vitesse radiale positive)\n- Position c: La source se rapproche du microphone (vitesse radiale n√©gative)\n\n### √âtape 2: D√©terminer l'effet sur les fr√©quences per√ßues\n\n- √Ä la position a: Pas d'effet Doppler car la vitesse radiale est nulle, donc $f_a = f_{source} = 1500$ Hz\n- √Ä la position b: La source s'√©loigne, donc $f_b &lt; f_{source}$, donc $f_b &lt; 1500$ Hz\n- √Ä la position c: La source se rapproche, donc $f_c &gt; f_{source}$, donc $f_c &gt; 1500$ Hz\n\n### √âtape 3: Comparer les fr√©quences\n\nD'apr√®s notre analyse:\n- $f_a = 1500$ Hz\n- $f_b &lt; 1500$ Hz\n- $f_c &gt; 1500$ Hz\n\nDonc: $f_c &gt; f_a &gt; f_b$\n\n## Conclusion\n\nLa proposition correcte est: $f_c &gt; f_a &gt; f_b$\n\nCette r√©ponse correspond √† la troisi√®me proposition dans l'√©nonc√©. Lorsque la source tourne, la fr√©quence per√ßue est plus √©lev√©e quand la source se rapproche (position c), √©gale √† la fr√©quence √©mise quand la source se d√©place perpendiculairement (position a), et plus basse quand la source s'√©loigne (position b).</code></pre> \n<p>The model uses the language of the question to respond. After a thoughtful analysis, it says that the correct answer is f_c &gt; f_a &gt; f_b (and it is right!)</p> \n<p>The beauty of this interaction was how seamlessly the Converse API handled the multimodal inputs. As a builder, I didn‚Äôt need to worry about the complexity of processing images alongside text‚Äîthe API managed that complexity and returned structured responses that my simple application could present directly to my daughter.</p> \n<p>Here is the code I wrote. I used the Swift programming language, just to show that Python is not the only option you have <img src=\"https://s.w.org/images/core/emoji/14.0.0/72x72/1f607.png\" alt=\"üòá\" />.</p> \n<pre><code>private let modelId = \"us.mistral.pixtral-large-2502-v1:0\"\n\n// Define the system prompt that instructs Claude how to respond\nlet systemPrompt = \"\"\"\nYou are a math and physics tutor. Your task is to:\n1. Read and understand the math or physics problem in the image\n2. Provide a clear, step-by-step solution to the problem\n3. Briefly explain any relevant concepts used in solving the problem\n4. Be precise and accurate in your calculations\n5. Use mathematical notation when appropriate\n\nFormat your response with clear section headings and numbered steps.\n\"\"\"\nlet system: BedrockRuntimeClientTypes.SystemContentBlock = .text(systemPrompt)\n\n// Create the user message with text prompt and image\nlet userPrompt = \"Please solve this math or physics problem. Show all steps and explain the concepts involved.\"\nlet prompt: BedrockRuntimeClientTypes.ContentBlock = .text(userPrompt)\nlet image: BedrockRuntimeClientTypes.ContentBlock = .image(.init(format: .jpeg, source: .bytes(finalImageData)))\n\n// Create the user message with both text and image content\nlet userMessage = BedrockRuntimeClientTypes.Message(\n    content: [prompt, image],\n    role: .user\n)\n\n// Initialize the messages array with the user message\nvar messages: [BedrockRuntimeClientTypes.Message] = []\nmessages.append(userMessage)\n\n// Configure the inference parameters\nlet inferenceConfig: BedrockRuntimeClientTypes.InferenceConfiguration = .init(maxTokens: 4096, temperature: 0.0)\n\n// Create the input for the Converse API with streaming\nlet input = ConverseStreamInput(inferenceConfig: inferenceConfig, messages: messages, modelId: modelId, system: [system])\n\n// Make the streaming request\ndo {\n    // Process the stream\n    let response = try await bedrockClient.converseStream(input: input)\n\n    // Iterate through the stream events\n    for try await event in stream {\n        switch event {\n        case .messagestart:\n            print(\"AI-assistant started to stream\")\n\n        case let .contentblockdelta(deltaEvent):\n            // Handle text content as it arrives\n            if case let .text(text) = deltaEvent.delta {\n                DispatchQueue.main.async {\n                    self.streamedResponse += text\n                }\n            }\n\n        case .messagestop:\n            print(\"Stream ended\")\n            // Create a complete assistant message from the streamed response\n            let assistantMessage = BedrockRuntimeClientTypes.Message(\n                content: [.text(self.streamedResponse)],\n                role: .assistant\n            )\n            messages.append(assistantMessage)\n\n        default:\n            break\n        }\n    }\n</code></pre> \n<p>And the result in the app is stunning.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2025/03/28/2025-03-28_17-34-39.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2025/03/28/2025-03-28_17-34-39.png\" alt=\"iOS Physics problem resolver\" width=\"447\" height=\"892\" /></a></p> \n<p>By the time her exam rolled around, she felt confident and prepared‚Äîand I had a compelling real-world example of how our multimodal capabilities in Amazon Bedrock can create meaningful experiences for users.</p> \n<p><span><strong>Get started today<br /> </strong></span>The new model is available through these Regional API endpoints: US East (Ohio, N. Virginia), US West (Oregon), and Europe (Frankfurt, Ireland, Paris, Stockholm). This Regional availability helps you meet data residency requirements while minimizing latency.</p> \n<p>You can start using the model through either the <a href=\"https://console.aws.amazon.com\">AWS Management Console</a> or programmatically through the <a href=\"https://aws.amazon.com/cli/\">AWS Command Line Interface (AWS CLI)</a> and <a href=\"https://aws.amazon.com/developer/tools/?trk=4b29643c-e00f-4ab6-ab9c-b1fb47aa1708&amp;sc_channel=blog\">AWS SDK</a> using the model ID <code>mistral.pixtral-large-2502-v1:0</code>.</p> \n<p>This launch represents a significant step forward in making advanced multimodal AI accessible to developers and organizations of all sizes. By combining Mistral AI‚Äôs cutting-edge model with AWS serverless infrastructure, you can now focus on building innovative applications without worrying about the underlying complexity.</p> \n<p>Visit the <a href=\"https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/overview?trk=4b29643c-e00f-4ab6-ab9c-b1fb47aa1708&amp;sc_channel=blog\">Amazon Bedrock console</a> today to start experimenting with Pixtral Large 25.02 and discover how it can enhance your AI-powered applications.</p> \n<a href=\"https://linktr.ee/sebsto\">‚Äî seb</a> \n<hr /> \n<p>How is the News Blog doing? Take this <a href=\"https://amazonmr.au1.qualtrics.com/jfe/form/SV_eyD5tC5xNGCdCmi\">1 minute survey</a>!</p> \n<p><em>(This <a href=\"https://amazonmr.au1.qualtrics.com/jfe/form/SV_eyD5tC5xNGCdCmi\">survey</a> is hosted by an external company. AWS handles your information as described in the <a href=\"https://aws.amazon.com/privacy/?trk=4b29643c-e00f-4ab6-ab9c-b1fb47aa1708&amp;sc_channel=blog\">AWS Privacy Notice</a>. AWS will own the data gathered via this survey and will not share the information collected with survey respondents.)</em></p>","author":"S√©bastien Stormacq","siteTitle":"AWS News Blog","siteHash":"6093e072e4117ec22616e844cb857d03ca62c57a411a8affc77cb5e8b6b15bf6","entryHash":"30a4647fdfbd54f09a091b9825d878e1c61dd8fc9c1e3cb4ca059c89bf972c52","category":"Tech"}