{"title":"Apple Teams Up With NVIDIA to Speed Up AI Language Models","link":"https://www.macrumors.com/2024/12/20/apple-nvidia-speed-up-ai-language-models/","date":1734693481000,"content":"Apple has <a href=\"https://machinelearning.apple.com/research/redrafter-nvidia-tensorrt-llm\">shared details</a> on a collaboration with NVIDIA to greatly improve the performance of large language models (LLMs) by implementing a new text generation technique that offers substantial speed improvements for AI applications.\r<br />\n\r<br />\n<img src=\"https://images.macrumors.com/article-new/2024/12/ml-research-apple.jpg\" alt=\"\" width=\"1920\" height=\"1080\" />\r<br />\nApple earlier this year <a href=\"https://machinelearning.apple.com/research/recurrent-drafter\">published</a> and <a href=\"https://github.com/apple/ml-recurrent-drafter\">open-sourced</a> Recurrent Drafter (ReDrafter), an approach that combines beam search and dynamic tree attention methods to accelerate text generation. Beam search explores multiple potential text sequences at once for better results, while tree attention organizes and removes redundant overlaps among these sequences to improve efficiency. \r<br />\n\r<br />\nApple has now integrated the technology into NVIDIA's TensorRT-LLM framework, which optimizes LLMs running on NVIDIA GPUs, where it achieved \"state of the art performance,\" according to Apple. The integration saw the technique manage a 2.7x speed increase in tokens generated per second during testing with a production model containing tens of billions of parameters.\r<br />\n\r<br />\nApple says the improved performance not only reduces user-perceived latency but also leads to decreased GPU usage and power consumption. From Apple's <a href=\"https://machinelearning.apple.com/research/redrafter-nvidia-tensorrt-llm\">Machine Learning Research blog</a>:\r<br />\n<blockquote>\"LLMs are increasingly being used to power production applications, and improving inference efficiency can both impact computational costs and reduce latency for users. With ReDrafter's novel approach to speculative decoding integrated into the NVIDIA TensorRT-LLM framework, developers can now benefit from faster token generation on NVIDIA GPUs for their production LLM applications.\"</blockquote>\r<br />\n\r<br />\nDevelopers interested in implementing ReDrafter can find detailed information on both <a href=\"https://machinelearning.apple.com/research/redrafter-nvidia-tensorrt-llm\">Apple's website</a> and <a href=\"https://developer.nvidia.com/blog/nvidia-tensorrt-llm-now-supports-recurrent-drafting-for-optimizing-llm-inference/\">NVIDIA's developer blog</a>.<div>Tag: <a href=\"https://www.macrumors.com/guide/nvidia/\">Nvidia</a></div><br />This article, \"<a href=\"https://www.macrumors.com/2024/12/20/apple-nvidia-speed-up-ai-language-models/\">Apple Teams Up With NVIDIA to Speed Up AI Language Models</a>\" first appeared on <a href=\"https://www.macrumors.com\">MacRumors.com</a><br /><br /><a href=\"https://forums.macrumors.com/threads/apple-teams-up-with-nvidia-to-speed-up-ai-language-models.2445826/\">Discuss this article</a> in our forums<br /><br />","author":"Tim Hardwick","siteTitle":"MacRumors: Mac News and Rumors - All Stories","siteHash":"4c0f1b1ecc2ed084c9f5be50f1058e33a55cdf9b904dadc33a2071fc2d63e8c1","entryHash":"bc511481dcca285e3fef7971740d353dcf48e3d24dc770815792b2b899fa0381","category":"Apple"}