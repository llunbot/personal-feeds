{"title":"Introducing Amazon MSK Connect – Stream Data to and from Your Apache Kafka Clusters Using Managed Connectors","link":"https://aws.amazon.com/blogs/aws/introducing-amazon-msk-connect-stream-data-to-and-from-your-apache-kafka-clusters-using-managed-connectors/","date":1631829378000,"content":"<p><a href=\"https://kafka.apache.org/\">Apache Kafka</a> is an open-source platform for building real-time streaming data pipelines and applications. At re:Invent 2018, we announced <a href=\"https://aws.amazon.com/msk/\">Amazon Managed Streaming for Apache Kafka</a>, a fully managed service that makes it easy to build and run applications that use Apache Kafka to process streaming data.</p> \n<p>When you use Apache Kafka, you capture real-time data from sources such as IoT devices, database change events, and website clickstreams, and deliver it to destinations such as databases and persistent storage.</p> \n<p><a href=\"https://kafka.apache.org/documentation/#connect\">Kafka Connect</a> is an open-source component of Apache Kafka that provides a framework for connecting with external systems such as databases, key-value stores, search indexes, and file systems. However, manually running Kafka Connect clusters requires you to plan and provision the required infrastructure, deal with cluster operations, and scale it in response to load changes.</p> \n<p>Today, we’re announcing a new capability that makes it easier to manage Kafka Connect clusters. <strong>MSK Connect</strong> allows you to configure and deploy a connector using Kafka Connect with a just few clicks. MSK Connect provisions the required resources and sets up the cluster. It continuously monitors the health and delivery state of connectors, patches and manages the underlying hardware, and auto-scales connectors to match changes in throughput. As a result, you can focus your resources on building applications rather than managing infrastructure.</p> \n<p>MSK Connect is fully compatible with Kafka Connect, which means you can migrate your existing connectors without code changes. You don’t need an MSK cluster to use MSK Connect. It supports Amazon MSK, Apache Kafka, and Apache Kafka compatible clusters as sources and sinks. These clusters can be self-managed or managed by AWS partners and 3rd parties as long as MSK Connect can privately connect to the clusters.</p> \n<p><span><strong>Using MSK Connect with Amazon Aurora and Debezium</strong><br /> </span>To test MSK Connect, I want to use it to stream data change events from one of my databases. To do so, I use <a href=\"https://debezium.io/\">Debezium</a>, an open-source distributed platform for change data capture built on top of Apache Kafka.</p> \n<p>I use a MySQL-compatible <a href=\"https://aws.amazon.com/rds/aurora/\">Amazon Aurora</a> database as the source and the Debezium MySQL connector with the setup described in this architectural diagram:</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/08/18/msk-connect-aurora-debezium-architecture-1.png\"><img src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/08/18/msk-connect-aurora-debezium-architecture-1-1024x243.png\" /></a></p> \n<p>To use my Aurora database with Debezium, I need to turn on binary logging in the <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_WorkingWithParamGroups.html\">DB cluster parameter group</a>. I follow the steps in the <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/enable-binary-logging-aurora/\">How do I turn on binary logging for my Amazon Aurora MySQL cluster</a> article.</p> \n<p>Next, I have to create a <strong>custom plugin</strong> for MSK Connect. A custom plugin is a set of <a href=\"https://en.wikipedia.org/wiki/JAR_(file_format)\">JAR</a> files that contain the implementation of one or more connectors, transforms, or converters. Amazon MSK will install the plugin on the workers of the connect cluster where the connector is running.</p> \n<p>From the <a href=\"https://debezium.io/releases/\">Debezium website</a>, I download the MySQL connector plugin for the latest stable release. Because MSK Connect accepts custom plugins in ZIP or JAR format, I convert the downloaded archive to ZIP format and keep the JARs files in the main directory:</p> \n<div> \n <pre><code>$ tar xzf debezium-connector-mysql-1.6.1.Final-plugin.tar.gz\n$ cd debezium-connector-mysql\n$ zip -9 ../debezium-connector-mysql-1.6.1.zip *\n$ cd ..</code></pre> \n</div> \n<p>Then, I use the <a href=\"https://aws.amazon.com/cli/\">AWS Command Line Interface (CLI)</a> to upload the custom plugin to an <a href=\"https://aws.amazon.com/s3/\">Amazon Simple Storage Service (Amazon S3)</a> bucket in the same <a href=\"https://aws.amazon.com/about-aws/global-infrastructure/regions_az/#Regions\">AWS Region</a> I am using for MSK Connect:</p> \n<div> \n <pre><code>$ aws s3 cp debezium-connector-mysql-1.6.1.zip s3://my-bucket/path/</code></pre> \n</div> \n<p>On the <a href=\"https://console.aws.amazon.com/msk/home\">Amazon MSK console</a> there is a new <strong>MSK Connect</strong> section. I look at the connectors and choose <strong>Create connector</strong>. Then, I create a custom plugin and browse my S3 buckets to select the custom plugin ZIP file I uploaded before.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/08/16/msk-connect-create-custom-plugin.png\"><img src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/08/16/msk-connect-create-custom-plugin-1024x535.png\" /></a></p> \n<p>I enter a name and a description for the plugin and then choose <strong>Next</strong>.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/08/16/msk-connect-create-custom-plugin-name.png\"><img src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/08/16/msk-connect-create-custom-plugin-name-1024x428.png\" /></a></p> \n<p>Now that the configuration of the custom plugin is complete, I start the creation of the <strong>connector</strong>. I enter a name and a description for the connector.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/08/16/msk-connect-create-connector.png\"><img src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/08/16/msk-connect-create-connector-1024x527.png\" /></a></p> \n<p>I have the option to use a self-managed Apache Kafka cluster or one that is managed by MSK. I select one of my MSK cluster that is configured to use <a href=\"https://docs.amazonaws.cn/en_us/msk/latest/developerguide/security_iam_service-with-iam.html\">IAM authentication</a>. The MSK cluster I select is in the same <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html\">virtual private cloud (VPC)</a> as my Aurora database. To connect, the MSK cluster and Aurora database use the <code>default</code> <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html#DefaultSecurityGroup\">security group</a> for the VPC. For simplicity, I use a <a href=\"https://docs.aws.amazon.com/msk/latest/developerguide/msk-configuration.html\">cluster configuration</a> with <code>auto.create.topics.enable</code> set to <code>true</code>.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/08/17/msk-connect-create-connector-kafka-cluster-iam.png\"><img src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/08/17/msk-connect-create-connector-kafka-cluster-iam-1024x795.png\" /></a></p> \n<p>In <strong>Connector configuration</strong>, I use the following settings:</p> \n<div> \n <div> \n  <pre><code>connector.class=io.debezium.connector.mysql.MySqlConnector\ntasks.max=1\ndatabase.hostname=&lt;aurora-database-writer-instance-endpoint&gt;\ndatabase.port=3306\ndatabase.user=my-database-user\ndatabase.password=my-secret-password\ndatabase.server.id=123456\ndatabase.server.name=ecommerce-server\ndatabase.include.list=ecommerce\ndatabase.history.kafka.topic=dbhistory.ecommerce\ndatabase.history.kafka.bootstrap.servers=&lt;bootstrap servers&gt;\ndatabase.history.consumer.security.protocol=SASL_SSL\ndatabase.history.consumer.sasl.mechanism=AWS_MSK_IAM\ndatabase.history.consumer.sasl.jaas.config=software.amazon.msk.auth.iam.IAMLoginModule required;\ndatabase.history.consumer.sasl.client.callback.handler.class=software.amazon.msk.auth.iam.IAMClientCallbackHandler\ndatabase.history.producer.security.protocol=SASL_SSL\ndatabase.history.producer.sasl.mechanism=AWS_MSK_IAM\ndatabase.history.producer.sasl.jaas.config=software.amazon.msk.auth.iam.IAMLoginModule required;\ndatabase.history.producer.sasl.client.callback.handler.class=software.amazon.msk.auth.iam.IAMClientCallbackHandler\ninclude.schema.changes=true</code></pre> \n </div> \n</div> \n<p>Some of these settings are generic and should be specified for any connector. For example:</p> \n<ul> \n <li><code>connector.class</code> is the Java class of the connector.</li> \n <li><code>tasks.max</code> is the maximum number of tasks that should be created for this connector.</li> \n</ul> \n<p>Other settings are specific to the Debezium MySQL connector:</p> \n<ul> \n <li>The <code>database.hostname</code> contains the writer instance endpoint of my Aurora database.</li> \n <li>The <code>database.server.name</code> is a logical name of the database server. It is used for the names of the Kafka topics created by Debezium.</li> \n <li>The <code>database.include.list</code> contains the list of databases hosted by the specified server.</li> \n <li>The <code>database.history.kafka.topic</code> is a Kafka topic used internally by Debezium to track database schema changes.</li> \n <li>The <code>database.history.kafka.bootstrap.servers</code> contains the bootstrap servers of the MSK cluster.</li> \n <li>The final eight lines (<code>database.history.consumer.*</code> and <code>database.history.producer.*</code>) enable IAM authentication to access the database history topic.</li> \n</ul> \n<p>In <strong>Connector capacity</strong>, I can choose between autoscaled or provisioned capacity. For this setup, I choose <strong>Autoscaled</strong> and leave all other settings at their defaults.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/08/16/msk-connect-create-connector-capacity.png\"><img src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/08/16/msk-connect-create-connector-capacity-1024x591.png\" /></a></p> \n<p>With autoscaled capacity, I can configure these parameters:</p> \n<ul> \n <li><strong>MSK Connect Unit (MCU) count per worker</strong> – Each MCU provides 1 vCPU of compute and 4 GB of memory.</li> \n <li>The minimum and maximum <strong>number of workers</strong>.</li> \n <li><strong>Autoscaling utilization thresholds</strong> – The upper and lower target utilization thresholds on MCU consumption in percentage to trigger auto scaling.</li> \n</ul> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/08/16/msk-connect-create-connector-autoscaling-1.png\"><img src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/08/16/msk-connect-create-connector-autoscaling-1-1024x484.png\" /></a></p> \n<p>There is a summary of the minimum and maximum MCUs, memory, and network bandwidth for the connector.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/08/16/msk-connect-create-connector-capacity-summary-1.png\"><img src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/08/16/msk-connect-create-connector-capacity-summary-1-1024x459.png\" /></a></p> \n<p>For <strong>Worker configuration</strong>, you can use the default one provided by Amazon MSK or provide your own configuration. In my setup, I use the default one.</p> \n<p>In <strong>Access permissions</strong>, I create a IAM role. In the trusted entities, I add <code>kafkaconnect.amazonaws.com</code> to allow MSK Connect to assume the role.</p> \n<p>The role is used by MSK Connect to interact with the MSK cluster and other AWS services. For my setup, I add:</p> \n<ul> \n <li>Permissions to <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AWS-logs-and-resource-policy.html\">write logs</a> to a <a href=\"https://aws.amazon.com/cloudwatch/\">Amazon CloudWatch</a> log group I created earlier.</li> \n <li>Permissions to <a href=\"https://docs.aws.amazon.com/msk/latest/developerguide/iam-access-control.html#create-iam-access-control-policies\">authenticate to my MSK cluster</a> through IAM.</li> \n</ul> \n<p>The Debezium connector needs access to the cluster configuration to find the replication factor to use to create the history topic. For this reason, I add to the permissions policy the <code>kafka-cluster:DescribeClusterDynamicConfiguration</code> action (equivalent Apache Kafka’s <code>DESCRIBE_CONFIGS</code> cluster ACL).</p> \n<p>Depending on your configuration, you might need to add more permissions to the role (for example, in case the connector needs access to other AWS resources such as an S3 bucket). If that is the case, you should add permissions before creating the connector.</p> \n<p>In <strong>Security</strong>, the settings for authentication and encryption in transit are taken from the MSK cluster.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/08/17/msk-connect-create-connector-security.png\"><img src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/08/17/msk-connect-create-connector-security-1024x792.png\" /></a></p> \n<p>In <strong>Logs</strong>, I choose to deliver logs to <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html\">CloudWatch Logs</a> to have more information on the execution of the connector. By using CloudWatch Logs, I can easily manage retention and interactively search and analyze my log data with <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html\">CloudWatch Logs Insights</a>. I enter the log group <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html\">ARN</a> (it’s the same log group I used before in the IAM role) and then choose <strong>Next</strong>.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/08/16/msk-connect-create-connector-logs.png\"><img src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/08/16/msk-connect-create-connector-logs-1024x693.png\" /></a></p> \n<p>I review the settings and then choose <strong>Create connector</strong>. After a few minutes, the connector is running.</p> \n<p><span><strong>Testing MSK Connect with Amazon Aurora and Debezium</strong></span><br /> Now let’s test the architecture I just set up. I start an <a href=\"https://aws.amazon.com/ec2/\">Amazon Elastic Compute Cloud (Amazon EC2)</a> instance to update the database and start a couple of Kafka consumers to see Debezium in action. To be able to connect to both the MSK cluster and the Aurora database, I use the same VPC and assign the <code>default</code> security group. I also add another security group that gives me SSH access to the instance.</p> \n<p>I <a href=\"https://kafka.apache.org/downloads\">download a binary distribution of Apache Kafka</a> and extract the archive in the home directory:</p> \n<div> \n <pre><code>$ tar xvf kafka_2.13-2.7.1.tgz</code></pre> \n</div> \n<p>To use IAM to authenticate with the MSK cluster, I follow the instructions in the Amazon MSK Developer Guide to <a href=\"https://docs.aws.amazon.com/msk/latest/developerguide/iam-access-control.html#configure-clients-for-iam-access-control\">configure clients for IAM access control</a>. I download the <a href=\"https://github.com/aws/aws-msk-iam-auth/releases\">latest stable release</a> of the <a href=\"https://github.com/aws/aws-msk-iam-auth\">Amazon MSK Library for IAM</a>:</p> \n<div> \n <pre><code>$ wget https://github.com/aws/aws-msk-iam-auth/releases/download/1.1.0/aws-msk-iam-auth-1.1.0-all.jar</code></pre> \n</div> \n<p>In the <code>~/kafka_2.13-2.7.1/config/</code> directory I create a <code>client-config.properties</code> file to configure a Kafka client to use IAM authentication:</p> \n<div> \n <pre><code># Sets up TLS for encryption and SASL for authN.\nsecurity.protocol = SASL_SSL\n\n# Identifies the SASL mechanism to use.\nsasl.mechanism = AWS_MSK_IAM\n\n# Binds SASL client implementation.\nsasl.jaas.config = software.amazon.msk.auth.iam.IAMLoginModule required;\n\n# Encapsulates constructing a SigV4 signature based on extracted credentials.\n# The SASL client bound by \"sasl.jaas.config\" invokes this class.\nsasl.client.callback.handler.class = software.amazon.msk.auth.iam.IAMClientCallbackHandler</code></pre> \n</div> \n<p>I add a few lines to my Bash profile to:</p> \n<ul> \n <li>Add Kafka binaries to the <code>PATH</code>.</li> \n <li>Add the MSK Library for IAM to the <code>CLASSPATH</code>.</li> \n <li>Create the <code>BOOTSTRAP_SERVERS</code> environment variable to store the bootstrap servers of my MSK cluster.</li> \n</ul> \n<pre><code>$ cat &gt;&gt; ~./bash_profile\nexport PATH=~/kafka_2.13-2.7.1/bin:$PATH\nexport CLASSPATH=/home/ec2-user/aws-msk-iam-auth-1.1.0-all.jar\nexport BOOTSTRAP_SERVERS=&lt;bootstrap servers&gt;</code></pre> \n<p>Then, I open three terminal connections to the instance.</p> \n<p>In the <strong>first terminal </strong>connection, I start a Kafka consumer for a topic with the same name as the database server (<code>ecommerce-server</code>). This topic is used by Debezium to stream schema changes (for example, when a new table is created).</p> \n<div> \n <pre><code>$ cd ~/kafka_2.13-2.7.1/\n$ kafka-console-consumer.sh --bootstrap-server $BOOTSTRAP_SERVERS \\\n                            --consumer.config config/client-config.properties \\\n                            --topic ecommerce-server --from-beginning</code></pre> \n</div> \n<p>In the <strong>second terminal </strong>connection, I start another Kafka consumer for a topic with a name built by concatenating the database server (<code>ecommerce-server</code>), the database (<code>ecommerce</code>), and the table (<code>orders</code>). This topic is used by Debezium to stream data changes for the table (for example, when a new record is inserted).</p> \n<div> \n <pre><code>$ cd ~/kafka_2.13-2.7.1/\n$ kafka-console-consumer.sh --bootstrap-server $BOOTSTRAP_SERVERS \\\n                            --consumer.config config/client-config.properties \\\n                            --topic ecommerce-server.ecommerce.orders --from-beginning</code></pre> \n</div> \n<p>In the <strong>third terminal </strong>connection, I install a MySQL client using the MariaDB package and connect to the Aurora database:</p> \n<div> \n <pre><code>$ sudo yum install mariadb\n$ mysql -h &lt;aurora-database-writer-instance-endpoint&gt; -u &lt;database-user&gt; -p</code></pre> \n</div> \n<p>From this connection, I create the <code>ecommerce</code> database and a table for my <code>orders</code>:</p> \n<div> \n <pre><code>CREATE DATABASE ecommerce;\n\nUSE ecommerce\n\nCREATE TABLE orders (\n       order_id VARCHAR(255),\n       customer_id VARCHAR(255),\n       item_description VARCHAR(255),\n       price DECIMAL(6,2),\n       order_date DATETIME DEFAULT CURRENT_TIMESTAMP\n);</code></pre> \n</div> \n<p>These database changes are captured by the Debezium connector managed by MSK Connect and are streamed to the MSK cluster. In the <strong>first terminal</strong>, consuming the topic with schema changes, I see the information on the creation of database and table:</p> \n<div> \n <pre><code>Struct{source=Struct{version=1.6.1.Final,connector=mysql,name=ecommerce-server,ts_ms=1629202831473,db=ecommerce,server_id=1980402433,file=mysql-bin-changelog.000003,pos=9828,row=0},databaseName=ecommerce,ddl=CREATE DATABASE ecommerce,tableChanges=[]}\nStruct{source=Struct{version=1.6.1.Final,connector=mysql,name=ecommerce-server,ts_ms=1629202878811,db=ecommerce,table=orders,server_id=1980402433,file=mysql-bin-changelog.000003,pos=10002,row=0},databaseName=ecommerce,ddl=CREATE TABLE orders ( order_id VARCHAR(255), customer_id VARCHAR(255), item_description VARCHAR(255), price DECIMAL(6,2), order_date DATETIME DEFAULT CURRENT_TIMESTAMP ),tableChanges=[Struct{type=CREATE,id=\"ecommerce\".\"orders\",table=Struct{defaultCharsetName=latin1,primaryKeyColumnNames=[],columns=[Struct{name=order_id,jdbcType=12,typeName=VARCHAR,typeExpression=VARCHAR,charsetName=latin1,length=255,position=1,optional=true,autoIncremented=false,generated=false}, Struct{name=customer_id,jdbcType=12,typeName=VARCHAR,typeExpression=VARCHAR,charsetName=latin1,length=255,position=2,optional=true,autoIncremented=false,generated=false}, Struct{name=item_description,jdbcType=12,typeName=VARCHAR,typeExpression=VARCHAR,charsetName=latin1,length=255,position=3,optional=true,autoIncremented=false,generated=false}, Struct{name=price,jdbcType=3,typeName=DECIMAL,typeExpression=DECIMAL,length=6,scale=2,position=4,optional=true,autoIncremented=false,generated=false}, Struct{name=order_date,jdbcType=93,typeName=DATETIME,typeExpression=DATETIME,position=5,optional=true,autoIncremented=false,generated=false}]}}]}</code></pre> \n</div> \n<p>Then, I go back to the database connection in the <strong>third terminal</strong> to insert a few records in the <code>orders</code> table:</p> \n<pre><code>INSERT INTO orders VALUES (\"123456\", \"123\", \"A super noisy mechanical keyboard\", \"50.00\", \"2021-08-16 10:11:12\");\nINSERT INTO orders VALUES (\"123457\", \"123\", \"An extremely wide monitor\", \"500.00\", \"2021-08-16 11:12:13\");\nINSERT INTO orders VALUES (\"123458\", \"123\", \"A too sensible microphone\", \"150.00\", \"2021-08-16 12:13:14\");\n</code></pre> \n<p>In the <strong>second terminal</strong>, I see the information on the records inserted into the <code>orders</code> table:</p> \n<div> \n <pre><code>Struct{after=Struct{order_id=123456,customer_id=123,item_description=A super noisy mechanical keyboard,price=50.00,order_date=1629108672000},source=Struct{version=1.6.1.Final,connector=mysql,name=ecommerce-server,ts_ms=1629202993000,db=ecommerce,table=orders,server_id=1980402433,file=mysql-bin-changelog.000003,pos=10464,row=0},op=c,ts_ms=1629202993614}\nStruct{after=Struct{order_id=123457,customer_id=123,item_description=An extremely wide monitor,price=500.00,order_date=1629112333000},source=Struct{version=1.6.1.Final,connector=mysql,name=ecommerce-server,ts_ms=1629202993000,db=ecommerce,table=orders,server_id=1980402433,file=mysql-bin-changelog.000003,pos=10793,row=0},op=c,ts_ms=1629202993621}\nStruct{after=Struct{order_id=123458,customer_id=123,item_description=A too sensible microphone,price=150.00,order_date=1629115994000},source=Struct{version=1.6.1.Final,connector=mysql,name=ecommerce-server,ts_ms=1629202993000,db=ecommerce,table=orders,server_id=1980402433,file=mysql-bin-changelog.000003,pos=11114,row=0},op=c,ts_ms=1629202993630}\n</code></pre> \n</div> \n<p>My change data capture architecture is up and running and the connector is fully managed by MSK Connect.</p> \n<p><span><strong>Availability and Pricing<br /> </strong></span>MSK Connect is available in the following <a href=\"https://aws.amazon.com/about-aws/global-infrastructure/regions_az/#Regions\">AWS Regions</a>: Asia Pacific (Mumbai), Asia Pacific (Seoul), Asia Pacific (Singapore), Asia Pacific (Sydney), Asia Pacific (Tokyo), Canada (Central), EU (Frankfurt), EU (Ireland), EU (London), EU (Paris), EU (Stockholm), South America (Sao Paulo), US East (N. Virginia), US East (Ohio), US West (N. California), US West (Oregon). For more information, see the <a href=\"https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/\">AWS Regional Services List</a>.</p> \n<p>With MSK Connect you pay for what you use. The resources used by your connectors can be scaled automatically based on your workload. For more information, see the <a href=\"https://aws.amazon.com/msk/pricing/\">Amazon MSK pricing page</a>.</p> \n<p><a href=\"https://aws.amazon.com/msk/\"><strong>Simplify the management of your Apache Kafka connectors today with MSK Connect.</strong></a></p> \n<p>— <a href=\"https://twitter.com/danilop\">Danilo</a></p>","author":"Danilo Poccia","siteTitle":"AWS News Blog","siteHash":"6093e072e4117ec22616e844cb857d03ca62c57a411a8affc77cb5e8b6b15bf6","entryHash":"0dc8de85a6d11da4aaf8710174b01df051b9485933c64dec2b5c7d685ad4be35","category":"Tech"}