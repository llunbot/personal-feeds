{"title":"Apple Provides Further Clarity on Why It Abandoned Plan to Detect CSAM in iCloud Photos","link":"https://www.macrumors.com/2023/09/01/apple-explains-csam-plan-abandoned/","date":1693562949000,"content":"Apple on Thursday provided its fullest explanation yet for last year <a href=\"https://www.macrumors.com/2022/12/07/apple-abandons-icloud-csam-detection/\">abandoning</a> its controversial plan to detect known Child Sexual Abuse Material (CSAM) stored in <a href=\"https://www.macrumors.com/guide/icloud-photo-library/\">iCloud Photos</a>.\r<br />\n\r<br />\n<img src=\"https://images.macrumors.com/article-new/2021/06/iCloud-General-Feature.jpg\" alt width=\"2250\" height=\"1266\" />\r<br />\nApple's statement, shared with <em><a href=\"https://www.wired.com/story/apple-csam-scanning-heat-initiative-letter/\">Wired</a></em> and reproduced below, came in response to child safety group Heat Initiative's demand that the company \"detect, report, and remove\" CSAM from <a href=\"https://www.macrumors.com/guide/icloud/\">iCloud</a> and offer more tools for users to report such content to the company. \r<br />\n<blockquote>\"Child sexual abuse material is abhorrent and we are committed to breaking the chain of coercion and influence that makes children susceptible to it,\" Erik Neuenschwander, Apple's director of user privacy and child safety, wrote in the company's response to Heat Initiative. He added, though, that after collaborating with an array of privacy and security researchers, digital rights groups, and child safety advocates, the company concluded that it could not proceed with development of a CSAM-scanning mechanism, even one built specifically to preserve privacy.\r<br />\n\r<br />\n\"Scanning every user's privately stored iCloud data would create new threat vectors for data thieves to find and exploit,\" Neuenschwander wrote. \"It would also inject the potential for a slippery slope of unintended consequences. Scanning for one type of content, for instance, opens the door for bulk surveillance and could create a desire to search other encrypted messaging systems across content types.\"</blockquote>In August 2021, Apple <a href=\"https://www.macrumors.com/2021/08/05/apple-new-child-safety-features/\">announced plans</a> for three new child safety features, including a system to detect known CSAM images stored in ‌iCloud Photos‌, a Communication Safety option that blurs sexually explicit photos in the Messages app, and child exploitation resources for <a href=\"https://www.macrumors.com/guide/siri/\">Siri</a>. Communication Safety launched in the U.S. with iOS 15.2 in December 2021 and has since expanded to the U.K., Canada, Australia, and New Zealand, and the ‌Siri‌ resources are also available, but CSAM detection never ended up launching.\r<br />\n\r<br />\nApple initially said CSAM detection would be implemented in an update to iOS 15 and iPadOS 15 by the end of 2021, but the company ultimately postponed the feature based on \"feedback from customers, advocacy groups, researchers, and others.\" \r<br />\n\r<br />\nBut the plans were criticized by a wide range of individuals and organizations, including <a href=\"https://www.macrumors.com/2021/08/05/security-researchers-alarmed-apple-csam-plans/\">security researchers</a>, the <a href=\"https://www.macrumors.com/2021/08/06/snowden-eff-slam-plan-to-scan-messages-images/\">Electronic Frontier Foundation (EFF)</a>, <a href=\"https://www.macrumors.com/2021/08/18/german-politician-letter-tim-cook-csam-scanning/\">politicians</a>, <a href=\"https://www.macrumors.com/2021/08/19/policy-groups-urge-apple-abandon-csam-scanning/\">policy groups</a>, <a href=\"https://www.macrumors.com/2021/08/20/university-researchers-csam-dangerous/\">university researchers</a>, and even <a href=\"https://www.macrumors.com/2021/08/13/apple-employees-concerns-over-csam/\">some Apple employees</a>.\r<br />\n\r<br />\nApple's latest response to the issue comes at a time when the encryption debate has been reignited by the U.K. government, which is <a href=\"https://www.gov.uk/government/consultations/revised-investigatory-powers-act-notices-regimes-consultation\">considering plans to amend surveillance legislation</a> that would require tech companies to disable security features like end-to-end encryption without telling the public.\r<br />\n\r<br />\nApple says it will <a href=\"https://www.macrumors.com/2023/07/20/apple-threatens-to-pull-facetime-and-imessage-uk/\">pull services including FaceTime and iMessage in the U.K.</a> if the legislation is passed in its current form.<div>Tags: <a href=\"https://www.macrumors.com/guide/encryption/\">Encryption</a>, <a href=\"https://www.macrumors.com/guide/apple-child-safety-features/\">Apple Child Safety Features</a></div><br />This article, \"<a href=\"https://www.macrumors.com/2023/09/01/apple-explains-csam-plan-abandoned/\">Apple Provides Further Clarity on Why It Abandoned Plan to Detect CSAM in iCloud Photos</a>\" first appeared on <a href=\"https://www.macrumors.com\">MacRumors.com</a><br /><br /><a href=\"https://forums.macrumors.com/threads/apple-provides-further-clarity-on-why-it-abandoned-plan-to-detect-csam-in-icloud-photos.2400202/\">Discuss this article</a> in our forums<br /><br />","author":"Tim Hardwick","siteTitle":"MacRumors: Mac News and Rumors - All Stories","siteHash":"4c0f1b1ecc2ed084c9f5be50f1058e33a55cdf9b904dadc33a2071fc2d63e8c1","entryHash":"ee802ef7173a0c4d495eae8a9332c97c27a56935c58e69c5611ae1ab39e5ddad","category":"Apple"}