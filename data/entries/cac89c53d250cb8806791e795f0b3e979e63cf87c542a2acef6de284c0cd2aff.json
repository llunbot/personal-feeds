{"title":"Apple เผยแพร่งานวิจัย AI ที่เข้าใจคำถามกำกวม โดยอาศัยบริบทเพิ่มเติมจากข้อมูลในหน้าจอโทรศัพท์","link":"https://www.blognone.com/node/139043","date":1712140396000,"content":"<div><div><div><p>ทีมนักวิจัยของแอปเปิลเผยแพร่ผลงาน AI ที่เพิ่มความสามารถเข้าใจบริบท (context) ของคำถามจากผู้ใช้งาน โดยอ้างอิงจากสิ่งที่ปรากฏบนหน้าจอของผู้ใช้งาน</p>\n<p>ระบบ AI นี้มีชื่อเรียกว่า ReALM ย่อมาจาก Reference Resolution As Language Modeling เป็นโมเดล LLM ที่สามารถเข้าใจคำถามซึ่งไม่ชัดเจนหรือกำกวม ด้วยการหาข้อมูลเพิ่มเติมจากสิ่งที่ปรากฏบนหน้าจอโทรศัพท์ ทำให้บทสนทนามีความลื่นไหลและเป็นธรรมชาติมากยิ่งขึ้น</p>\n<p>ตัวอย่างการใช้งาน เช่น ถามแชทบอตให้แสดงรายชื่อร้านขายยาที่อยู่ใกล้ ตามปกติจะได้คำตอบเป็นรายชื่อร้านขายยา ก็สามารถส่งคำถามต่อไปเช่น \"โทรหาร้านที่อยู่ถนน XXX\" หรือ \"โทรหาร้านล่างสุด\" หรือ \"โทรหาเบอร์นี้\" (หน้าจอมีเบอร์โทรเบอร์เดียว) ซึ่งแชทบอตทั่วไปจะถามกลับขอข้อมูลเพิ่มเติม เนื่องจากคำถามกำกวม แต่ ReALM สามารถทำงานต่อได้เพราะใช้ข้อมูลบนหน้าจอประกอบ ในงานวิจัยนี้ยังทดสอบคำถามประเภทนี้เทียบกับ ChatGPT (ทั้ง GPT-3.5 และ GPT-4) พบว่าทำงานได้ดีกว่า</p>\n<p>งานวิจัยนี้ทำให้เห็นทิศทางฟีเจอร์หนึ่งที่แอปเปิลสามารถนำมาใส่ใน Siri ได้นั่นเอง</p>\n<p>ที่มา: <a href=\"https://venturebeat.com/ai/apple-researchers-develop-ai-that-can-see-and-understand-screen-context/\">VentureBeat</a></p>\n<p><img src=\"https://www.blognone.com/sites/default/files/externals/99adf9c5a191c955920a86e146c2d386.jpg\" /></p>\n</div></div></div><div><div>Topics: </div><div><div><a href=\"/topics/apple\">Apple</a></div><div><a href=\"/topics/artificial-intelligence\">Artificial Intelligence</a></div><div><a href=\"/topics/siri\">Siri</a></div><div><a href=\"/topics/llm\">LLM</a></div></div></div>","author":"arjin","siteTitle":"Blognone","siteHash":"ededadcf18490b3937e7dd89ebe8c00dc129addbdf1ebe4aff1f458146693da0","entryHash":"cac89c53d250cb8806791e795f0b3e979e63cf87c542a2acef6de284c0cd2aff","category":"Thai"}