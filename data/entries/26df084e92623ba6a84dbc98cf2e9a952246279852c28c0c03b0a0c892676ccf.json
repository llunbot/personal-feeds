{"title":"Researchers concerned to find AI models hiding their true “reasoning” processes","link":"https://arstechnica.com/ai/2025/04/researchers-concerned-to-find-ai-models-hiding-their-true-reasoning-processes/","date":1744324633000,"content":"<p>Remember when teachers demanded that you \"show your work\" in school? Some fancy new AI models promise to do exactly that, but <a href=\"https://www.anthropic.com/research/reasoning-models-dont-say-think\">new research</a> suggests that they sometimes hide their actual methods while fabricating elaborate explanations instead.</p>\n<p>New research from Anthropic—creator of the ChatGPT-like Claude AI assistant—examines simulated reasoning (SR) models like <a href=\"https://arstechnica.com/ai/2025/01/china-is-catching-up-with-americas-best-reasoning-ai-models/\">DeepSeek's R1</a>, and its own Claude series. In a research paper <a href=\"https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf\">posted last week</a>, Anthropic's Alignment Science team demonstrated that these SR models frequently fail to disclose when they've used external help or taken shortcuts, despite features designed to show their \"reasoning\" process.</p>\n<p>(It's worth noting that OpenAI's o1 and o3 series SR models deliberately obscure the accuracy of their \"thought\" process, so this study does not apply to them.)</p><p><a href=\"https://arstechnica.com/ai/2025/04/researchers-concerned-to-find-ai-models-hiding-their-true-reasoning-processes/\">Read full article</a></p>\n<p><a href=\"https://arstechnica.com/ai/2025/04/researchers-concerned-to-find-ai-models-hiding-their-true-reasoning-processes/#comments\">Comments</a></p>","author":"Benj Edwards","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"26df084e92623ba6a84dbc98cf2e9a952246279852c28c0c03b0a0c892676ccf","category":"Tech"}