{"title":"Anthropic เปิดตัวระบบป้องกัน jailbreak โมเดลแบบใหม่ ท้าให้ลองเจาะ ทำสำเร็จมีเงินรางวัล","link":"https://www.blognone.com/node/144516","date":1738763272000,"content":"<div><div><div><p>Anthropic เปิดตัวระบบป้องกันการเจาะ (jailbreak) โมเดลปัญญาประดิษฐ์แบบใหม่ชื่อว่า Constitutional Classifiers</p>\n<p>โมเดลภาษาขนาดใหญ่มีระบบ safety ป้องกันการนำโมเดลไปใช้สร้างเนื้อหาอันตราย แต่มนุษย์ก็สามารถใช้ช่องโหว่บางอย่าง เช่น พรอมต์ขนาดยาวมากๆ หรือ การเขียนอินพุตที่ต่างไปจากข้อความที่พบเจอทั่วไป (ตัวอย่าง uSiNg uNuSuAl cApItALiZaTiOn) มาเจาะระบบป้องกันของโมเดล เพื่อหลอกล่อให้ได้คำตอบแบบที่ต้องการ</p>\n<p>เนื่องจากช่องโหว่ของโมเดลมีหลายรูปแบบมาก วิธีการป้องกันย่อมแตกต่างกันไป ทำให้ Anthropic พัฒนากลไกแบบใหม่ที่ป้องกันการเจาะโมเดลได้ทุกแบบ (universal jailbreak)</p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/2aa04105c8bebb2fb76b8b3f035099f7.png\" /></p>\n<p>เทคนิคของ Constitutional Classifiers พัฒนาต่อมาจาก Constitutional AI ที่ใช้ใน Claude อยู่แล้ว แนวคิดของมันคือโมเดลจะมี \"หลักการ\" หรือ \"ธรรมนูญ\" (constitution) ว่าโมเดลตอบอะไรได้ และตอบอะไรไม่ได้ เช่น บอกสูตรการทำมัสตาร์ด (อาหาร) ได้ แต่ไม่สามารถบอกสูตรการทำแก๊สมัสตาร์ด (แก๊สพิษ) ได้</p>\n<p>หลังจากนั้น Anthropic ให้ Claude ช่วยสร้างพรอมต์จำนวนมากๆ อิงจากพรอมต์ที่มนุษย์สร้างเพื่อใช้เจาะโมเดล ปรับแต่งให้หลากหลายขึ้น แล้วนำไปแปลเป็นภาษาต่างๆ อีกชั้น ผลคือพรอมต์จำนวนมากที่ใช้ทดสอบการเจาะโมเดล</p>\n<p>จากนั้นทีมของ Anthropic จะคัดแยกพรอมต์และผลลัพธ์ที่ได้เป็นหมวดหมู่ต่างๆ (classifier) เพื่อให้สามารถบล็อคพรอมต์ลักษณะเดียวกันได้ ทีมวิจัยยังปรับความสมดุลไม่ให้โมเดลปฏิเสธการตอบคำถามมากจนเกินไป (over-refusal) อีกชั้นด้วย</p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/a9b1b67091cb838b7450ed01241025d2.png\" /></p>\n<p>Anthropic บอกว่ามั่นใจในระบบ Constitutional Classifiers และเชิญชวนให้คนทั่วไป <a href=\"https://claude.ai/constitutional-classifiers\">ร่วมทดสอบเจาะโมเดล</a> เพื่อลองดูว่าอินพุตในโลกจริงๆ เป็นอย่างไร ระบบป้องกันทำงานได้ดีแค่ไหน โดยมีเงินรางวัล bug bounty ให้ 15,000 ดอลลาร์ หากมีคนหลอกล่อให้โมเดลตอบคำถามอันตราย 10 ข้อได้ ซึ่งที่ผ่านมาบริษัทได้เชิญผู้เชี่ยวชาญในสาขาต่างๆ 183 คน มาลองเจาะเป็นเวลารวมกันมากกว่า 3,000 ชั่วโมงแล้วยังไม่สำเร็จ</p>\n<p>ที่มา - <a href=\"https://www.anthropic.com/research/constitutional-classifiers\">Anthropic</a>, <a href=\"https://arstechnica.com/ai/2025/02/anthropic-dares-you-to-jailbreak-its-new-ai-model/\">Ars Technica</a></p>\n</div></div></div><div><div>Topics: </div><div><div><a href=\"/topics/anthropic\">Anthropic</a></div><div><a href=\"/topics/llm\">LLM</a></div><div><a href=\"/topics/claude\">Claude</a></div><div><a href=\"/topics/artificial-intelligence\">Artificial Intelligence</a></div></div></div>","author":"mk","siteTitle":"Blognone","siteHash":"ededadcf18490b3937e7dd89ebe8c00dc129addbdf1ebe4aff1f458146693da0","entryHash":"3d61db084b2261a12bc137d235a79a9d78e9f9baff4978aff03de826504c949a","category":"Thai"}