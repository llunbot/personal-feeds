{"title":"Nvidia’s new monster CPU+GPU chip may power the next gen of AI chatbots","link":"https://arstechnica.com/?p=1945664","date":1686239299000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2023/06/gh200_grace_hopper_chip-800x450.jpg\" alt=\"NVIDIA's GH200 \" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2023/06/gh200_grace_hopper_chip.jpg\">Enlarge</a> <span>/</span> NVIDIA's GH200 \"Grace Hopper\" AI superchip. (credit: Nvidia)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>Early last week at COMPUTEX, Nvidia announced that its new <a href=\"https://www.nvidia.com/en-us/data-center/grace-hopper-superchip/\">GH200 Grace Hopper \"Superchip\"</a>—a combination CPU and GPU specifically created for large-scale AI applications—has entered full production. It's a beast. It has 528 GPU tensor cores, supports up to 480GB of CPU RAM and 96GB of GPU RAM, and boasts a GPU memory bandwidth of up to 4TB per second.</p>\n\n<p>We've previously covered the <a href=\"https://arstechnica.com/information-technology/2022/09/hopper-time-nvidias-most-powerful-ai-chip-yet-ships-in-october/\">Nvidia H100 Hopper chip</a>, which is currently Nvidia's most powerful data center GPU. It powers AI models like OpenAI's <a href=\"https://arstechnica.com/information-technology/2022/12/openai-invites-everyone-to-test-new-ai-powered-chatbot-with-amusing-results/\">ChatGPT</a>, and it marked a <a href=\"https://arstechnica.com/information-technology/2022/09/nvidias-flagship-ai-chip-reportedly-4-5x-faster-than-the-previous-champ/\">significant upgrade</a> over 2020's <a href=\"https://arstechnica.com/gadgets/2020/05/nvidia-ditches-intel-cozies-up-to-amd-with-its-new-dgx-a100/\">A100</a> chip, which powered the first round of training runs for many of the news-making generative AI chatbots and image generators we're talking about today.</p>\n<p>Faster GPUs roughly translate into more powerful generative AI models because they can run more matrix multiplications in parallel (and do it faster), which is necessary for today's artificial neural networks to function.</p></div><p><a href=\"https://arstechnica.com/?p=1945664#p3\">Read 6 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=1945664&amp;comments=1\">Comments</a></p>","author":"Benj Edwards","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"b05d1457f40ba6ef8602e9fb4c9e144460d79656e7134a8f6642455a8e839095","category":"Tech"}