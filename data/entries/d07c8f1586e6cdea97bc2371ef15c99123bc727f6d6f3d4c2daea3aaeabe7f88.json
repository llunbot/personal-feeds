{"title":"“Superhuman” Go AIs still have trouble defending against these simple exploits","link":"https://arstechnica.com/?p=2036897","date":1720815631000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/07/GettyImages-1257709162-800x533.jpg\" alt=\"Man vs. machine in a sea of stones.\" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2024/07/GettyImages-1257709162.jpg\">Enlarge</a> <span>/</span> Man vs. machine in a sea of stones. (credit: Getty Images)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>In the ancient Japanese game of <em>Go</em>, state-of-the-art artificial intelligence has generally been able to defeat the best human players <a href=\"https://arstechnica.com/information-technology/2016/03/google-ai-begins-battle-with-humanitys-best-go-player-tonight/\">since at least 2016</a>. But in the last few years, researchers have <a href=\"https://arstechnica.com/information-technology/2022/11/new-go-playing-trick-defeats-world-class-go-ai-but-loses-to-human-amateurs/\">discovered flaws in these top-level AI <em>Go</em> algorithms</a> that <a href=\"https://arstechnica.com/information-technology/2023/02/man-beats-machine-at-go-in-human-victory-over-ai/\">give humans a fighting chance</a>. By using unorthodox \"cyclic\" strategies—ones that even a beginning human player could detect and defeat—a crafty human can often exploit gaps in a top-level AI's strategy and fool the algorithm into a loss.</p>\n<p>Researchers at MIT and <a href=\"https://far.ai/\">FAR AI</a> wanted to see if they could improve this \"worst case\" performance in otherwise \"superhuman\" AI Go algorithms, testing a trio of methods to harden the top-level <a href=\"https://github.com/lightvector/KataGo\">KataGo algorithm</a>'s defenses against adversarial attacks. The results show that creating truly robust, unexploitable AIs may be difficult, even in areas as tightly controlled as board games.</p>\n<h2>Three failed strategies</h2>\n<p>In the pre-print paper <a href=\"https://arxiv.org/pdf/2406.12843\">\"Can <em>Go</em> AIs be adversarially robust?\"</a>, the researchers aim to create a <em>Go</em> AI that is truly \"robust\" against any and all attacks. That means an algorithm that can't be fooled into \"game-losing blunders that a human would not commit\" but also one that would require any competing AI algorithm to spend significant computing resources to defeat it. Ideally, a robust algorithm should also be able to overcome potential exploits by using additional computing resources when confronted with unfamiliar situations.</p></div><p><a href=\"https://arstechnica.com/?p=2036897#p3\">Read 11 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=2036897&amp;comments=1\">Comments</a></p>","author":"Kyle Orland","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"d07c8f1586e6cdea97bc2371ef15c99123bc727f6d6f3d4c2daea3aaeabe7f88","category":"Tech"}