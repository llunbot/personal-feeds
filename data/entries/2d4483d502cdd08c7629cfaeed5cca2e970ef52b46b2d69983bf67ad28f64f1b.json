{"title":"Mistral เปิดตัวโมเดลภาษาขนาดใหญ่ AI รุ่นใหม่ Mixtral 8x22B","link":"https://www.blognone.com/node/139183","date":1712815725000,"content":"<div><div><div><p>Mistral สตาร์ทอัปด้าน AI จากฝรั่งเศส เปิดตัวโมเดลภาษาขนาดใหญ่ (LLM) Mixtral 8x22B ขนาดโมเดล 176 พันล้านพารามิเตอร์ ซึ่งเป็นรุ่นถัดจากโมเดลก่อนหน้า <a href=\"https://www.blognone.com/node/137187\">Mixtral 8x7B</a></p>\n<p>การทำงานของ Mixtral 8x22B ยังคงใช้วิธี MoE (mixture of experts) ที่ทุกเลเยอร์ของแต่ละโทเค็น จะเลือกโมเดลที่เหมาะสม 2 ตัว (เรียกว่า experts) มาใช้ในการประมวลผล ทำให้ไม่ต้องใช้พารามิเตอร์ทั้งหมดมารัน ตอนนี้ Mistral ยังไม่ได้เปิดเผยรายละเอียดทั้งหมดของโมเดล แต่คาดการณ์ว่าโมเดลที่ถูกใช้จริงตอนรันมีประมาณ 38 พันล้านพารามิเตอร์</p>\n<p>ไฟล์ทั้งหมดมีขนาด 262GB สามารถดาวน์โหลดได้จาก <a href=\"https://twitter.com/mistralai/status/1777869263778291896\">X ของ Mistral</a> หรือบน <a href=\"https://huggingface.co/mistral-community/Mixtral-8x22B-v0.1\">Hugging Face</a> ที่มีคนแปลงไฟล์ออกมาให้</p>\n<p>ที่มา: <a href=\"https://venturebeat.com/ai/mistral-ai-drops-new-mixture-of-experts-model-with-a-torrent-link/\">VentureBeat</a></p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/c35c1e63df66099f8b4b52758cd80fcd.jpg\" /></p>\n</div></div></div><div><div>Topics: </div><div><div><a href=\"/topics/mistral\">Mistral</a></div><div><a href=\"/topics/llm\">LLM</a></div></div></div>","author":"arjin","siteTitle":"Blognone","siteHash":"ededadcf18490b3937e7dd89ebe8c00dc129addbdf1ebe4aff1f458146693da0","entryHash":"2d4483d502cdd08c7629cfaeed5cca2e970ef52b46b2d69983bf67ad28f64f1b","category":"Thai"}