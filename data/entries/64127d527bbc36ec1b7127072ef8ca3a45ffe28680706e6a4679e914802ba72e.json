{"title":"Microsoft researchers create super‑efficient AI that uses up to 96% less energy","link":"https://arstechnica.com/ai/2025/04/microsoft-researchers-create-super%e2%80%91efficient-ai-that-uses-up-to-96-less-energy/","date":1745005566000,"content":"<p>When it comes to actually storing the numerical weights that <a href=\"https://arstechnica.com/science/2023/07/a-jargon-free-explanation-of-how-ai-large-language-models-work/\">power an LLM's underlying neural network</a>, most modern AI models rely on the precision of 16- or 32-bit <a href=\"https://blog.demofox.org/2017/11/21/floating-point-precision/\">floating point numbers</a>. But that level of precision can come at the cost of large memory footprints (in the hundreds of gigabytes for the largest models) and significant processing resources needed for <a href=\"https://arstechnica.com/information-technology/2024/03/matrix-multiplication-breakthrough-could-lead-to-faster-more-efficient-ai-models/\">the complex matrix multiplication</a> used when responding to prompts.</p>\n<p>Now, researchers at Microsoft's <a href=\"https://www.microsoft.com/en-us/research/group/general-artificial-intelligence/\">General Artificial Intelligence group</a> have <a href=\"https://huggingface.co/microsoft/bitnet-b1.58-2B-4T\">released a new neural network model</a> that works with just three distinct weight values: -1, 0, or 1. Building on top of previous work Microsoft Research <a href=\"https://arxiv.org/abs/2310.11453\">originally published in 2023</a>, the new model's \"ternary\" architecture offers a reduction in overall complexity and \"substantial advantages in computational efficiency,\" the researchers write, allowing it to <a href=\"https://github.com/microsoft/BitNet\">run effectively on a simple desktop CPU</a>. And despite the massive reduction in weight precision, the researchers claim that the model \"can achieve performance comparable to leading open-weight, full-precision models of similar size across a wide range of tasks.\"</p>\n<h2>Watching your weights</h2>\n<p>The idea of simplifying model weights isn't a completely new one in AI research. For years, researchers have been experimenting with <a href=\"https://huggingface.co/docs/optimum/en/concept_guides/quantization\">quantization techniques</a> that squeeze their neural network weights into smaller memory envelopes. In recent years, the most extreme quantization efforts have <a href=\"https://arxiv.org/abs/2310.11453\">focused on so-called \"BitNets\"</a> that represent each weight in a single bit (representing +1 or -1).</p><p><a href=\"https://arstechnica.com/ai/2025/04/microsoft-researchers-create-super%e2%80%91efficient-ai-that-uses-up-to-96-less-energy/\">Read full article</a></p>\n<p><a href=\"https://arstechnica.com/ai/2025/04/microsoft-researchers-create-super%e2%80%91efficient-ai-that-uses-up-to-96-less-energy/#comments\">Comments</a></p>","author":"Kyle Orland","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"64127d527bbc36ec1b7127072ef8ca3a45ffe28680706e6a4679e914802ba72e","category":"Tech"}