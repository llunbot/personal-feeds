{"title":"IBM has made a new, highly efficient AI processor","link":"https://arstechnica.com/?p=1977529","date":1697826713000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2023/10/image-3-800x450.png\" alt=\"Image of a series of chips on a black background, with one chip labelled \" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2023/10/image-3.png\">Enlarge</a> (credit: <a href=\"https://research.ibm.com/blog/northpole-ibm-ai-chip\">IBM</a>)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>As the utility of AI systems has grown dramatically, so has their energy demand. Training new systems is extremely energy intensive, as it generally requires massive data sets and lots of processor time. Executing a trained system tends to be much less involvedâ€”smartphones can easily manage it in some cases. But, because you execute them so many times, that energy use also tends to add up.</p>\n<p>Fortunately, there are lots of ideas on how to bring the latter energy use back down. IBM and Intel have <a href=\"https://arstechnica.com/science/2014/08/ibm-researchers-make-a-chip-full-of-artificial-neurons/\">experimented with processors</a> designed to <a href=\"https://arstechnica.com/science/2021/09/understanding-neuromorphic-computing-and-why-intels-excited-about-it/\">mimic the behavior</a> of actual neurons. IBM has also tested executing neural network calculations <a href=\"https://arstechnica.com/science/2023/08/ibm-team-builds-low-power-analog-ai-processor/\">in phase change memory</a> to avoid making repeated trips to RAM.</p>\n<p>Now, IBM is back with yet another approach, one that's a bit of \"none of the above.\" The company's new NorthPole processor has taken some of the ideas behind all of these approaches and merged them with a very stripped-down approach to running calculations to create a highly power-efficient chip that can efficiently execute inference-based neural networks. For things like image classification or audio transcription, the chip can be up to 35 times more efficient than relying on a GPU.</p></div><p><a href=\"https://arstechnica.com/?p=1977529#p3\">Read 14 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=1977529&amp;comments=1\">Comments</a></p>","author":"John Timmer","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"bc004b86638e2526b2b8e8fb8644a4ca37bf4afa16493197a7a9a30af359188e","category":"Tech"}