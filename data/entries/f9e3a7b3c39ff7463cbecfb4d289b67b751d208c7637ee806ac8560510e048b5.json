{"title":"“Do not hallucinate”: Testers find prompts meant to keep Apple Intelligence on the rails","link":"https://arstechnica.com/?p=2041307","date":1722970784000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/06/apple_intelligence_hero-800x444.jpg\" alt=\"Craig Federighi stands in front of a screen with the words \" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2024/06/apple_intelligence_hero.jpg\">Enlarge</a> <span>/</span> Apple Intelligence was unveiled at WWDC 2024. (credit: Apple)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>As the parent of a young child, I can tell you that getting a kid to respond the way you want can require careful expectation-setting. Especially when we’re trying something new for the first time, I find that the more detail I can provide, the better he is able to anticipate events and roll with the punches.</p>\n<p>I bring this up because testers of the new Apple Intelligence AI features in the recently released macOS Sequoia beta have <a href=\"https://www.reddit.com/r/MacOSBeta/comments/1ehivcp/comment/lfzi379/\">discovered plaintext JSON files</a> that list a whole bunch of conditions meant to keep the generative AI tech from being unhelpful or inaccurate. I don’t mean to humanize generative AI algorithms, because they don’t deserve to be, but the carefully phrased lists of instructions remind me of what it’s like to try to give basic instructions to (or explain morality to) an entity that isn’t quite prepared to understand it.</p>\n<p>The files in question are stored in the <code>/System/Library/AssetsV2/com_apple_MobileAsset_UAF_FM_GenerativeModels\\purpose_auto</code> folder on Macs running the macOS Sequoia 15.1 beta that have also opted into the Apple Intelligence beta. That folder contains 29 <code>metadata.json</code> files, several of which include a few sentences of what appear to be plain-English <a href=\"https://promptengineering.org/system-prompts-in-large-language-models/\">system prompts</a> to set behavior for an AI chatbot powered by a large-language model (LLM).</p></div><p><a href=\"https://arstechnica.com/?p=2041307#p3\">Read 6 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=2041307&amp;comments=1\">Comments</a></p>","author":"Andrew Cunningham","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"f9e3a7b3c39ff7463cbecfb4d289b67b751d208c7637ee806ac8560510e048b5","category":"Tech"}