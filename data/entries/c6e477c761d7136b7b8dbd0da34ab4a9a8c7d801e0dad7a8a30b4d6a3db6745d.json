{"title":"Red Hat เปิดตัว AI Inference Server ระบบเซิร์ฟเวอร์รัน AI ด้วย vLLM","link":"https://www.blognone.com/node/146543","date":1747831862000,"content":"<span>Red Hat เปิดตัว AI Inference Server ระบบเซิร์ฟเวอร์รัน AI ด้วย vLLM</span>\n\n  <div>\n    <div>Body</div>\n              <div><p>Red Hat เปิดตัว AI Inference Server ชุดซอฟต์แวร์สำหรับทำเซิร์ฟเวอร์รันงาน AI ตามสมัยนิยม</p>\n<p>พื้นฐานของมันคือ Red Hat Enterprise Linux AI (RHEL AI) และ Red Hat OpenShift AI แล้วรันซอฟต์แวร์ยอดนิยมอย่าง <a href=\"https://docs.vllm.ai/en/latest/\">vLLM</a> ไลบรารีสำหรับการรัน LLM inference และเรียกโมเดลจากบน Hugging Face</p>\n<p>ศูนย์กลางของ Red Hat AI Inference Server ย่อมเป็น vLLM ซึ่งกำเนิดจากโครงการวิจัยของ University of California, Berkeley และใช้เทคนิค PagedAttention จัดการแบ่งส่วนจีพียู รวมถึงเทคนิคปรับแต่งประสิทธิภาพอื่นๆ (เช่น LLM Compressor) เพื่อให้โมเดลรันในเครื่องเซิร์ฟเวอร์ทั่วไปได้</p>\n<p>Red Hat AI Inference Server สามารถทำงานบนฮาร์ดแวร์ได้หลากหลาย รองรับจีพียู NVIDIA, AMD รวมถึง Google TPU</p>\n<p>ชุดซอฟต์แวร์ Red Hat AI Inference Server สามารถรันได้บนลินุกซ์ค่ายอื่นๆ ได้ด้วย ออกแบบมาเพื่อความ portability รันได้ทั้งบนคลาวด์และรันแบบ on-premise (สโลแกนคือ any model, any accelerator, any cloud) ส่วนวิธีคิดเงินบอกว่านับตามจำนวนจีพียูหรือชิปเร่งความเร็ว AI</p>\n<p>ที่มา - <a href=\"https://www.redhat.com/en/blog/red-hat-ai-inference-server-technical-deep-dive\">Red Hat</a></p>\n</div>\n          </div>\n<span><a href=\"https://www.blognone.com/user/mk\">mk</a></span>\n<span><time>Wed, 05/21/2025 - 19:51</time>\n</span>","author":"mk","siteTitle":"Blognone","siteHash":"ededadcf18490b3937e7dd89ebe8c00dc129addbdf1ebe4aff1f458146693da0","entryHash":"c6e477c761d7136b7b8dbd0da34ab4a9a8c7d801e0dad7a8a30b4d6a3db6745d","category":"Thai"}