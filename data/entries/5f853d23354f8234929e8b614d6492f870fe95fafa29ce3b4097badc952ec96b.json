{"title":"Amazon Bedrock now provides access to Cohere Command Light and Cohere Embed English and multilingual models","link":"https://aws.amazon.com/blogs/aws/amazon-bedrock-now-provides-access-to-cohere-command-light-and-cohere-embed-english-and-multilingual-models/","date":1699905746000,"content":"<p>Cohere provides text generation and representation models powering business applications to generate text, summarize, search, cluster, classify, and utilize <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html\">Retrieval Augmented Generation (RAG)</a>. Today, we’re announcing the availability of <a href=\"https://aws.amazon.com/bedrock/cohere-command/\">Cohere Command Light</a> and <a href=\"https://aws.amazon.com/bedrock/cohere-command/\">Cohere Embed English and multilingual models</a> on <a href=\"https://aws.amazon.com/bedrock/\">Amazon Bedrock</a>. They’re joining the already available <a href=\"https://aws.amazon.com/bedrock/cohere-command/\">Cohere Command model</a>.</p> \n<p>Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies, including <a href=\"https://aws.amazon.com/bedrock/jurassic/\">AI21 Labs</a>, <a href=\"https://aws.amazon.com/bedrock/claude/\">Anthropic</a>, <a href=\"https://aws.amazon.com/bedrock/cohere-command/\">Cohere</a>, <a href=\"https://aws.amazon.com/bedrock/llama-2/\">Meta</a>, <a href=\"https://aws.amazon.com/bedrock/stable-diffusion/\">Stability AI</a>, and <a href=\"https://aws.amazon.com/bedrock/titan/\">Amazon</a>, along with a broad set of capabilities to build generative AI applications, simplifying the development while maintaining privacy and security. With this launch, Amazon Bedrock further expands the breadth of model choices to help you build and scale enterprise-ready generative AI. <a href=\"https://aws.amazon.com/blogs/aws/amazon-bedrock-is-now-generally-available-build-and-scale-generative-ai-applications-with-foundation-models/\">You can read more about Amazon Bedrock in Antje’s post here</a>.</p> \n<p>Command is Cohere’s flagship text generation model. It is trained to follow user commands and to be useful in business applications. Embed is a set of models trained to produce high-quality embeddings from text documents.</p> \n<p>Embeddings are one of the most fascinating concepts in <a href=\"https://aws.amazon.com/machine-learning/\">machine learning (ML)</a>. They are central to many applications that process natural language, recommendations, and search algorithms. Given any type of document, text, image, video, or sound, it is possible to transform it into a suite of numbers, known as a <a href=\"https://en.wikipedia.org/wiki/Vector_(mathematics_and_physics)\">vector</a>. Embeddings refer specifically to the technique of representing data as vectors in such a way that it captures meaningful information, semantic relationships, or contextual characteristics. In simple terms, embeddings are useful because the vectors representing similar documents are “close” to each other. In more formal terms, embeddings translate <a href=\"https://en.wikipedia.org/wiki/Semantic_similarity\">semantic similarity</a> as perceived by humans to proximity in a <a href=\"https://en.wikipedia.org/wiki/Vector_space\">vector space</a>. Embeddings are typically generated through training algorithms or models.</p> \n<p>Cohere Embed is a family of models trained to generate embeddings from text documents. Cohere Embed comes in two forms, an English language model and a multilingual model, both of which are now available in Amazon Bedrock.</p> \n<p>There are three main use cases for text embeddings:</p> \n<p><a href=\"https://aws.amazon.com/blogs/big-data/try-semantic-search-with-the-amazon-opensearch-service-vector-engine/\">Semantic searches</a> – Embeddings enable searching collections of documents by meaning, which leads to search systems that better incorporate context and user intent compared to existing keyword-matching systems.</p> \n<p><a href=\"https://aws.amazon.com/blogs/machine-learning/text-classification-for-online-conversations-with-machine-learning-on-aws/\">Text Classification</a> – Build systems that automatically categorize text and take action based on the type. For example, an email filtering system might decide to route one message to sales and escalate another message to tier-two support.</p> \n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html\">Retrieval Augmented Generation (RAG)</a> – Improve the quality of a large language model (LLM) text generation by augmenting your prompts with data provided in context. The external data used to augment your prompts can come from multiple data sources, such as document repositories, databases, or APIs.</p> \n<p>Imagine you have hundreds of documents describing your company policies. Due to the limited size of prompts accepted by LLMs, you have to select relevant parts of these documents to be included as context into prompts. The solution is to transform all your documents into embeddings and store them in a vector database, such as <a href=\"https://opensearch.org/platform/search/vector-database.html\">OpenSearch</a>.</p> \n<p>When a user wants to query this corpus of documents, you transform the user’s natural language query into a vector and perform a similarity search on the vector database to find the most relevant documents for this query. Then, you embed (pun intended) the original query from the user and the relevant documents surfaced by the vector database together in a prompt for the LLM. Including relevant documents in the context of the prompt helps the LLM generate more accurate and relevant answers.</p> \n<p>You can now integrate Cohere Command Light and Embed models in your applications written in any programming language by calling the Bedrock API or using the <a href=\"https://aws.amazon.com/tools/\">AWS SDKs</a> or the <a href=\"https://aws.amazon.com/cli/\">AWS Command Line Interface (AWS CLI)</a>.</p> \n<p><span><strong>Cohere Embed in action<br /> </strong></span>Those of you who regularly read the AWS News Blog know we like to show you the technologies we write about.</p> \n<p>We’re launching three distinct models today: Cohere Command Light, Cohere Embed English, and Cohere Embed multilingual. Writing code to invoke Cohere Command Light is no different than for Cohere Command, <a href=\"https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/providers?model=cohere.command-text-v14\" target=\"_blank\">which is already part of Amazon Bedrock</a>. So for this example, I decided to show you how to write code to interact with Cohere Embed and review how to use the embedding it generates.</p> \n<p>To get started with a new model on Bedrock, I first navigate to the <a href=\"https://console.aws.amazon.com\">AWS Management Console</a> and open <a href=\"https://console.aws.amazon.com/bedrock/\">the Bedrock page</a>. Then, I select <strong>Model access</strong> on the bottom left pane. Then I select the <strong>Edit</strong> button on the top right side, and I enable access to the Cohere model.</p> \n<p><strong><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/11/10/Screenshot-2023-11-09-at-1.16.17-PM.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/11/10/Screenshot-2023-11-09-at-1.16.17-PM.png\" alt=\"Bedrock - model activation with Cohere models\" width=\"800\" height=\"329\" /></a></strong></p> \n<p>Now that I know I can access the model, I open a code editor on my laptop. I assume you have the <a href=\"https://aws.amazon.com/cli/\">AWS Command Line Interface (AWS CLI)</a> configured, which will allow the AWS SDK to locate your AWS credentials. I use Python for this demo, but I want to show that Bedrock can be called from any language. I also <a href=\"https://gist.github.com/sebsto/544df0894b5e69b5e3cb504a004f772f\">share a public gist</a> with the same code sample written in the <a href=\"https://www.swift.org/\">Swift programming language</a>.</p> \n<p>Back to Python, I first run the <a href=\"https://docs.aws.amazon.com/bedrock/latest/APIReference/API_ListFoundationModels.html\">ListFoundationModels</a> API call to discover the <code>modelId</code> for Cohere Embed.</p> \n<pre><code>import boto3\nimport json\nimport numpy\n\nbedrock = boto3.client(service_name='bedrock', region_name='us-east-1')\n\nlistModels = bedrock.list_foundation_models(byProvider='cohere')\nprint(\"\\n\".join(list(map(lambda x: f\"{x['modelName']} : { x['modelId'] }\", listModels['modelSummaries']))))\n</code></pre> \n<p>Running this code produces the list:</p> \n<pre><code>Command : cohere.command-text-v14\nCommand Light : cohere.command-light-text-v14\nEmbed English : cohere.embed-english-v3\nEmbed Multilingual : cohere.embed-multilingual-v3</code></pre> \n<p>I select <code>cohere.embed-english-v3</code> model ID and write the code to transform a text document into an embedding.</p> \n<pre><code>cohereModelId = 'cohere.embed-english-v3'\n\n# For the list of parameters and their possible values, \n# check Cohere's API documentation at https://docs.cohere.com/reference/embed\n\ncoherePayload = json.dumps({\n     'texts': [\"This is a test document\", \"This is another document\"],\n     'input_type': 'search_document',\n     'truncate': 'NONE'\n})\n\nbedrock_runtime = boto3.client(\n    service_name='bedrock-runtime', \n    region_name='us-east-1'\n)\nprint(\"\\nInvoking Cohere Embed...\")\nresponse = bedrock_runtime.invoke_model(\n    body=coherePayload, \n    modelId=cohereModelId, \n    accept='application/json', \n    contentType='application/json'\n)\n\nbody = response.get('body').read().decode('utf-8')\nresponse_body = json.loads(body)\nprint(np.array(response_body['embeddings']))\n</code></pre> \n<p>The response is printed</p> \n<p><code>[ 1.234375 -0.63671875 -0.28515625 ... 0.38085938 -1.2265625 0.22363281]</code></p> \n<p>Now that I have the embedding, the next step depends on my application. I can store this embedding in a vector store or use it to search similar documents in an existing store, and so on.</p> \n<p>To learn more, I highly recommend following the hands-on instructions provided by <a href=\"https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/03_QuestionAnswering/01_qa_w_rag_claude.ipynb\">this section of the Amazon Bedrock workshop</a>. This is an end-to-end example of RAG. It demonstrates how to load documents, generate embeddings, store the embeddings in a vector store, perform a similarity search, and use relevant documents in a prompt sent to an LLM.</p> \n<p><span><strong>Availability<br /> </strong></span>The Cohere Embed models are available today for all AWS customers in two of the AWS Regions where Amazon Bedrock is available: US East (N. Virginia) and US West (Oregon).</p> \n<p>AWS charges for model inference. For Command Light, AWS charges per processed input or output token. For Embed models, AWS charges per input tokens. You can choose to be charged on a pay-as-you-go basis, with no upfront or recurring fees. You can also provision sufficient throughput to meet your application’s performance requirements in exchange for a time-based term commitment. <a href=\"https://aws.amazon.com/bedrock/pricing/\">The Amazon Bedrock pricing page has the details</a>.</p> \n<p>With this information, you’re ready to use text embeddings with Amazon Bedrock and the Cohere Embed models in your applications.</p> \n<p><a href=\"https://console.aws.amazon.com/bedrock\">Go build!</a></p> \n<a href=\"https://twitter.com/sebsto\">-- seb</a>","author":"Sébastien Stormacq","siteTitle":"AWS News Blog","siteHash":"6093e072e4117ec22616e844cb857d03ca62c57a411a8affc77cb5e8b6b15bf6","entryHash":"5f853d23354f8234929e8b614d6492f870fe95fafa29ce3b4097badc952ec96b","category":"Tech"}