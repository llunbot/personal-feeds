{"title":"Stability announces Stable Diffusion 3, a next-gen AI image generator","link":"https://arstechnica.com/?p=2005341","date":1708637332000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/02/sd3_cham_hero_1-800x450.jpg\" alt=\"Stable Diffusion 3 generation with the prompt: studio photograph closeup of a chameleon over a black background.\" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2024/02/sd3_cham_hero_1.jpg\">Enlarge</a> <span>/</span> Stable Diffusion 3 generation with the prompt: studio photograph closeup of a chameleon over a black background. (credit: <a href=\"https://x.com/StabilityAI/status/1760658260317675788?s=20\">Stability AI</a>)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>On Thursday, Stability AI announced Stable Diffusion 3, an open-weights next-generation image-synthesis model. It follows its predecessors by reportedly generating detailed, multi-subject images with improved quality and accuracy in text generation. The brief announcement was not accompanied by a public demo, but Stability is <a href=\"http://stability.ai/stablediffusion3\">opening up a waitlist</a> today for those who would like to try it.</p>\n\n<p>Stability says that its Stable Diffusion 3 family of models (which takes text descriptions called \"prompts\" and turns them into matching images) range in size from 800 million to 8 billion parameters. The size range accommodates allowing different versions of the model to run locally on a variety of devicesâ€”from smartphones to servers. Parameter size roughly corresponds to model capability in terms of how much detail it can generate. Larger models also require more VRAM on GPU accelerators to run.</p>\n<p>Since 2022, we've seen Stability launch a progression of AI image-generation models: Stable Diffusion <a href=\"https://arstechnica.com/information-technology/2022/09/with-stable-diffusion-you-may-never-believe-what-you-see-online-again/\">1.4</a>, 1.5, <a href=\"https://stability.ai/news/stable-diffusion-v2-release\">2.0</a>, <a href=\"https://arstechnica.com/information-technology/2022/12/artstation-artists-stage-mass-protest-against-ai-generated-artwork/\">2.1</a>, <a href=\"https://arstechnica.com/information-technology/2023/07/stable-diffusion-xl-puts-ai-generated-visual-worlds-at-your-gpus-command/\">XL</a>, <a href=\"https://arstechnica.com/information-technology/2023/11/stable-diffusion-turbo-xl-accelerates-image-synthesis-with-one-step-generation/\">XL Turbo</a>, and now 3. Stability has made a name for itself as providing a more open alternative to proprietary image-synthesis models like OpenAI's DALL-E 3, though <a href=\"https://arstechnica.com/information-technology/2022/09/have-ai-image-generators-assimilated-your-art-new-tool-lets-you-check/\">not without controversy</a> due to the use of copyrighted training data, bias, and the <a href=\"https://arstechnica.com/information-technology/2022/12/thanks-to-ai-its-probably-time-to-take-your-photos-off-the-internet/\">potential for abuse</a>. (This has led to <a href=\"https://arstechnica.com/tech-policy/2023/04/stable-diffusion-copyright-lawsuits-could-be-a-legal-earthquake-for-ai/\">lawsuits</a> that are unresolved.) Stable Diffusion models have been open-weights and source-available, which means the models can be run locally and fine-tuned to change their outputs.</p></div><p><a href=\"https://arstechnica.com/?p=2005341#p3\">Read 7 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=2005341&amp;comments=1\">Comments</a></p>","author":"Benj Edwards","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"9be064198637dd9c7e41329abc2437408779c7ae059e1658f8ff0a578b211054","category":"Tech"}