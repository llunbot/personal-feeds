{"title":"Amazon SageMaker Clarify makes it easier to evaluate and select foundation models (preview)","link":"https://aws.amazon.com/blogs/aws/amazon-sagemaker-clarify-makes-it-easier-to-evaluate-and-select-foundation-models-preview/","date":1701282186000,"content":"<p>I’m happy to share that <a href=\"https://aws.amazon.com/sagemaker/clarify/\">Amazon SageMaker Clarify</a> now supports foundation model (FM) evaluation (preview). As a data scientist or machine learning (ML) engineer, you can now use SageMaker Clarify to evaluate, compare, and select FMs in minutes based on metrics such as accuracy, robustness, creativity, factual knowledge, bias, and toxicity. This new capability adds to SageMaker Clarify’s existing ability to detect bias in ML data and models and explain model predictions.</p> \n<p>The new capability provides both automatic and human-in-the-loop evaluations for large language models (LLMs) anywhere, including LLMs available in <a href=\"https://aws.amazon.com/sagemaker/jumpstart/\">SageMaker JumpStart</a>, as well as models trained and hosted outside of AWS. This removes the heavy lifting of finding the right model evaluation tools and integrating them into your development environment. It also simplifies the complexity of trying to adopt academic benchmarks to your generative artificial intelligence (AI) use case.</p> \n<p><strong><u>Evaluate FMs with SageMaker Clarify<br /> </u></strong>With SageMaker Clarify, you now have a single place to evaluate and compare any LLM based on predefined criteria during model selection and throughout the model customization workflow. In addition to automatic evaluation, you can also use the human-in-the-loop capabilities to set up human reviews for more subjective criteria, such as helpfulness, creative intent, and style, by using your own workforce or managed workforce from <a href=\"https://aws.amazon.com/sagemaker/groundtruth/\">SageMaker Ground Truth</a>.</p> \n<p>To get started with model evaluations, you can use curated prompt datasets that are purpose-built for common LLM tasks, including open-ended text generation, text summarization, question answering (Q&amp;A), and classification. You can also extend the model evaluation with your own custom prompt datasets and metrics for your specific use case. Human-in-the-loop evaluations can be used for any task and evaluation metric. After each evaluation job, you receive an evaluation report that summarizes the results in natural language and includes visualizations and examples. You can download all metrics and reports and also integrate model evaluations into SageMaker MLOps workflows.</p> \n<p>In SageMaker Studio, you can find <strong>Model evaluation</strong> under <strong>Jobs</strong> in the left menu. You can also select <strong>Evaluate</strong> directly from the model details page of any LLM in SageMaker JumpStart.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/11/22/2023-sm-clarify-model-evaluate-08.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/11/22/2023-sm-clarify-model-evaluate-08.png\" alt=\"Evaluate foundation models with Amazon SageMaker Clarify\" width=\"1582\" height=\"912\" /></a></p> \n<p>Select <strong>Evaluate a model</strong> to set up the evaluation job. The UI wizard will guide you through the selection of automatic or human evaluation, model(s), relevant tasks, metrics, prompt datasets, and review teams.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/11/22/2023-sm-clarify-model-evaluate-05.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/11/22/2023-sm-clarify-model-evaluate-05.png\" alt=\"Evaluate foundation models with Amazon SageMaker Clarify\" width=\"1700\" height=\"1028\" /></a></p> \n<p>Once the model evaluation job is complete, you can view the results in the evaluation report.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/11/22/2023-sm-clarify-model-evaluate-07.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/11/22/2023-sm-clarify-model-evaluate-07.png\" alt=\"Evaluate foundation models with Amazon SageMaker Clarify\" width=\"1579\" height=\"1071\" /></a></p> \n<p>In addition to the UI, you can also start with example Jupyter notebooks that walk you through step-by-step instructions on how to programmatically run model evaluation in SageMaker.</p> \n<p><strong><u>Evaluate models anywhere with the FMEval open source library<br /> </u></strong>To run model evaluation anywhere, including models trained and hosted outside of AWS, use the <a href=\"https://github.com/aws/fmeval\">FMEval open source library</a>. The following example demonstrates how to use the library to evaluate a custom model by extending the ModelRunner class.</p> \n<p>For this demo, I choose <a href=\"https://huggingface.co/gpt2\">GPT-2</a> from the Hugging Face model hub and define a custom <code>HFModelConfig</code> and <code>HuggingFaceCausalLLMModelRunner</code> class that works with causal decoder-only models from the Hugging Face model hub such as GPT-2. The example is also available in the <a href=\"https://github.com/aws/fmeval/tree/main/examples\">FMEval GitHub repo</a>.</p> \n<pre><code>!pip install fmeval\n\n# ModelRunners invoke FMs\nfrom amazon_fmeval.model_runners.model_runner import ModelRunner\n\n# Additional imports for custom model\nimport warnings\nfrom dataclasses import dataclass\nfrom typing import Tuple, Optional\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n@dataclass\nclass HFModelConfig:\n    model_name: str\n    max_new_tokens: int\n    normalize_probabilities: bool = False\n    seed: int = 0\n    remove_prompt_from_generated_text: bool = True\n\nclass HuggingFaceCausalLLMModelRunner(ModelRunner):\n    def __init__(self, model_config: HFModelConfig):\n        self.config = model_config\n        self.model = AutoModelForCausalLM.from_pretrained(self.config.model_name)\n        self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)\n\n    def predict(self, prompt: str) -&gt; Tuple[Optional[str], Optional[float]]:\n        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n        generations = self.model.generate(\n            **input_ids,\n            max_new_tokens=self.config.max_new_tokens,\n            pad_token_id=self.tokenizer.eos_token_id,\n        )\n        generation_contains_input = (\n            input_ids[\"input_ids\"][0] == generations[0][: input_ids[\"input_ids\"].shape[1]]\n        ).all()\n        if self.config.remove_prompt_from_generated_text and not generation_contains_input:\n            warnings.warn(\n                \"Your model does not return the prompt as part of its generations. \"\n                \"`remove_prompt_from_generated_text` does nothing.\"\n            )\n        if self.config.remove_prompt_from_generated_text and generation_contains_input:\n            output = self.tokenizer.batch_decode(generations[:, input_ids[\"input_ids\"].shape[1] :])[0]\n        else:\n            output = self.tokenizer.batch_decode(generations, skip_special_tokens=True)[0]\n\n        with torch.inference_mode():\n            input_ids = self.tokenizer(self.tokenizer.bos_token + prompt, return_tensors=\"pt\")[\"input_ids\"]\n            model_output = self.model(input_ids, labels=input_ids)\n            probability = -model_output[0].item()\n\n        return output, probability</code></pre> \n<p>Next, create an instance of <code>HFModelConfig</code> and <code>HuggingFaceCausalLLMModelRunner</code> with the model information.</p> \n<div> \n <pre><code>hf_config = HFModelConfig(model_name=\"gpt2\", max_new_tokens=32)\nmodel = HuggingFaceCausalLLMModelRunner(model_config=hf_config)</code></pre> \n</div> \n<p>Then, select and configure the evaluation algorithm.</p> \n<div> \n <pre><code># Let's evaluate the FM for FactualKnowledge\nfrom amazon_fmeval.fmeval import get_eval_algorithm\nfrom amazon_fmeval.eval_algorithms.factual_knowledge import FactualKnowledgeConfig\n\neval_algorithm_config = FactualKnowledgeConfig(\"&lt;OR&gt;\")\neval_algorithm = get_eval_algorithm(\"factual_knowledge\", eval_algorithm_config)\n</code></pre> \n</div> \n<p>Let’s first test with one sample. The evaluation score is the percentage of factually correct responses.</p> \n<div> \n <pre><code>model_output = model.predict(\"London is the capital of\")[0]\nprint(model_output)\n\neval_algo.evaluate_sample(\n    target_output=\"UK&lt;OR&gt;England&lt;OR&gt;United Kingdom\", \n\tmodel_output=model_output\n)</code></pre> \n</div> \n<div> \n <pre><code>the UK, and the UK is the largest producer of food in the world.\n\nThe UK is the world's largest producer of food in the world.\n[EvalScore(name='factual_knowledge', value=1)]\n</code></pre> \n</div> \n<p>Although it’s not a perfect response, it includes “UK.”</p> \n<p>Next, you can evaluate the FM using built-in datasets or define your custom dataset. If you want to use a custom evaluation dataset, create an instance of <code>DataConfig</code>:</p> \n<div> \n <pre><code>config = DataConfig(\n    dataset_name=\"my_custom_dataset\",\n    dataset_uri=\"dataset.jsonl\",\n    dataset_mime_type=MIME_TYPE_JSONLINES,\n    model_input_location=\"question\",\n    target_output_location=\"answer\",\n)\n\neval_output = eval_algorithm.evaluate(\n    model=model, \n    dataset_config=config, \n    prompt_template=\"$feature\", #$feature is replaced by the input value in the dataset \n    save=True\n)\n</code></pre> \n</div> \n<p>The evaluation results will return a combined evaluation score across the dataset and detailed results for each model input stored in a local output path.</p> \n<p><b><u>Join the preview<br /> </u></b>FM evaluation with Amazon SageMaker Clarify is available today in public preview in AWS Regions US East (Ohio), US East (N. Virginia), US West (Oregon), Asia Pacific (Singapore), Asia Pacific (Tokyo), Europe (Frankfurt), and Europe (Ireland). The <a href=\"https://github.com/aws/fmeval\">FMEval open source library</a> is available on GitHub. To learn more, visit <a href=\"https://aws.amazon.com/sagemaker/clarify/\">Amazon SageMaker Clarify.</a></p> \n<p><span><strong>Get started</strong></span><br /> Log in to the <a href=\"https://console.aws.amazon.com/sagemaker/home\">AWS Management Console</a> and start evaluating your FMs with SageMaker Clarify today!</p> \n<p>— <a href=\"https://www.linkedin.com/in/antje-barth/\" target=\"_blank\">Antje</a></p>","author":"Antje Barth","siteTitle":"AWS News Blog","siteHash":"6093e072e4117ec22616e844cb857d03ca62c57a411a8affc77cb5e8b6b15bf6","entryHash":"02a61e08590418972793354aee32fffda28ffe935e466cfa50abbf91d689fbf0","category":"Tech"}