{"title":"Nomad Bench: Load testing and benchmarking for Nomad","link":"https://www.hashicorp.com/blog/nomad-bench-load-testing-and-benchmarking-for-nomad","date":1721149200000,"content":"<p><a href=\"https://www.nomadproject.io/\">HashiCorp Nomad</a> is simple to deploy and highly scalable, but exactly how scalable? Production clusters reach 10,000 clients and beyond, but reproducing bugs or testing load characteristics at this scale is challenging and costly. The <a href=\"https://www.hashicorp.com/c1m\">one million </a>and <a href=\"https://www.hashicorp.com/c2m\">two million container challenges</a> provide an impressive baseline for scheduling performance, but they were not intended to create a realistic scenario for constant experimentation.</p>\n\n<p>The <a href=\"https://github.com/hashicorp-forge/nomad-bench\">nomad-bench project</a> set out to create reusable infrastructure automation to run test scenarios to collect metrics and data from Nomad clusters running at scale. The core goal of this effort is to create reproducible, large-scale test scenarios so users can better understand how Nomad scales and uncover problems detected only in large cluster deployments.</p>\n\n<h2>nomad-bench components</h2>\n\n<p>The nomad-bench infrastructure consists of two main components. The <em>core cluster</em> is a long-lived, production-ready core Nomad cluster used to run base services and drive test cases. One important service running in the core cluster is an <a href=\"https://www.influxdata.com/\">InfluxDB</a> instance that collects real-time data from test runs.</p>\n\n<p>The <em>test cluster</em> is a short-lived ephemeral cluster running Nomad servers on Amazon EC2 instances and Nomad clients using <code><a href=\"https://github.com/hashicorp-forge/nomad-nodesim\">nomad-nodesim</a></code>, allowing clusters to scale to tens of thousands of nodes. Each test cluster can have a different number of servers, EC2 instance type, disk performance, and operating system. Test clusters may also be configured with a custom binary to easily test and compare code changes.</p>\n<img src=\"https://www.datocms-assets.com/2885/1721148187-nomad-bench-test-clusters.png\" alt=\"test\" /><h3>Data collection</h3>\n\n<p>To collect and analyze data from tests, each cluster has an associated InfluxDB bucket to isolate its data. InfluxDB also allows for real-time data analysis to monitor test progress via dashboards. Data is collected using <a href=\"https://www.influxdata.com/time-series-platform/telegraf/\">Telegraf</a> daemons deployed on all test cluster servers. In addition to Nomad metrics and logs, these daemons collect system metrics, such as <a href=\"https://github.com/influxdata/telegraf/blob/master/plugins/inputs/cpu/README.md#cpu-input-plugin\">CPU</a>, <a href=\"https://github.com/influxdata/telegraf/blob/master/plugins/inputs/mem/README.md#memory-input-plugin\">memory</a>, and <a href=\"https://github.com/influxdata/telegraf/blob/master/plugins/inputs/diskio/README.md#diskio-input-plugin\">disk IO</a>.</p>\n\n<p>We chose InfluxDB over Prometheus, Grafana, and other tools due to its ability to easily load existing data via the <a href=\"https://docs.influxdata.com/influxdb/cloud/reference/syntax/line-protocol/\">plain-text line protocol format</a>, allow data isolation in buckets, and deploy as a single binary.</p>\n\n<h3>nomad-nodesim</h3>\n\n<p><code>nomad-nodesim</code> is a lightweight, virtualized Nomad client wrapper that can simulate and run hundreds of processes per application instance. This helps simulate and register tens of thousands of Nomad clients in a single test cluster, without having to stand up tens of thousands of real hosts. The clients can have different configurations, such as being partitioned in different <a href=\"https://developer.hashicorp.com/nomad/docs/configuration#datacenter\">datacenters</a> or <a href=\"https://developer.hashicorp.com/nomad/docs/configuration/client#node_pool\">node pools</a>, or holding different <a href=\"https://developer.hashicorp.com/nomad/docs/configuration/client#meta\">metadata</a> values.</p>\n\n<p>These <code>nomad-nodesim</code> processes are deployed to the core Nomad cluster so they run within the same private network. Each test cluster has its own <code>nomad-nodesim</code> job that can be customized for the scenario being tested. </p>\n\n<p>The application simplifies deployment and eases financial headaches when attempting to run large scales of Nomad clients. The calculations below roughly estimate the cost of running 3 Nomad servers and 10,000 Nomad clients for a month.</p>\n\n<ul>\n<li>Without <code>nomad-nodesim</code>: 3 x t3.medium and 10,000 x t3.nano = <strong>$25,617 (EC2 only)</strong></li>\n<li>With <code>nomad-nodesim</code>: 13 x t3.medium = <strong>$268 (EC2 only)</strong></li>\n</ul>\n\n<h3>Validation</h3>\n\n<p>To have confidence in running Nomad server load and stress tests using<code>nomad-nodesim</code>, the team needed to validate that running<code>nomad-nodesim</code>  closely mimics “real” clients. To do this, we ran two experiment variations, one with real Nomad clients running on dedicated hosts, the other with <code>nodesim</code> clients. Both ran three dedicated hosts for the Nomad server process and five clients.</p>\n\n<p>Each experiment ran through a simple set of steps:</p>\n\n<ul>\n<li>Register a job using the mock driver with a single task group whose count is 100</li>\n<li>Update the task group resources to force a destructive update</li>\n<li>Deregister the job</li>\n</ul>\n\n<p>The job specification that was initially registered is detailed below. It used an HCL variable to control the task memory resource assignment, which madescripting of updates easier as no manipulation of the specification is required.</p>\n<pre><code>variable \"resource_memory\" { default = 10 }\n\njob \"mock\" {\n  update {\n    max_parallel = 25\n  }\n  group \"mock\" {\n    count = 100\n    task \"mock\" {\n      driver = \"mock_driver\"\n      config {\n        run_for = \"24h\"\n      }\n      resources {\n        cpu    = 1\n        memory = var.resource_memory\n      }\n    }\n  }</code></pre><p>The script below is used to run the experiment in a controlled and repeatable manner, pausing after each step to allow system stabilization. </p>\n<pre><code>#!/usr/bin/env bash\n\nset -e -x\n\nsleep 90\nnomad run mock.nomad.hcl\nsleep 90\nnomad run -var='resource_memory=11' mock.nomad.hcl\nsleep 90\nnomad stop mock\nsleep 90\nnomad system gc</code></pre><p>Here’s the resulting <code>count(nomad.client.update_status)</code> charts:</p>\n<img src=\"https://www.datocms-assets.com/2885/1721148454-graph1.png\" alt=\"count(nomad.client.update_status)\" /><p>And the resulting <code>count(nomad.client.update_alloc)</code> charts:</p>\n<img src=\"https://www.datocms-assets.com/2885/1721148520-graph2.png\" alt=\"count(nomad.client.update_alloc)\" /><p>And the <code>count(nomad.client.get_client_allocs)</code> chart:</p>\n<img src=\"https://www.datocms-assets.com/2885/1721148693-graph3.png\" alt=\"count(nomad.client.get_client_allocs)\" /><h2>nomad-bench results and next steps</h2>\n\n<p>With this validation experiment, we confirmed that the <code>nomad-nodesim</code> application performs similarly to real Nomad clients. It updates Nomad servers of allocation updates slightly faster because it does not have a task-runner or driver implementation. In some cases, this may also cause updates to be batched slightly more efficiently than real clients.</p>\n\n<p>In order to account for this minor difference and to allow for more flexible testing, we added configuration functionality within <a href=\"https://github.com/hashicorp-forge/nomad-nodesim/pull/24\">PR #24</a> to allow running <code>nomad-nodesim</code> with either a simulated or real allocation runner implementation.</p>\n\n<h2>Try it yourself</h2>\n\n<p>The Nomad benchmarking infrastructure and <code>nomad-nodesim</code> application provide an excellent base for running repeatable and large scale tests. They allows engineers and users to test Nomad at scale and iterate on changes to identify throughput improvements. The Nomad engineering team uses this to run a persistent cluster for soak testing and short-lived clusters to test code changes.</p>\n\n<p>If you want to check out and run the <code>nomad-bench</code> infrastructure suite, you can do this using the <a href=\"https://github.com/hashicorp-forge/nomad-bench\">publicly available repository</a>. Instructions on how to get started are included and all feedback is welcome.</p>\n","author":"Piotr Kazmierczak","siteTitle":"HashiCorp Blog","siteHash":"219aa6310b3388f2335eba49871f4df9581f2c58eaeb5e498363b54e835b7001","entryHash":"5eec3feb52fc1e045c2bd55a8f1f0a5a661fa9eaed3d504b09a45b57ac9f636f","category":"Tech"}