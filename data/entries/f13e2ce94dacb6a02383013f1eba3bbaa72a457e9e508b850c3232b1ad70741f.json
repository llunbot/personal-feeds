{"title":"Apple details reasons to abandon CSAM-scanning tool, more controversy ensues","link":"https://arstechnica.com/?p=1964978","date":1693650805000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2023/08/GettyImages-1462322900-800x533.jpg\" alt=\"Apple logo obscured by foliage\" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2023/08/GettyImages-1462322900-scaled.jpg\">Enlarge</a> (credit: <a href=\"https://www.gettyimages.com/detail/news-photo/apple-logo-is-displayed-in-the-apple-store-on-february-03-news-photo/1462322900\">Leonardo Munoz/Getty</a>)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>In December, Apple said that it was <a href=\"https://www.wired.com/story/apple-photo-scanning-csam-communication-safety-messages/\">killing an effort</a> to design a <a href=\"https://www.wired.com/story/apple-csam-detection-icloud-photos-encryption-privacy/\">privacy-preserving iCloud photo scanning tool</a> for detecting child sexual abuse material (CSAM) on the platform. Originally announced in August 2021, the project had been controversial since its inception. Apple first paused it that September in response to concerns from digital rights groups and researchers that such a tool would inevitably be abused and exploited to compromise the privacy and security of all iCloud users. This week, a new child safety group known as Heat Initiative told Apple that it is organizing a campaign to demand that the company “detect, report, and remove” child sexual abuse material from iCloud and offer more tools for users to report CSAM to the company. </p><div><img src=\"https://cdn.arstechnica.net/wp-content/uploads/2019/02/wired-logo.png\" /></div>\n<p>Today, in a rare move, Apple responded to Heat Initiative, outlining its reasons for abandoning the development of its iCloud CSAM scanning feature and instead focusing on a <a href=\"https://www.wired.com/story/apple-photo-scanning-csam-communication-safety-messages/\">set of on-device tools and resources for users</a> known collectively as “Communication Safety” features. The company's response to Heat Initiative, which Apple shared with WIRED this morning, offers a rare look not just at its rationale for pivoting to Communication Safety, but at its broader views on creating mechanisms to circumvent user privacy protections, such as encryption, to monitor data. This stance is relevant to the encryption debate more broadly, especially as countries like the United Kingdom weigh passing laws that would require tech companies to be able to access user data to comply with law enforcement requests.</p>\n<p>“Child sexual abuse material is abhorrent and we are committed to breaking the chain of coercion and influence that makes children susceptible to it,” Erik Neuenschwander, Apple's director of user privacy and child safety, wrote in the company's response to Heat Initiative. He added, though, that after collaborating with an array of privacy and security researchers, digital rights groups, and child safety advocates, the company concluded that it could not proceed with development of a CSAM-scanning mechanism, even one built specifically to preserve privacy.</p></div><p><a href=\"https://arstechnica.com/?p=1964978#p3\">Read 9 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=1964978&amp;comments=1\">Comments</a></p>","author":"WIRED","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"f13e2ce94dacb6a02383013f1eba3bbaa72a457e9e508b850c3232b1ad70741f","category":"Tech"}