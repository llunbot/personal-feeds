{"title":"Everybody’s talking about Mistral, an upstart French challenger to OpenAI","link":"https://arstechnica.com/?p=1989986","date":1702412101000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2023/12/french_robot_1-800x450.jpg\" alt=\"An illustrated robot holding a French flag.\" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2023/12/french_robot_1.jpg\">Enlarge</a> <span>/</span> An illustration of a robot holding a French flag, figuratively reflecting the rise of AI in France due to Mistral. It's hard to draw a picture of an LLM, so a robot will have to do. (credit: <a href=\"https://www.gettyimages.com/detail/photo/android-robot-standing-with-flag-of-france-isolated-royalty-free-image/815048394\">Getty Images</a>)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>On Monday, Mistral AI <a href=\"https://mistral.ai/news/mixtral-of-experts/\">announced</a> a new AI language model called Mixtral 8x7B, a \"mixture of experts\" (MoE) model with open weights that reportedly truly matches OpenAI's <a href=\"https://arstechnica.com/information-technology/2022/12/openai-invites-everyone-to-test-new-ai-powered-chatbot-with-amusing-results/\">GPT-3.5</a> in performance—an achievement that has been claimed by others in the past but is being taken seriously by AI heavyweights such as OpenAI's <a href=\"https://x.com/karpathy/status/1734251375163511203?s=20\">Andrej Karpathy</a> and <a href=\"https://x.com/DrJimFan/status/1734269362100437315?s=20\">Jim Fan</a>. That means we're closer to having a ChatGPT-3.5-level AI assistant that can run freely and locally on our devices, given the right implementation.</p>\n\n<p>Mistral, <a href=\"https://mistral.ai/\">based in Paris</a> and founded by Arthur Mensch, Guillaume Lample, and Timothée Lacroix, has seen a rapid rise in the AI space recently. It has been quickly <a href=\"https://www.bloomberg.com/news/articles/2023-12-04/openai-rival-mistral-nears-2-billion-valuation-with-nvidia-funding?sref=gni836kR\">raising venture capital</a> to become a sort of French anti-OpenAI, championing smaller models with eye-catching performance. Most notably, Mistral's models run locally with open weights that can be downloaded and used with fewer restrictions than closed AI models from OpenAI, Anthropic, or Google. (In this context \"weights\" are the computer files that represent a trained neural network.)</p>\n<p>Mixtral 8x7B can process a 32K token context window and works in French, German, Spanish, Italian, and English. It works much like ChatGPT in that it can assist with compositional tasks, analyze data, troubleshoot software, and write programs. Mistral claims that it outperforms Meta's much larger <a href=\"https://arstechnica.com/information-technology/2023/07/meta-launches-llama-2-an-open-source-ai-model-that-allows-commercial-applications/\">LLaMA 2 70B</a> (70 billion parameter) <a href=\"https://arstechnica.com/science/2023/07/a-jargon-free-explanation-of-how-ai-large-language-models-work/\">large language model</a> and that it matches or exceeds OpenAI's GPT-3.5 on certain benchmarks, as seen in the chart below.</p></div><p><a href=\"https://arstechnica.com/?p=1989986#p3\">Read 6 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=1989986&amp;comments=1\">Comments</a></p>","author":"Benj Edwards","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"c4ed8e0dce369748561ad175ed55e5a48eecf8ed91dabc4dcd1d8a601931ebe5","category":"Tech"}