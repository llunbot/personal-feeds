{"title":"Mamba vs Transformer","link":"https://markpeak.net/mamba-vs-transformer/","date":1762004683000,"content":"<p>ถ้าถามว่าอะไรคือสิ่งที่ทำให้ผมตื่นเต้นที่สุดกับวงการ AI ในช่วงครึ่งหลังของปีนี้ คำตอบง่ายมากคือกระแสของสถาปัตยกรรมใหม่ Mamba ที่กำลังเริ่มมาแรง และอาจขึ้นมาท้าทาย Transformer ได้ด้วย</p>\n<h1>จุดอ่อนของ Transformer</h1>\n<p>โมเดลภาษาที่เราใช้กันอยู่ในทุกวันนี้ ล้วนพัฒนามาจากสถาปัตยกรรมโมเดล Transformer ที่คิดขึ้นโดยกูเกิลในปี 2017 (ตัว T ในคำว่า GPT มาจากคำว่า Transformer) จุดเด่นของโมเดลตระกูล Transformer คือกลไกที่เรียกว่า self-attention (ตามชื่อเปเปอร์ Attention Is All You Need) ที่ทำให้อ่านคำ (token) แบบย้อนหลังได้ ช่วยให้คุณภาพของผลลัพธ์ที่โมเดลพยากรณ์คำออกมาดีกว่าโมเดลรุ่นก่อนๆ หน้าอย่างก้าวกระโดด</p>\n<p>อย่างไรก็ตาม ข้อเสียของ Transformer คือวิธีการประมวลผลของ self-attention มันกินพลังเยอะ ต้องจับคู่ token ทั้งหมดมาเทียบกัน ถ้าพูดเป็นภาษาคณิตศาสตร์คือเป็นสมการยกกำลังสอง (quadratic) หรือถ้าเรียกเป็น Big O คือ O(N^2) ยิ่งถ้าต้องเจอกับ input sequence ขนาดยาวมากๆ ก็ยิ่งสิ้นเปลืองพลังประมวลผลมากสุดๆ ไปเลย</p>\n<p>หลังการเกิดขึ้นของ Transformer จึงมีเทคนิคต่างๆ พยายามแก้จุดอ่อนเรื่อง quadratic ของมัน (ซึ่งจะไม่กล่าวถึงในที่นี้) ในอีกทางก็มีคนพยายามคิดสถาปัตยกรรมโมเดลใหม่ๆ มาแข่งกับ Transformer ซึ่งผ่านมาหลายปี ดูเหมือนว่า Mamba จะเข้าเค้ามากที่สุดแล้ว</p>\n<h1>State Space Model (SSM)</h1>\n<p>สถาปัตยกรรม Mamba ถูกเสนอขึ้นมาใน<a href=\"https://arxiv.org/abs/2312.00752\">เปเปอร์</a> ปี 2023 โดยนักวิจัย 2 คนคือ Albert Gu (CMU) และ Tri Dao (Princeton) แต่แนวคิดของมันพัฒนามาก่อนหน้านั้นอย่างยาวนาน เพราะมันอิงอยู่บนโมเดลคณิตศาสตร์ที่เรียกว่า <strong>State Space Model (SSM)</strong> ที่ใช้กันมานานแล้วในวงการวิศวกรรมควบคุม-ประมวลผลสัญญาณ</p>\n<p>อธิบายแบบเข้าใจง่ายที่สุดเท่าที่ทำได้ State Space Model (SSM) เป็นวิธีการเทียบสถานะ (state representation) จากโลกจริงมาเป็นโมเดลคณิตศาสตร์แบบหนึ่ง หลักคิดสำคัญของมันคือ วัตถุใดๆ จะเปลี่ยนแปลงจากปัจจัย 2 อย่างหลักคือ ตัวมันเปลี่ยนสถานะด้วยตัวเอง และ มีอินพุตภายนอกเข้ามาเอี่ยวด้วย เมื่อเราได้ข้อมูลทั้ง 2 อย่างแล้ว เราจะสามารถ “ทำนาย” สถานะขั้นถัดไปของวัตถุนั้นได้</p>\n<p>เห็นคำว่า “ทำนาย” ไหมครับ นี่แหละเราจะเอา State Space Model มาใช้ทำนายคำ (token) ของโมเดลภาษากัน</p>\n<div><img loading=\"lazy\" src=\"https://markpeak.net/wp-content/uploads/2025/11/state-space-model-1024x644.webp\" alt=\"\" width=\"700\" height=\"440\" srcset=\"https://markpeak.net/wp-content/uploads/2025/11/state-space-model-1024x644.webp 1024w, https://markpeak.net/wp-content/uploads/2025/11/state-space-model-300x189.webp 300w, https://markpeak.net/wp-content/uploads/2025/11/state-space-model-768x483.webp 768w, https://markpeak.net/wp-content/uploads/2025/11/state-space-model-1536x966.webp 1536w, https://markpeak.net/wp-content/uploads/2025/11/state-space-model-700x440.webp 700w, https://markpeak.net/wp-content/uploads/2025/11/state-space-model.webp 1600w\" /><p>แนวคิดหลักของ State Space Model – ภาพจาก <a href=\"https://www.ibm.com/think/topics/mamba-model\">IBM</a></p></div>\n<p>State Space Model ถูกใช้ในวงการวิศวกรรมควบคุม วิศวกรรมไฟฟ้า มานานมากแล้ว เดิมทีมันถูกคิดขึ้นมาสำหรับประมวลผลสัญญาณที่มีความต่อเนื่อง (continuous sequence) เช่น คลื่นไฟฟ้า สภาพอากาศ</p>\n<p>แต่ภายหลังก็มีคนพยายามนำมันมาใช้กับข้อมูลที่ไม่ต่อเนื่อง (discrete sequence) อย่างโมเดลภาษาที่ทำนายคำออกมาทีละคำ โดยใช้วิธีแปลงอินพุตแบบ discrete เป็น continuous แล้วเอาไปเข้าโมเดล SSM พอได้เอาท์พุตออกมาก็แปลงกลับเป็น discrete ใหม่ วิธีการนี้เรียกว่า Discrete SSM</p>\n<p>การทำงานของ Discrete SSM คล้ายกับแนวทาง recurrent neural networks (RNNs) ที่เคยนิยมในวงการ AI ยุคก่อนหน้า Transformer ซึ่งมีจุดเด่นตรงที่มันประมวลผลตอนรัน (inference) แบบเชิงเส้น (linear) ได้ออกมาเป็น O(N logN) ที่ใช้พลังประมวลผลน้อยกว่า Transformer มาก</p>\n<p>อย่างไรก็ตาม Discrete SSM ยังมีข้อจำกัดเรื่องประสิทธิภาพหลายอย่างที่ผลลัพธ์ออกมาสู้ Transformer ไม่ได้ ในวงการวิจัยจึงพยายามพัฒนา Discrete SSM ต่อ</p>\n<h1>Structure State Space Sequence Model (S4)</h1>\n<p>ในปี 2021 Albert Gu ผู้คิดค้นโมเดล Mamba นี่ล่ะ ได้เสนอแนวทางพัฒนาที่เรียกว่า <a href=\"https://arxiv.org/abs/2111.00396\">Structure State Space Sequence Model</a> (หรือย่อว่า S4 เพราะมันมี S สี่ตัว) ถือเป็นเวอร์ชันต้นแบบที่จะกลายเป็น Mamba ในภายหลัง</p>\n<p>ข้อดีของโมเดลแบบ RNN (ซึ่งครอบคลุมถึง SSM) คือประมวลผลแบบเชิงเส้น ประมวลผลตามลำดับ (linear) ทำให้ไม่สิ้นเปลืองพลังประมวลผลตอนรัน แต่มันก็กลายเป็นข้อเสียในตอนเทรนโมเดล เพราะมันต้องทำตามลำดับ ประมวลผลแบบขนานไม่ได้ กลายเป็นว่าเทรนช้ามาก</p>\n<p>สถาปัตยกรรม S4 ของ Albert Gu แก้ปัญหาความช้าในการเทรนโมเดลของ SSM โดยนำแนวคิดจากสายตรงข้ามกับ RNN คือ convolutional neural networks (CNNs ซึ่งนิยมใช้กันในสายประมวลผลภาพ computer vision) มาใช้ตอนเทรน นำเทคนิคที่เรียกว่า kernel มาแปลงข้อมูลตอนเทรนให้ทำงานได้เร็วขึ้น</p>\n<p>นอกจากนี้ S4 ยังแก้ปัญหาอีกอย่างของ SSM คือการประมวลผลข้อมูลที่ยาวๆ แล้วความเชื่อมโยงระหว่างข้อมูลส่วนต้นกับส่วนปลายมันจะหายไป (ในวงการ AI เรียกปัญหานี้ว่า long-term memory) ผลลัพธ์ที่ได้ออกมาไม่แม่น แนวทางแก้ปัญหานี้เรียกว่า HiPPO (ย่อมาจาก High-order Polynomial Projection Operators) ซึ่งอธิบายสั้นๆ ตรงนี้พอว่าเป็นการทำ structuring หรือกำหนดข้อมูลในเมทริกซ์ที่นำมาประมวลผล (เป็นเหตุผลว่าทำไมชื่อของโมเดล S4 มีคำว่า Structure เพิ่มเข้ามา)</p>\n<blockquote>\n<p>Transformers power most advances in LLMs, but its core attention layer can’t scale to long context.</p>\n<p>With <a href=\"https://twitter.com/_albertgu?ref_src=twsrc%5Etfw\">@_albertgu</a>, we’re releasing Mamba, an SSM architecture that matches/beats Transformers in language modeling, yet with linear scaling and 5x higher inference throughput.<br />\n1/ <a href=\"https://t.co/xnWtFVFthS\">https://t.co/xnWtFVFthS</a> <a href=\"https://t.co/7gdXw1qP2H\">pic.twitter.com/7gdXw1qP2H</a></p>\n<p>— Tri Dao (@tri_dao) <a href=\"https://twitter.com/tri_dao/status/1731728602230890895?ref_src=twsrc%5Etfw\">December 4, 2023</a></p></blockquote>\n<p></p>\n<h1>Mamba</h1>\n<p>สถาปัตยกรรม S4 สามารถแก้ปัญหาความเร็วในการเทรน และความสามารถในการประมวลผลข้อความยาวๆ ได้แล้ว หลังจากนั้น ทีมของ Albert Gu ยังพัฒนาโมเดล S4 ต่อในแง่มุมอื่นเพื่อให้ทัดเทียมกับ Transformer จนออกมาเป็น Mamba ในปี 2023</p>\n<p>ในวงการ LLM พยากรณ์คำ มีบางครั้งที่เราต้องการให้โมเดลคัดลอกอินพุตเฉพาะแค่บางคำออกมาเป็นเอาท์พุต (ตัดบางส่วนออก แต่ต้องตอบแบบเรียงตามลำดับเดิม) ซึ่งกลไก attention ของ  Transformer สามารถทำได้สบาย แต่วิธีทำงานของ SSM ที่รับอินพุตเป็นเชิงเส้นไม่สามารถทำได้</p>\n<p>Gu แก้ปัญหานี้โดยกลไกที่เรียกว่า Selective Scan ปรับวิธีการบีบอัดข้อมูลอินพุตให้กรองข้อมูลบางอย่างออกได้ ส่งผลให้ความสามารถเรื่องนี้เทียบเท่ากับ Transformer โดยประสิทธิภาพดีกว่า</p>\n<div><img loading=\"lazy\" src=\"https://markpeak.net/wp-content/uploads/2025/11/mamba-selective-scan-1024x476.webp\" alt=\"\" width=\"700\" height=\"325\" srcset=\"https://markpeak.net/wp-content/uploads/2025/11/mamba-selective-scan-1024x476.webp 1024w, https://markpeak.net/wp-content/uploads/2025/11/mamba-selective-scan-300x139.webp 300w, https://markpeak.net/wp-content/uploads/2025/11/mamba-selective-scan-768x357.webp 768w, https://markpeak.net/wp-content/uploads/2025/11/mamba-selective-scan-700x325.webp 700w, https://markpeak.net/wp-content/uploads/2025/11/mamba-selective-scan.webp 1183w\" /><p>Selective Scan – ภาพจาก <a href=\"https://www.youtube.com/watch?v=BDTVVlUU1Ck\">Maarten Grootendorst</a></p></div>\n<p>การที่โมเดล S4 เพิ่มท่า Selective Scan เข้ามา ทำให้ตอนนี้ชื่อมันยาวขึ้นเป็น S6 ซึ่งชักจะยาวเกินไปแล้วนะ</p>\n<p>Albert Gu เลยตั้งชื่อใหม่ให้มันว่า Mamba ตามสายพันธุ์งูพิษพันธุ์ดุ ด้วยเหตุผลว่าการออกเสียง S เยอะๆ 6 ตัวเหมือนกับเสียงงูกำลังขู่ บวกกับ Mamba รวดเร็วและดุร้าย (fast &amp; deadly) เหมือนกับงูจริงๆ (มันเป็นแบบนี้นี่เอง)</p>\n<blockquote>\n<p>Why “Mamba”?</p>\n<p>– It’s fast: based on a (i) simple recurrence with linear scaling in sequence length, and (ii) hardware-aware design and implementation<br />\n– It’s deadly — to sequence modeling problems<br />\n– Its core mechanism is the latest evolution of S4 models… SSSS<br />\n8/</p>\n<p>— Albert Gu (@_albertgu) <a href=\"https://twitter.com/_albertgu/status/1731727694809723364?ref_src=twsrc%5Etfw\">December 4, 2023</a></p></blockquote>\n<p></p>\n<p>นอกจากเรื่อง Selective Scan แล้ว Mamba ยังมีของใหม่เพิ่มมาจาก S4 อีกอย่าง คือ การออกแบบให้คำนวณบนฮาร์ดแวร์ (GPU) ของจริงได้มีประสิทธิภาพมากขึ้นด้วย (ไม่ได้เป็นแค่โมเดลคณิตศาสตร์) โดยอาศัยข้อจำกัดของ GPU ที่มี SRAM (เร็ว) และ DRAM (ช้ากว่า) แล้วต้องโอนถ่ายข้อมูลไปมาระหว่างแรม 2 แบบนี้ การโอนถ่ายถือเป็นคอขวดสำคัญของ GPU (compute เร็วกว่า data transfer มาก) สถาปัตยกรรม Mamba จึงเลือกเก็บงานบางอย่างทำบน SRAM เท่านั้น และงานบางอย่างทำบน DRAM เท่านั้น ไม่ข้ามกลับไปกลับมาบ่อยๆ ผลลัพธ์สุดท้ายจึงกลายเป็นว่า Mamba ตอนไปรันบน GPU จริงๆ จึงเร็วขึ้นมาก</p>\n<div><img loading=\"lazy\" src=\"https://markpeak.net/wp-content/uploads/2025/11/mamba-sram-dram-1024x511.png\" alt=\"\" width=\"700\" height=\"349\" srcset=\"https://markpeak.net/wp-content/uploads/2025/11/mamba-sram-dram-1024x511.png 1024w, https://markpeak.net/wp-content/uploads/2025/11/mamba-sram-dram-300x150.png 300w, https://markpeak.net/wp-content/uploads/2025/11/mamba-sram-dram-768x383.png 768w, https://markpeak.net/wp-content/uploads/2025/11/mamba-sram-dram-700x350.png 700w, https://markpeak.net/wp-content/uploads/2025/11/mamba-sram-dram.png 1432w\" /><p>ภาพการออกแบบการทำงานบน SRAM (สีส้ม) และ DRAM (สีเขียว) จากเปเปอร์ Mamba</p></div>\n<p>สถาปัตยกรรม Mamba ที่แก้ปัญหาต่างๆ ของ SSM ไปมากแล้ว สามารถทำผลลัพธ์ออกมาได้ดีไม่แพ้ Transformer เลย (แม้ยังมีจุดอ่อนบางอย่างเหลืออยู่บ้าง) มันจึงส่งผลสะเทือนต่อวงการ LLM เป็นอย่างมาก ทำให้ Mamba กลายเป็นจุดสนใจและถูกนำไปพัฒนาต่อยอดอีกเยอะ</p>\n<p>ทีมของ Gu และ Dao ยังออก <a href=\"https://arxiv.org/abs/2405.21060\">เปเปอร์ใหม่ในปี 2024</a> พัฒนาเป็นสถาปัตยกรรม Mamba-2 ที่ทำงานได้เร็วขึ้นอีกขั้น ด้วยเทคนิคใหม่ที่เรียกว่า state space duality (SSD) นำแนวคิดเรื่อง multi-head attention ของ Transformer มาใช้งาน สามารถประมวลผลแบบขนานได้เร็วขึ้น</p>\n<div><img loading=\"lazy\" src=\"https://markpeak.net/wp-content/uploads/2025/11/mamba2-1024x1024.png\" alt=\"\" width=\"700\" height=\"700\" srcset=\"https://markpeak.net/wp-content/uploads/2025/11/mamba2-1024x1024.png 1024w, https://markpeak.net/wp-content/uploads/2025/11/mamba2-300x300.png 300w, https://markpeak.net/wp-content/uploads/2025/11/mamba2-150x150.png 150w, https://markpeak.net/wp-content/uploads/2025/11/mamba2-768x768.png 768w, https://markpeak.net/wp-content/uploads/2025/11/mamba2-700x700.png 700w, https://markpeak.net/wp-content/uploads/2025/11/mamba2.png 1432w\" /><p>สถาปัตยกรรม Mamba 2 ร่างล่าสุด – ภาพจาก <a href=\"https://www.ibm.com/think/topics/mamba-model\">IBM</a></p></div>\n<h1>Mamba vs Transformer</h1>\n<p>หลังจากโมเดลตระกูล SSM/Mamba พัฒนาอย่างก้าวกระโดดในช่วงหลัง ทำผลงานได้ดีไม่แพ้ Transformer ก็เริ่มมีคำถามว่าตกลงแล้ว Mamba จะมาโค่น Transformer หรือเปล่า</p>\n<p>คำตอบดูเหมือนจะไปทางตรงข้ามแทน นั่นคือ มันกำลังหลอมรวมเข้าหากันเป็นโมเดลแบบไฮบริด ที่มีทั้ง Mamba และ Transformer เพื่อชดเชยจุดอ่อนซึ่งกันและกัน โดย Transformer ยังให้ผลลัพธ์เหนือกว่า ในขณะที่ Mamba ทำงานเร็วกว่า</p>\n<p><a href=\"https://markpeak.net/how-transformer-works/\">ใครที่เคยศึกษา Transformer</a> คงพอทราบว่ากลไก Attention มันทำงานเป็น “บล็อค” แล้วนำมาต่อๆ กันเป็นหลาย “เลเยอร์” เพื่อเพิ่มขีดความสามารถของการประมวลผลคำให้เยอะขึ้น ด้วยโครงสร้างแบบนี้จึงมีคนเกิดไอเดียเอาบล็อค Mamba มาต่อกับบล็อค Attention นั่นเอง</p>\n<div><img loading=\"lazy\" src=\"https://markpeak.net/wp-content/uploads/2025/11/mamba-transformer-ibm.webp\" alt=\"\" width=\"1000\" height=\"1000\" srcset=\"https://markpeak.net/wp-content/uploads/2025/11/mamba-transformer-ibm.webp 1000w, https://markpeak.net/wp-content/uploads/2025/11/mamba-transformer-ibm-300x300.webp 300w, https://markpeak.net/wp-content/uploads/2025/11/mamba-transformer-ibm-150x150.webp 150w, https://markpeak.net/wp-content/uploads/2025/11/mamba-transformer-ibm-768x768.webp 768w, https://markpeak.net/wp-content/uploads/2025/11/mamba-transformer-ibm-700x700.webp 700w\" /><p>สถาปัตยกรรมไฮบริด Mamba + Attention ของ IBM Granite 4.0</p></div>\n<p>เท่าที่หาข้อมูลได้ มีอยู่ 2 ค่ายใหญ่ๆ ที่พัฒนาโมเดลตามแนวทางไฮบริด คือ</p>\n<ul>\n<li><a href=\"https://www.ai21.com/jamba/\">Jamba</a> จากสตาร์ตอัพชื่อ AI24 ของอิสราเอล (น่าจะล้อจากคำว่า Mumbo Jumbo มาเป็น Mamba Jamba) โดยค่ายนี้ได้รับการสนับสนุนจาก Andrew Ng จนมี <a href=\"https://www.deeplearning.ai/short-courses/build-long-context-ai-apps-with-jamba/\">คอร์สสอน Jamba บน DeepLearning.ai</a> ด้วย</li>\n<li><a href=\"https://huggingface.co/ibm-ai-platform/Bamba-9B-v1\">Bamba</a> จากยักษ์สีฟ้า IBM ที่ร่วมกับ Dao และ Gu พัฒนา Mamba ต่อแบบไฮบริด ร่างล่าสุดคือถูกนำไปใช้งานใน <a href=\"https://www.ibm.com/new/announcements/ibm-granite-4-0-hyper-efficient-high-performance-hybrid-models\">โมเดลเชิงพาณิชย์ Granite 4.0</a> ของ IBM แล้วด้วย</li>\n</ul>\n<div><img loading=\"lazy\" src=\"https://markpeak.net/wp-content/uploads/2025/11/jamba-hybrid.webp\" alt=\"\" width=\"1140\" height=\"682\" srcset=\"https://markpeak.net/wp-content/uploads/2025/11/jamba-hybrid.webp 1140w, https://markpeak.net/wp-content/uploads/2025/11/jamba-hybrid-300x179.webp 300w, https://markpeak.net/wp-content/uploads/2025/11/jamba-hybrid-1024x613.webp 1024w, https://markpeak.net/wp-content/uploads/2025/11/jamba-hybrid-768x459.webp 768w, https://markpeak.net/wp-content/uploads/2025/11/jamba-hybrid-700x419.webp 700w\" /><p>สถาปัตยกรรมไฮบริดของ Jamba</p></div>\n<p><img loading=\"lazy\" src=\"https://markpeak.net/wp-content/uploads/2025/11/jamba-1024x875.webp\" alt=\"\" width=\"700\" height=\"598\" srcset=\"https://markpeak.net/wp-content/uploads/2025/11/jamba-1024x875.webp 1024w, https://markpeak.net/wp-content/uploads/2025/11/jamba-300x256.webp 300w, https://markpeak.net/wp-content/uploads/2025/11/jamba-768x656.webp 768w, https://markpeak.net/wp-content/uploads/2025/11/jamba-700x598.webp 700w, https://markpeak.net/wp-content/uploads/2025/11/jamba.webp 1179w\" /></p>\n<p>โพสต์เชียร์จาก Andrew Ng</p>\n<blockquote>\n<p>New short course: Build Long-Context AI Apps with Jamba. Learn about state space models (SSMs), which have emerged as an alternative to transformers! Specifically, Jamba is a hybrid transformer-Mamba architecture that combines strengths of the transformer with ideas from SSMs.… <a href=\"https://t.co/JqtPVsxane\">pic.twitter.com/JqtPVsxane</a></p>\n<p>— Andrew Ng (@AndrewYNg) <a href=\"https://twitter.com/AndrewYNg/status/1877075439283482815?ref_src=twsrc%5Etfw\">January 8, 2025</a></p></blockquote>\n<p></p>\n<p>เท่าที่ผมหาข้อมูลล่าสุด ณ เวลาที่เขียนโพสต์นี้ โมเดลไฮบริดอย่าง Granite 4.0 ทำงานได้เร็วจริง กินแรมน้อยกว่าโมเดลตระกูล Transformer มาก ส่วนประสิทธิภาพในภาพรวมนั้น IBM ยังปล่อยมาเฉพาะ Granite-4.0-Small ตัวเล็ก (32B) ที่เอาชนะโมเดลสาย open weight ตัวใหญ่กว่าอย่าง Llama 4 Maverick (402B), GPT OSS (120B) ได้แล้ว ก็ต้องรอดูกันต่อไปว่าหาก Granite-4.0 ตัวใหญ่กว่านี้ออกมา ผลลัพธ์มันจะสามารถไปเทียบชั้นโมเดลรุ่นท็อปๆ อย่าง GPT-5, Claude Sonnet 4.5, Gemini 2.5 Pro ได้ประมาณไหนกัน</p>\n<p><img loading=\"lazy\" src=\"https://markpeak.net/wp-content/uploads/2025/11/granite4-perf.webp\" alt=\"\" width=\"1000\" height=\"562\" srcset=\"https://markpeak.net/wp-content/uploads/2025/11/granite4-perf.webp 1000w, https://markpeak.net/wp-content/uploads/2025/11/granite4-perf-300x169.webp 300w, https://markpeak.net/wp-content/uploads/2025/11/granite4-perf-768x432.webp 768w, https://markpeak.net/wp-content/uploads/2025/11/granite4-perf-700x393.webp 700w\" /></p>\n<p>ฝั่งของบ้าน Jamba เพิ่งออก <a href=\"https://www.ai21.com/blog/introducing-jamba-reasoning-3b/\">Jamba Reasoning</a> เพิ่มฟีเจอร์คิดเป็นเหตุเป็นผลออกมา โดยยังเป็นโมเดลไซส์เล็ก 3B เน้นทำงานบนพีซีหรือสมาร์ทโฟน แต่ก็คุยว่าความฉลาดเอาชนะคู่แข่งระดับเดียวกัน เช่น Qwen 3 1.7B และ Gemma 3 4B ได้แล้ว เรื่องความเร็วนั้นไม่ต้องพูดถึงอยู่แล้วเพราะมันชนะกันที่สถาปัตยกรรม</p>\n<p><img loading=\"lazy\" src=\"https://markpeak.net/wp-content/uploads/2025/11/jamba-reasoning-1024x644.webp\" alt=\"\" width=\"700\" height=\"440\" srcset=\"https://markpeak.net/wp-content/uploads/2025/11/jamba-reasoning-1024x644.webp 1024w, https://markpeak.net/wp-content/uploads/2025/11/jamba-reasoning-300x189.webp 300w, https://markpeak.net/wp-content/uploads/2025/11/jamba-reasoning-768x483.webp 768w, https://markpeak.net/wp-content/uploads/2025/11/jamba-reasoning-700x440.webp 700w, https://markpeak.net/wp-content/uploads/2025/11/jamba-reasoning.webp 1536w\" /></p>\n<p>สำหรับผู้สนใจศึกษาเรื่อง Mamba เพิ่มเติม แนะนำให้อ่าน</p>\n<ul>\n<li><a href=\"https://www.ibm.com/think/topics/mamba-model\">บทความของ IBM อธิบายพัฒนาการของ Mamba</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=BDTVVlUU1Ck\">คลิปของ Maarten Grootendorst</a> ทำภาพอธิบาย Mamba ได้เข้าใจง่ายมากสุดๆ คุ้มค่าเวลา 24 นาทีแน่นอน</li>\n</ul>\n<p></p>The post <a href=\"https://markpeak.net/mamba-vs-transformer/\">Mamba vs Transformer</a> first appeared on <a href=\"https://markpeak.net\">markpeak.net</a>.","author":"Isriya Paireepairit","siteTitle":"markpeak.net","siteHash":"174209a41ef21fd794de2993285c799df6ec31048fd82206fb5c8fe38898acfe","entryHash":"22ae3e06732eb3fa213648864b424dda4a3cf356cd8c8f792ac8bf959c1dee5a","category":"Thai"}