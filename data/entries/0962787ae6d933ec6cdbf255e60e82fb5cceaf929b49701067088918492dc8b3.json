{"title":"แนะนำการใช้งาน Local LLM ใน VSCode แบบง่าย ๆ","link":"https://www.somkiat.cc/local-llm-in-vscode/","date":1747929525000,"content":"<p><img width=\"150\" height=\"150\" src=\"https://www.somkiat.cc/wp-content/uploads/2025/05/vscode-ollama-150x150.png\" alt=\"\" loading=\"lazy\" srcset=\"https://www.somkiat.cc/wp-content/uploads/2025/05/vscode-ollama-150x150.png 150w, https://www.somkiat.cc/wp-content/uploads/2025/05/vscode-ollama-75x75.png 75w\" /></p>\n<figure><a href=\"https://www.somkiat.cc/wp-content/uploads/2025/05/vscode-ollama.png\"><img src=\"https://www.somkiat.cc/wp-content/uploads/2025/05/vscode-ollama.png\" alt=\"\" width=\"504\" height=\"328\" /></a></figure>\n\n\n\n<p>คำถาม จากการแบ่งปันเรื่องการใช้งาน LLM มาช่วยเขียน code คือ<br /><strong>จะ config อย่างไรใน VS Code ให้มาใช้งานพวก Local LLM ได้บ้าง ?</strong><br />เช่นการใช้งาน Ollama เป็นต้น</p>\n\n\n\n<p>คำตอบที่แนะนำไปเป็นดังนี้</p>\n\n\n\n<span></span>\n\n\n\n<p><strong>ถ้าใช้งาน GitHub Copilot อยู่แล้ว</strong></p>\n\n\n\n<p>สามารถเลือก provider เพิ่มเติมด้วย Ollama ได้เลย<br />แต่ต้องทำการเพิ่ม config ของ Ollama endpoint ไปยัง server ของเราเอง</p>\n\n\n\n<ul>\n<li>github.copilot.chat.byok.ollamaEndpoint</li>\n</ul>\n\n\n\n<figure><a href=\"https://www.somkiat.cc/wp-content/uploads/2025/05/copilot-ollama.jpg\"><img src=\"https://www.somkiat.cc/wp-content/uploads/2025/05/copilot-ollama-1024x467.jpg\" alt=\"\" width=\"477\" height=\"217\" /></a></figure>\n\n\n\n<p>จากนั้นก็สามารถเลือก model ที่ทำการติดตั้งไว้ใน Ollama server<br />ยกตัวอย่างดังรูป</p>\n\n\n\n<figure><a href=\"https://www.somkiat.cc/wp-content/uploads/2025/05/Screenshot-2568-05-22-at-22.42.54.png\"><img src=\"https://www.somkiat.cc/wp-content/uploads/2025/05/Screenshot-2568-05-22-at-22.42.54-1024x265.png\" alt=\"\" width=\"615\" height=\"159\" /></a></figure>\n\n\n\n<p>ทำการเลือก model และใช้ได้เลย</p>\n\n\n\n<figure><a href=\"https://www.somkiat.cc/wp-content/uploads/2025/05/Screenshot-2568-05-22-at-22.43.17.png\"><img src=\"https://www.somkiat.cc/wp-content/uploads/2025/05/Screenshot-2568-05-22-at-22.43.17-670x1024.png\" alt=\"\" width=\"270\" height=\"412\" /></a></figure>\n\n\n\n<p>แต่ว่า server หรือ เครื่องที่ใช้ติดตั้งต้องแรงหน่อยนะครับ<br />มิเช่นนั้น ช้าเป็นเต่ากันเลย</p>\n\n\n\n<p><strong>ถ้าใช้งานผ่าน Continue ก็ง่ายเข้าไปอีก</strong></p>\n\n\n\n<p>ถ้าใช้งาน Ollama อยู่แล้ว ให้ทำการติดตั้ง extension ใน VS Code</p>\n\n\n\n<figure><a href=\"https://www.somkiat.cc/wp-content/uploads/2025/05/Screenshot-2568-05-22-at-22.46.51.png\"><img src=\"https://www.somkiat.cc/wp-content/uploads/2025/05/Screenshot-2568-05-22-at-22.46.51-1024x635.png\" alt=\"\" width=\"573\" height=\"354\" /></a></figure>\n\n\n\n<p>จากนั้นก็ config model ใน config fileได้<br />มีขั้นตอนให้เรียบร้อย</p>\n\n\n\n<figure><a href=\"https://www.somkiat.cc/wp-content/uploads/2025/05/Screenshot-2568-05-22-at-22.49.09.png\"><img src=\"https://www.somkiat.cc/wp-content/uploads/2025/05/Screenshot-2568-05-22-at-22.49.09-720x1024.png\" alt=\"\" width=\"300\" height=\"426\" /></a></figure>\n\n\n\n<p>จากนั้นก็ใช้งานแบบชิว ๆ</p>\n\n\n\n<figure><a href=\"https://www.somkiat.cc/wp-content/uploads/2025/05/continue-01.jpg\"><img src=\"https://www.somkiat.cc/wp-content/uploads/2025/05/continue-01-717x1024.jpg\" alt=\"\" width=\"299\" height=\"427\" /></a></figure>\n\n\n\n<p>ลองใช้กันดูครับ config ง่าย ๆ<br />แต่การใช้นั้น ไม่ง่าย ถ้าจะให้ได้ผลดี</p>\n\n\n\n<p>ขอให้สนุกกับการ coding ครับ !!</p>\n","author":"somkiat","siteTitle":"cc :: somkiat","siteHash":"3a23a5a4389e1e40c6fbb16520a8cc20df5b3591c25145ce72aaa18b19e48201","entryHash":"0962787ae6d933ec6cdbf255e60e82fb5cceaf929b49701067088918492dc8b3","category":"Thai"}