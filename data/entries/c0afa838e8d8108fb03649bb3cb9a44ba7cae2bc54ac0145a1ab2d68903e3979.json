{"title":"ทานกระแสไม่ไหว แอปเปิลยอมเลื่อนฟีเจอร์สแกนภาพโป๊เด็ก CSAM ออกไปก่อน","link":"https://www.blognone.com/node/124581","date":1630678468000,"content":"<div><div><div><p>เดือนที่แล้ว <a href=\"https://www.blognone.com/node/124093\">แอปเปิลประกาศแนวทางสแกนภาพโป๊เด็ก หรือที่เรียกว่า child sexual abuse material - CSAM</a> สร้างเสียงวิจารณ์ต่อบริษัทอย่างหนักในแง่ความเป็นส่วนตัว</p>\n<p>ล่าสุดแอปเปิลประกาศเลื่อนการใช้งาน CSAM ไปก่อน แต่ยังไม่ระบุกรอบเวลาที่ชัดเจน โดยให้เหตุผลว่าต้องการเวลาเพิ่มเติมเพื่อรับฟังความเห็น และพัฒนาฟีเจอร์ให้ดีขึ้นก่อนใช้งานจริง</p>\n<p>ที่มา - <a href=\"https://9to5mac.com/2021/09/03/apple-delays-csam-detection-feature/\">9to5mac</a></p>\n<p><img src=\"https://www.blognone.com/sites/default/files/externals/ba7ce76bd4e1ffeb074eaf03a4158a6a.jpg\" /></p>\n<p><img src=\"https://www.blognone.com/sites/default/files/externals/722d7bdacf4f3a2031bc154cd8a36505.jpg\" /></p>\n</div></div></div><div><div>Topics: </div><div><div><a href=\"/topics/apple\">Apple</a></div><div><a href=\"/topics/privacy\">Privacy</a></div></div></div>","author":"mk","siteTitle":"Blognone","siteHash":"ededadcf18490b3937e7dd89ebe8c00dc129addbdf1ebe4aff1f458146693da0","entryHash":"c0afa838e8d8108fb03649bb3cb9a44ba7cae2bc54ac0145a1ab2d68903e3979","category":"Thai"}