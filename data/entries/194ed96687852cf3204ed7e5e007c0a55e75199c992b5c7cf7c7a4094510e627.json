{"title":"OpenAI experiments with giving ChatGPT a long-term conversation memory","link":"https://arstechnica.com/?p=2003078","date":1707861660000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/02/chatgpt_memory_1-800x450.jpg\" alt=\"A pixelated green illustration of a pair of hands looking through file records.\" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2024/02/chatgpt_memory_1.jpg\">Enlarge</a> <span>/</span> When ChatGPT looks things up, a pair of green pixelated hands look through paper records, much like this. Just kidding. (credit: Benj Edwards / Getty Images)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>On Tuesday, OpenAI <a href=\"https://openai.com/blog/memory-and-new-controls-for-chatgpt\">announced</a> that it is experimenting with adding a form of long-term memory to ChatGPT that will allow it to remember details between conversations. You can ask ChatGPT to remember something, see what it remembers, and ask it to forget. Currently, it's only available to a small number of ChatGPT users for testing.</p>\n\n<p>So far, large language models have typically used two types of memory: one baked into the AI model during the training process (before deployment) and an in-context memory (the conversation history) that persists for the duration of your session. Usually, ChatGPT forgets what you have told it during a conversation once you start a new session.</p>\n<p>Various projects have experimented with giving LLMs a memory that persists beyond a context window. (The context window is the hard limit on the number of tokens the LLM can process at once.) The techniques include <a href=\"https://arxiv.org/abs/2310.08560\">dynamically managing context history</a>, compressing previous history through summarization, links to vector databases that store information externally, or simply periodically injecting information into a system prompt (the instructions ChatGPT receives at the beginning of every chat).</p></div><p><a href=\"https://arstechnica.com/?p=2003078#p3\">Read 7 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=2003078&amp;comments=1\">Comments</a></p>","author":"Benj Edwards","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"194ed96687852cf3204ed7e5e007c0a55e75199c992b5c7cf7c7a4094510e627","category":"Tech"}