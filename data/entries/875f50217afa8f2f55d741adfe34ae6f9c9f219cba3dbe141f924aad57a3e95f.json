{"title":"Nvidia is ditching dedicated G-Sync modules to push back against FreeSync’s ubiquity","link":"https://arstechnica.com/?p=2044240","date":1724180992000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/08/nvidia-mediatek-gsync-800x450.jpeg\" alt=\"Nvidia is ditching dedicated G-Sync modules to push back against FreeSync’s ubiquity\" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2024/08/nvidia-mediatek-gsync.jpeg\">Enlarge</a> (credit: Nvidia)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>Back in 2013, Nvidia introduced a new technology called G-Sync to eliminate screen tearing and stuttering effects and reduce input lag when playing PC games. The company accomplished this by tying your display's refresh rate to the actual frame rate of the game you were playing, and similar variable refresh-rate (VRR) technology has become a mainstay even in budget monitors and TVs today.</p>\n\n<p>The issue for Nvidia is that G-Sync isn't what has been driving most of that adoption. G-Sync has always required extra dedicated hardware inside of displays, increasing the costs for both users and monitor manufacturers. The VRR technology in most low-end to midrange screens these days is usually some version of the royalty-free AMD FreeSync or the similar <a href=\"https://arstechnica.com/gadgets/2022/05/new-adaptive-sync-tiers-crack-down-on-misleading-response-times-flicker/\">VESA Adaptive-Sync</a> standard, both of which provide G-Sync's most important features without requiring extra hardware. Nvidia more or less acknowledged that the free-to-use, cheap-to-implement VRR technologies had won <a href=\"https://arstechnica.com/gaming/2019/01/nvidia-certifies-select-freesync-monitors-as-g-sync-compatible/\">back in 2019</a> when it announced its \"G-Sync Compatible\" certification tier for FreeSync monitors. <a href=\"https://www.nvidia.com/en-us/geforce/products/g-sync-monitors/specs/\">The list of G-Sync Compatible screens</a> now vastly outnumbers the list of G-Sync and G-Sync Ultimate screens.</p>\n<p>Today Nvidia is announcing a change that's meant to keep G-Sync alive as its own separate technology, while eliminating the requirement for expensive additional hardware. Nvidia <a href=\"https://blogs.nvidia.com/blog/mediatek-g-sync-displays/\">says</a> it's partnering with chipmaker MediaTek to build G-Sync capabilities directly into scaler chips that MediaTek is creating for upcoming monitors. G-Sync modules ordinarily replace these scaler chips, but they're <a href=\"https://www.techpowerup.com/245463/nvidia-g-sync-hdr-module-adds-usd-500-to-monitor-pricing\">entirely separate boards</a> with expensive FPGA chips and dedicated RAM.</p></div><p><a href=\"https://arstechnica.com/?p=2044240#p3\">Read 3 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=2044240&amp;comments=1\">Comments</a></p>","author":"Andrew Cunningham","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"875f50217afa8f2f55d741adfe34ae6f9c9f219cba3dbe141f924aad57a3e95f","category":"Tech"}