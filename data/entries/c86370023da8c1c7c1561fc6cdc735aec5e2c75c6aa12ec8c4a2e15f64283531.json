{"title":"NVIDIA ร่วมมือ Hugging Face เปิดให้ใช้ NVIDIA DGX รันโมเดล คิดตามเวลาจริง","link":"https://www.blognone.com/node/141152","date":1722350459000,"content":"<div><div><div><p>NVIDIA ประกาศความร่วมมือกับ Hugging Face นำเซิร์ฟเวอร์ NVIDIA DGX Cloud ชิป H100 ออกมาให้บริการขายปลีกสำหรับรันโมเดลโดยคิดตามเวลารันจริง</p>\n<p>ผู้ใช้ที่ต้องการใช้บริการนี้ต้องเป็นสมาชิก Hugging Face แบบ Enterprise (เดือนละ 20 ดอลลาร์ต่อคน) และจะสามารถเรียกใช้โมเดลผ่านทางตัวเลือก \"NVIDIA NIM Enterprise\" โดยเรียกผ่านทางไลบรารี openai ในภาษา Python ได้เลย โดยก่อนหน้านี้ Hugging Face เคยนำชิป H100 มาให้บริการสำหรับการฝึกโมเดลมาก่อนแล้ว</p>\n<p>ทาง NVIDIA และ Hugging Face คิดค่าบริการตามจริง 8.25 ดอลลาร์ต่อชั่วโมง ไม่ได้คิดตามจำนวนโทเค็น โดยทั่วไปแล้ว Llama 3 8B ขนาด input 500 token และ output 100 token ใช้เวลาประมาณ 1 วินาทีบนชิป H100 คิดเป็นค่าใช้จ่าย 0.0023 ดอลลาร์  ขณะที่โมเดล Llama 3 70B จะใช้ชิป H100 4 ตัว และเวลารัน 2 วินาที รวมค่าใช้ที่พรอมพ์เท่ากันเป็น 8 เท่าประมาณ 0.0184 ดอลลาร์</p>\n<p>โมเดลที่ใช้งานได้ยังมีจำกัด เป็นโมเดลยอดนิยม เช่น Mixtral 8x22B, Llama 3.1, Mistral 7B, และ Llama 3 ด้วยแนวทางนี้อาจจะเหมาะกับผู้ที่ใช้งาน Hugging Face เป็นหลักอยู่แล้วและต้องการทดลองเพิ่มเติม <a href=\"https://www.blognone.com/node/141059\">ไม่เช่นนั้นไปใช้งานคลาวด์เฉพาะทางน่าจะถูกกว่า</a></p>\n<p>ที่มา - <a href=\"https://huggingface.co/blog/inference-dgx-cloud\">Hugging Face</a></p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/32ae2d84b93f2b67354f1a9ecb28badc.png\" /></p>\n</div></div></div><div><div>Topics: </div><div><div><a href=\"/topics/nvidia\">NVIDIA</a></div><div><a href=\"/topics/hugging-face\">Hugging Face</a></div><div><a href=\"/topics/artificial-intelligence\">Artificial Intelligence</a></div></div></div>","author":"lew","siteTitle":"Blognone","siteHash":"ededadcf18490b3937e7dd89ebe8c00dc129addbdf1ebe4aff1f458146693da0","entryHash":"c86370023da8c1c7c1561fc6cdc735aec5e2c75c6aa12ec8c4a2e15f64283531","category":"Thai"}