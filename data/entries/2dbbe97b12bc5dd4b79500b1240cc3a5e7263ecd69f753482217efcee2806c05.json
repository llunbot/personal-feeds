{"title":"New for Amazon Comprehend – Toxicity Detection","link":"https://aws.amazon.com/blogs/aws/new-for-amazon-comprehend-toxicity-detection/","date":1699560166000,"content":"<p>With <a href=\"https://aws.amazon.com/comprehend/\">Amazon Comprehend</a>, you can extract insights from text without being a machine learning expert. Using its built-in models, Comprehend can analyze the syntax of your input documents and find entities, events, key phrases, personally identifiable information (PII), and the overall sentiment or sentiments associated with specific entities (such as brands or products).</p> \n<p>Today, we are adding the capability to detect toxic content. This new capability helps you build safer environments for your end users. For example, you can use toxicity detection to improve the safety of applications open to external contributions such as comments. When using generative AI, toxicity detection can be used to check the input prompts and the output responses from large language models (LLMs).</p> \n<p>You can use toxicity detection with the <a href=\"https://aws.amazon.com/cli/\">AWS Command Line Interface (AWS CLI)</a> and <a href=\"https://aws.amazon.com/tools/\">AWS SDKs</a>. Let’s see how this works in practice with a few examples using the AWS CLI, an AWS SDK, and to check the use of an LLM.</p> \n<p><span><strong>Using Amazon Comprehend Toxicity Detection with AWS CLI<br /> </strong></span>The new <code>detect-toxic-content</code> subcommand in the AWS CLI detects toxicity in text. The output contains a list of labels, one for each text segment in input. For each text segment, a list is provided with the labels and a score (between 0 and 1).</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/11/02/aws-comprehend-toxicity-detection-api-arch.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/11/02/aws-comprehend-toxicity-detection-api-arch.png\" alt=\"Amazon Comprehend toxicity detection API\" width=\"1200\" height=\"373\" /></a></p> \n<p>For example, this AWS CLI command analyzes one text segment and returns one <code>Labels</code> section and an overall <code>Toxicity</code> score for the segment between o and 1:</p> \n<div> \n <pre><code>aws comprehend detect-toxic-content --language-code en --text-segments Text=\"'Good morning, it\\'s a beautiful day.'\"</code></pre> \n</div> \n<pre><code>{\n    \"ResultList\": [\n        {\n            \"Labels\": [\n                {\n                    \"Name\": \"PROFANITY\",\n                    \"Score\": 0.00039999998989515007\n                },\n                {\n                    \"Name\": \"HATE_SPEECH\",\n                    \"Score\": 0.01510000042617321\n                },\n                {\n                    \"Name\": \"INSULT\",\n                    \"Score\": 0.004699999932199717\n                },\n                {\n                    \"Name\": \"GRAPHIC\",\n                    \"Score\": 9.999999747378752e-05\n                },\n                {\n                    \"Name\": \"HARASSMENT_OR_ABUSE\",\n                    \"Score\": 0.0006000000284984708\n                },\n                {\n                    \"Name\": \"SEXUAL\",\n                    \"Score\": 0.03889999911189079\n                },\n                {\n                    \"Name\": \"VIOLENCE_OR_THREAT\",\n                    \"Score\": 0.016899999231100082\n                }\n            ],\n            \"Toxicity\": 0.012299999594688416\n        }\n    ]\n}</code></pre> \n<p>As expected, all scores are close to zero, and no toxicity was detected in this text.</p> \n<p>To pass input as a file, I first use the AWS CLI <code>--generate-cli-skeleton</code> option to generate a skeleton of the JSON syntax used by the <code>detect-toxic-content</code> command:</p> \n<div> \n <pre><code>aws comprehend detect-toxic-content --generate-cli-skeleton</code></pre> \n</div> \n<pre><code>{\n    \"TextSegments\": [\n        {\n            \"Text\": \"\"\n        }\n    ],\n    \"LanguageCode\": \"en\"\n}</code></pre> \n<p>I write the output to a file and add three text segments (I will not show here the text used to show what happens with toxic content). This time, different levels of toxicity content has been found. Each <code>Labels</code> section is related to the corresponding input text segment.</p> \n<div> \n <pre><code>aws comprehend detect-toxic-content --cli-input-json file://input.json</code></pre> \n</div> \n<pre><code>{\n    \"ResultList\": [\n        {\n            \"Labels\": [\n                {\n                    \"Name\": \"PROFANITY\",\n                    \"Score\": 0.03020000085234642\n                },\n                {\n                    \"Name\": \"HATE_SPEECH\",\n                    \"Score\": 0.12549999356269836\n                },\n                {\n                    \"Name\": \"INSULT\",\n                    \"Score\": 0.0738999992609024\n                },\n                {\n                    \"Name\": \"GRAPHIC\",\n                    \"Score\": 0.024399999529123306\n                },\n                {\n                    \"Name\": \"HARASSMENT_OR_ABUSE\",\n                    \"Score\": 0.09510000050067902\n                },\n                {\n                    \"Name\": \"SEXUAL\",\n                    \"Score\": 0.023900000378489494\n                },\n                {\n                    \"Name\": \"VIOLENCE_OR_THREAT\",\n                    \"Score\": 0.15549999475479126\n                }\n            ],\n            \"Toxicity\": 0.06650000065565109\n        },\n        {\n            \"Labels\": [\n                {\n                    \"Name\": \"PROFANITY\",\n                    \"Score\": 0.03400000184774399\n                },\n                {\n                    \"Name\": \"HATE_SPEECH\",\n                    \"Score\": 0.2676999866962433\n                },\n                {\n                    \"Name\": \"INSULT\",\n                    \"Score\": 0.1981000006198883\n                },\n                {\n                    \"Name\": \"GRAPHIC\",\n                    \"Score\": 0.03139999881386757\n                },\n                {\n                    \"Name\": \"HARASSMENT_OR_ABUSE\",\n                    \"Score\": 0.1777999997138977\n                },\n                {\n                    \"Name\": \"SEXUAL\",\n                    \"Score\": 0.013000000268220901\n                },\n                {\n                    \"Name\": \"VIOLENCE_OR_THREAT\",\n                    \"Score\": 0.8395000100135803\n                }\n            ],\n            \"Toxicity\": 0.41280001401901245\n        },\n        {\n            \"Labels\": [\n                {\n                    \"Name\": \"PROFANITY\",\n                    \"Score\": 0.9997000098228455\n                },\n                {\n                    \"Name\": \"HATE_SPEECH\",\n                    \"Score\": 0.39469999074935913\n                },\n                {\n                    \"Name\": \"INSULT\",\n                    \"Score\": 0.9265999794006348\n                },\n                {\n                    \"Name\": \"GRAPHIC\",\n                    \"Score\": 0.04650000110268593\n                },\n                {\n                    \"Name\": \"HARASSMENT_OR_ABUSE\",\n                    \"Score\": 0.4203999936580658\n                },\n                {\n                    \"Name\": \"SEXUAL\",\n                    \"Score\": 0.3353999853134155\n                },\n                {\n                    \"Name\": \"VIOLENCE_OR_THREAT\",\n                    \"Score\": 0.12409999966621399\n                }\n            ],\n            \"Toxicity\": 0.8180999755859375\n        }\n    ]\n}</code></pre> \n<p><span><strong>Using Amazon Comprehend Toxicity Detection with AWS SDKs</strong></span><br /> Similar to what I did with the AWS CLI, I can use an AWS SDK to programmatically detect toxicity in my applications. The following Python script uses the <a href=\"https://aws.amazon.com/sdk-for-python/\">AWS SDK for Python (Boto3)</a> to detect toxicity in the text segments and print the labels if the score is greater than a specified threshold. In the code, I redacted the content of the second and third text segments and replaced it with <code>***</code>.</p> \n<pre><code>import boto3\n\ncomprehend = boto3.client('comprehend')\n\nTHRESHOLD = 0.2\nresponse = comprehend.detect_toxic_content(\n    TextSegments=[\n        {\n            \"Text\": \"You can go through the door go, he's waiting for you on the right.\"\n        },\n        {\n            \"Text\": \"***\"\n        },\n        {\n            \"Text\": \"***\"\n        }\n    ],\n    LanguageCode='en'\n)\n\nresult_list = response['ResultList']\n\nfor i, result in enumerate(result_list):\n    labels = result['Labels']\n    detected = [ l for l in labels if l['Score'] &gt; THRESHOLD ]\n    if len(detected) &gt; 0:\n        print(\"Text segment {}\".format(i + 1))\n        for d in detected:\n            print(\"{} score {:.2f}\".format(d['Name'], d['Score']))</code></pre> \n<p>I run the Python script. The output contains the labels and the scores detected in the second and third text segments. No toxicity is detected in the first text segment.</p> \n<div> \n <pre><code>Text segment 2\nHATE_SPEECH score 0.27\nVIOLENCE_OR_THREAT score 0.84\nText segment 3\nPROFANITY score 1.00\nHATE_SPEECH score 0.39\nINSULT score 0.93\nHARASSMENT_OR_ABUSE score 0.42\nSEXUAL score 0.34</code></pre> \n</div> \n<p><span><strong>Using Amazon Comprehend Toxicity Detection with LLMs</strong></span><br /> I deployed the <a href=\"https://mistral.ai/news/announcing-mistral-7b/\">Mistral 7B</a> model using <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html\">Amazon SageMaker JumpStart</a> as <a href=\"https://aws.amazon.com/blogs/machine-learning/mistral-7b-foundation-models-from-mistral-ai-are-now-available-in-amazon-sagemaker-jumpstart/\">described in this blog post</a>.</p> \n<p>To avoid toxicity in the responses of the model, I built a Python script with three functions:</p> \n<ul> \n <li><code>query_endpoint</code> invokes the Mistral 7B model using the endpoint deployed by SageMaker JumpStart.</li> \n <li><code>check_toxicity</code> uses Comprehend to detect toxicity in a text and return a list of the detected labels.</li> \n <li><code>avoid_toxicity</code> takes in input a list of the detected labels and returns a message describing what to do to avoid toxicity.</li> \n</ul> \n<p>The query to the LLM goes through only if no toxicity is detected in the input prompt. Then, the response from the LLM is printed only if no toxicity is detected in output. In case toxicity is detected, the script provides suggestions on how to fix the input prompt.</p> \n<p>Here’s the code of the Python script:</p> \n<pre><code>import json\nimport boto3\n\ncomprehend = boto3.client('comprehend')\nsagemaker_runtime = boto3.client(\"runtime.sagemaker\")\n\nENDPOINT_NAME = \"&lt;REPLACE_WITH_YOUR_SAGEMAKER_JUMPSTART_ENDPOINT&gt;\"\nTHRESHOLD = 0.2\n\n\ndef query_endpoint(prompt):\n    payload = {\n        \"inputs\": prompt,\n        \"parameters\": {\n            \"max_new_tokens\": 68,\n            \"no_repeat_ngram_size\": 3,\n        },\n    }\n    response = sagemaker_runtime.invoke_endpoint(\n        EndpointName=ENDPOINT_NAME, ContentType=\"application/json\", Body=json.dumps(payload).encode(\"utf-8\")\n    )\n    model_predictions = json.loads(response[\"Body\"].read())\n    generated_text = model_predictions[0][\"generated_text\"]\n    return generated_text\n\n\ndef check_toxicity(text):\n    response = comprehend.detect_toxic_content(\n        TextSegments=[\n            {\n                \"Text\":  text\n            }\n        ],\n        LanguageCode='en'\n    )\n\n    labels = response['ResultList'][0]['Labels']\n    detected = [ l['Name'] for l in labels if l['Score'] &gt; THRESHOLD ]\n\n    return detected\n\n\ndef avoid_toxicity(detected):\n    formatted = [ d.lower().replace(\"_\", \" \") for d in detected ]\n    message = (\n        \"Avoid content that is toxic and is \" +\n        \", \".join(formatted) + \".\\n\"\n    )\n    return message\n\n\nprompt = \"Building a website can be done in 10 simple steps:\"\n\ndetected_labels = check_toxicity(prompt)\n\nif len(detected_labels) &gt; 0:\n    # Toxicity detected in the input prompt\n    print(\"Please fix the prompt.\")\n    print(avoid_toxicity(detected_labels))\nelse:\n    response = query_endpoint(prompt)\n\n    detected_labels = check_toxicity(response)\n\n    if len(detected_labels) &gt; 0:\n        # Toxicity detected in the output response\n        print(\"Here's an improved prompt:\")\n        prompt = avoid_toxicity(detected_labels) + prompt\n        print(prompt)\n    else:\n        print(response)</code></pre> \n<p>You’ll not get a toxic response with the sample prompt in the script, but it’s safe to know that you can set up an automatic process to check and mitigate if that happens.</p> \n<p><span><strong>Availability and Pricing</strong></span><br /> Toxicity detection for Amazon Comprehend is available today in the following <a href=\"https://aws.amazon.com/about-aws/global-infrastructure/regions_az/\">AWS Regions</a>: US East (N. Virginia), US West (Oregon), Europe (Ireland), and Asia Pacific (Sydney).</p> \n<p>When using toxicity detection, there are no long-term commitments, and you pay based on the number of input characters in units of 100 characters (1 unit = 100 characters), with a minimum charge of 3 units (300 character) per request. For more information, see <a href=\"https://aws.amazon.com/comprehend/pricing/\">Amazon Comprehend pricing</a>.</p> \n<p><strong><a href=\"https://aws.amazon.com/comprehend/features/\">Improve the safety of your online communities and simplify the adoption of LLMs in your applications with toxicity detection.</a></strong></p> \n<p>— <a href=\"https://twitter.com/danilop\">Danilo</a></p>","author":"Danilo Poccia","siteTitle":"AWS News Blog","siteHash":"6093e072e4117ec22616e844cb857d03ca62c57a411a8affc77cb5e8b6b15bf6","entryHash":"2dbbe97b12bc5dd4b79500b1240cc3a5e7263ecd69f753482217efcee2806c05","category":"Tech"}