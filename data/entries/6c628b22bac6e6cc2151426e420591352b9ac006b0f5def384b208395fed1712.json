{"title":"Integrating Image-To-Text And Text-To-Speech Models (Part 1)","link":"https://smashingmagazine.com/2024/07/integrating-image-to-text-and-text-to-speech-models-part1/","date":1721815200000,"content":"<p>Audio descriptions involve narrating contextual visual information in images or videos, improving user experiences, especially for those who rely on audio cues.</p>\n<p>At the core of audio description technology are two crucial components: the <strong>description</strong> and the <strong>audio</strong>. The description involves understanding and interpreting the visual content of an image or video, which includes details such as actions, settings, expressions, and any other relevant visual information. Meanwhile, the audio component converts these descriptions into spoken words that are clear, coherent, and natural-sounding.</p>\n<p>So, here’s something we can do: <strong>build an app that generates and announces audio descriptions</strong>. The app can integrate a pre-trained vision-language model to analyze image inputs, extract relevant information, and generate accurate descriptions. These descriptions are then converted into speech using text-to-speech technology, providing a seamless and engaging audio experience.</p>\n<p><img src=\"https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part1/1-image-audio-captioning-app.png\" /></p>\n<p>By the end of this tutorial, you will gain a solid grasp of the components that are used to build  audio description tools. We’ll spend time discussing what VLM and TTS models are, as well as many examples of them and tooling for integrating them into your work.</p>\n<p>When we finish, you will be ready to follow along with a second tutorial in which we level up and build a chatbot assistant that you can interact with to get more insights about your images or videos.</p>\nVision-Language Models: An Introduction\n<p>VLMs are a form of artificial intelligence that can understand and learn from visuals and linguistic modalities. </p>\n<p><img src=\"https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part1/2-four-tasks-vision-language-model-can-handle.jpg\" /></p>\n<p>They are trained on vast amounts of data that include images, videos, and text, allowing them to learn patterns and relationships between these modalities. In simple terms, a VLM can look at an image or video and generate a corresponding text description that accurately matches the visual content.</p>\n<p>VLMs typically consist of three main components:</p>\n<ol>\n<li>An <strong>image model</strong> that extracts meaningful visual information,</li>\n<li>A <strong>text model</strong> that processes and understands natural language,</li>\n<li>A <strong>fusion mechanism</strong> that combines the representations learned by the image and text models, enabling cross-modal interactions.</li>\n</ol>\n<p>Generally speaking, the <strong>image model</strong> — also known as the vision encoder — extracts visual features from input images and maps them to the language model’s input space, creating visual tokens. The <strong>text model</strong> then processes and understands natural language by generating text embeddings. Lastly, these visual and textual representations are combined through the <strong>fusion mechanism</strong>, allowing the model to integrate visual and textual information.</p>\n<p>VLMs bring a new level of intelligence to applications by bridging visual and linguistic understanding. Here are some of the applications where VLMs shine:</p>\n<ul>\n<li><strong>Image captions</strong>: VLMs can provide automatic descriptions that enrich user experiences, improve searchability, and even enhance visuals for vision impairments.</li>\n<li><strong>Visual answers to questions</strong>: VLMs could be integrated into educational tools to help students learn more deeply by allowing them to ask questions about visuals they encounter in learning materials, such as complex diagrams and illustrations.</li>\n<li><strong>Document analysis</strong>: VLMs can streamline document review processes, identifying critical information in contracts, reports, or patents much faster than reviewing them manually.</li>\n<li><strong>Image search</strong>: VLMs could open up the ability to perform reverse image searches. For example, an e-commerce site might allow users to upload image files that are processed to identify similar products that are available for purchase.</li>\n<li><strong>Content moderation</strong>: Social media platforms could benefit from VLMs by identifying and removing harmful or sensitive content automatically before publishing it.</li>\n<li><strong>Robotics</strong>: In industrial settings, robots equipped with VLMs can perform quality control tasks by understanding visual cues and describing defects accurately.</li>\n</ul>\n<p>This is merely an overview of what VLMs are and the pieces that come together to generate audio descriptions. To get a clearer idea of how VLMs work, let’s look at a few real-world examples that leverage VLM processes.</p>\nVLM Examples\n<p>Based on the use cases we covered alone, you can probably imagine that VLMs come in many forms, each with its unique strengths and applications. In this section, we will look at a few examples of VLMs that can be used for a variety of different purposes.</p>\n<h3>IDEFICS</h3>\n<p><strong>IDEFICS</strong> is an open-access model inspired by Deepmind’s <a href=\"https://arxiv.org/abs/2204.14198\">Flamingo</a>, designed to understand and generate text from images and text inputs. It’s similar to OpenAI’s GPT-4 model in its multimodal capabilities but is built entirely from publicly available data and models.</p>\n<p><img src=\"https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part1/3-idefics.png\" /></p>\n<p>IDEFICS is trained on public data and models — like LLama V1 and Open Clip — and comes in two versions: the base and instructed versions, each available in <a href=\"https://huggingface.co/HuggingFaceM4/idefics-9b-instruct\">9 billion</a> and <a href=\"https://huggingface.co/HuggingFaceM4/idefics-80b-instruct\">80 billion</a> parameter sizes.</p>\n<p>The model combines two pre-trained unimodal models (for vision and language) with newly added Transformer blocks that allow it to bridge the gap between understanding images and text. It’s trained on a mix of image-text pairs and multimodal web documents, enabling it to handle a wide range of visual and linguistic tasks. As a result, IDEFICS can answer questions about images, provide detailed descriptions of visual content, generate stories based on a series of images, and function as a pure language model when no visual input is provided. </p>\n<h3>PaliGemma</h3>\n<p><a href=\"https://huggingface.co/blog/paligemma\">PaliGemma</a> is an advanced VLM that draws inspiration from <a href=\"https://arxiv.org/abs/2310.09199\">PaLI-3</a> and leverages open-source components like the <a href=\"https://huggingface.co/google/siglip-so400m-patch14-384\">SigLIP</a> vision model and the <a href=\"https://ai.google.dev/gemma\">Gemma</a> language model.</p>\n<p><img src=\"https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part1/4-paligemma-model.png\" /></p>\n<p>Designed to process both images and textual input, PaliGemma excels at generating descriptive text in multiple languages. Its capabilities extend to a variety of tasks, including image captioning, answering questions from visuals, reading text, detecting subjects in images, and segmenting objects displayed in images.</p>\n<p>The core architecture of PaliGemma includes a Transformer decoder paired with a Vision Transformer image encoder that boasts an impressive 3 <em>billion</em> parameters. The text decoder is derived from <a href=\"https://huggingface.co/google/gemma-2b\">Gemma-2B</a>, while the image encoder is based on <a href=\"https://huggingface.co/google/siglip-so400m-patch14-384\">SigLIP-So400m/14</a>.</p>\n<p><img src=\"https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part1/5-paligemma-architecture.png\" /></p>\n<p>Through training methods similar to PaLI-3, PaliGemma achieves exceptional performance across numerous vision-language challenges.</p>\n<p>PaliGemma is offered in two distinct sets:</p>\n<ul>\n<li><strong>General Purpose Models (PaliGemma):</strong> These pre-trained models are designed for fine-tuning a wide array of tasks, making them ideal for practical applications.</li>\n<li><strong>Research-Oriented Models (PaliGemma-FT):</strong> Fine-tuned on specific research datasets, these models are tailored for deep research on a range of topics.</li>\n</ul>\n<h3>Phi-3-Vision-128K-Instruct</h3>\n<p>The <a href=\"https://huggingface.co/microsoft/Phi-3-vision-128k-instruct\">Phi-3-Vision-128K-Instruct</a> model is a Microsoft-backed venture that combines text and vision capabilities. It’s built on a dataset of high-quality, reasoning-dense data from both text and visual sources. Part of the Phi-3 family, the model has a context length of 128K, making it suitable for a range of applications.</p>\n<p>You might decide to use Phi-3-Vision-128K-Instruct in cases where your application has limited memory and computing power, thanks to its <strong>relatively lightweight</strong> that helps with latency. The model works best for generally understanding images, recognizing characters in text, and describing charts and tables.</p>\n<h3>Yi Vision Language (Yi-VL)</h3>\n<p><a href=\"https://huggingface.co/01-ai/Yi-VL-34B\">Yi-VL</a> is an open-source AI model developed by <a href=\"https://www.01.ai/\">01-ai</a> that can have multi-round conversations with images by reading text from images and translating it. This model is part of the Yi LLM series and has two versions: <a href=\"https://huggingface.co/01-ai/Yi-VL-6B\">6B</a> and <a href=\"https://huggingface.co/01-ai/Yi-VL-34B\">34B</a>.</p>\n<p>What distinguishes Yi-VL from other models is its <strong>ability to carry a conversation</strong>, whereas other models are typically limited to a single text input. Plus, it’s bilingual making it more versatile in a variety of language contexts.</p>\nFinding And Evaluating VLMs\n<p>There are many, many VLMs and we only looked at a few of the most notable offerings. As you commence work on an application with image-to-text capabilities, you may find yourself wondering where to look for VLM options and how to compare them.</p>\n<p>There are two resources in the Hugging Face community you might consider using to help you find and compare VLMs. I use these regularly and find them incredibly useful in my work.</p>\n<h3>Vision Arena</h3>\n<p><a href=\"https://huggingface.co/spaces/WildVision/vision-arena\">Vision Arena</a> is a leaderboard that ranks VLMs based on anonymous user voting and reviews. But what makes it great is the fact that you can <strong>compare any two models side-by-side</strong> for yourself to find the best fit for your application.</p>\n<p>And when you compare two models, you can contribute your own anonymous votes and reviews for others to lean on as well.</p>\n<p><img src=\"https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part1/6-vision-arena-leaderboard.png\" /></p>\n<h3>OpenVLM Leaderboard</h3>\n<p><a href=\"https://huggingface.co/spaces/opencompass/open_vlm_leaderboard\">OpenVLM</a> is another leaderboard hosted on Hugging Face for getting technical specs on different models. What I like about this resource is the wealth of metrics for evaluating VLMs, including the speed and accuracy of a given VLM.</p>\n<p>Further, OpenVLM lets you filter models by size, type of license, and other ranking criteria. I find it particularly useful for finding VLMs I might have overlooked or new ones I haven’t seen yet.</p>\n<p><img src=\"https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part1/7-openvlm-leaderboard.png\" /></p>\nText-To-Speech Technology\n<p>Earlier, I mentioned that the app we are about to build will use vision-language models to generate written descriptions of images, which are then read aloud. The technology that handles converting text to audio speech is known as text-to-speech synthesis or simply <strong>text-to-speech (TTS)</strong>.</p>\n<p>TTS converts written text into synthesized speech that sounds natural. The goal is to take published content, like a blog post, and read it out loud in a realistic-sounding human voice.</p>\n<p>So, how does TTS work? First, it breaks down text into the smallest units of sound, called <strong>phonemes</strong>, and this process allows the system to figure out proper word pronunciations. Next, AI enters the mix, including deep learning algorithms trained on hours of human speech data. This is how we get the app to mimic human speech patterns, tones, and rhythms — all the things that make for “natural” speech. The AI component is key as it elevates a voice from robotic to something with personality. Finally, the system combines the phoneme information with the AI-powered digital voice to render the fully expressive speech output.</p>\n<p>The result is automatically generated speech that sounds fairly smooth and natural. Modern TTS systems are extremely advanced in that they can replicate different tones and voice inflections, work across languages, and understand context. This naturalness makes TTS <strong>ideal for humanizing interactions with technology</strong>, like having your device read text messages out loud to you, just like Apple’s Siri or Microsoft’s Cortana.</p>\nTTS Examples\n<p>Based on the use cases we covered alone, you can probably imagine that VLMs come in many forms, each with its unique strengths and applications. In this section, we will look at a few examples of VLMs that can be used for a variety of different purposes.</p>\n<p>Just as we took a moment to review existing vision language models, let’s pause to consider some of the more popular TTS resources that are available.</p>\n<h3>Bark</h3>\n<p>Straight from Bark’s model card in Hugging Face:</p>\n<blockquote>“Bark is a transformer-based text-to-audio model created by <a href=\"https://www.suno.ai/\">Suno</a>. Bark can generate highly realistic, multilingual speech as well as other audio — including music, background noise, and simple sound effects. The model can also produce nonverbal communication, like laughing, sighing, and crying. To support the research community, we are providing access to pre-trained model checkpoints ready for inference.”</blockquote>\n\n<p>The non-verbal communication cues are particularly interesting and a distinguishing feature of Bark. Check out the various things Bark can do to communicate emotion, pulled directly from the model’s <a href=\"https://github.com/suno-ai/bark?tab=readme-ov-file#%EF%B8%8F-details\">GitHub repo</a>:</p>\n<ul>\n<li><code>[laughter]</code></li>\n<li><code>[laughs]</code></li>\n<li><code>[sighs]</code></li>\n<li><code>[music]</code></li>\n<li><code>[gasps]</code></li>\n<li><code>[clears throat]</code></li>\n</ul>\n<p>This could be cool or creepy, depending on how it’s used, but reflects the sophistication we’re working with. In addition to laughing and gasping, Bark is different in that it <strong>doesn’t work with phonemes like a typical TTS model</strong>:</p>\n<blockquote>“It is not a conventional TTS model but instead a fully generative text-to-audio model capable of deviating in unexpected ways from any given script. Different from previous approaches, the input text prompt is converted directly to audio without the intermediate use of phonemes. It can, therefore, generalize to arbitrary instructions beyond speech, such as music lyrics, sound effects, or other non-speech sounds.”</blockquote>\n\n\n\n<p></p>\n<h3>Coqui</h3>\n<p><a href=\"https://huggingface.co/coqui/XTTS-v2\"><strong>Coqui/XTTS-v2</strong></a> can clone voices in different languages. All it needs for training is a short six-second clip of audio. This means the model can be used to translate audio snippets from one language into another while maintaining the same voice.</p>\n<p>At the time of writing, Coqui currently supports 16 languages, including English, Spanish, French, German, Italian, Portuguese, Polish, Turkish, Russian, Dutch, Czech, Arabic, Chinese, Japanese, Hungarian, and Korean.</p>\n\n\n<p></p>\n<h3>Parler-TTS</h3>\n<p><a href=\"https://huggingface.co/parler-tts/parler-tts-mini-expresso\">Parler-TTS</a> excels at generating high-quality, natural-sounding speech in the style of a given speaker. In other words, it <strong>replicates a person’s voice</strong>. This is where many folks might draw an ethical line because techniques like this can be used to essentially imitate a real person, even without their consent, in a process known as <a href=\"https://mitsloan.mit.edu/ideas-made-to-matter/deepfakes-explained\">“deepfake”</a> and the consequences can range from benign impersonations to <a href=\"https://www.nsa.gov/Press-Room/Press-Releases-Statements/Press-Release-View/Article/3523329/nsa-us-federal-agencies-advise-on-deepfake-threats/\">full-on phishing attacks</a>.</p>\n<p>But that’s not really the aim of Parler-TTS. Rather, it’s good in contexts that require personalized and natural-sounding speech generation, such as voice assistants and possibly even accessibility tooling to aid visual impairments by announcing content.</p>\n\n\n<p></p>\n<h3>TTS Arena Leaderboard</h3>\n<p>Do you know how I shared the OpenVLM Leaderboard for finding and comparing vision language models? Well, there’s an equivalent leadership for TTS models as well over at the Hugging Face community called <a href=\"https://huggingface.co/spaces/TTS-AGI/TTS-Arena\">TTS Arena</a>.</p>\n<p>TTS models are ranked by the “naturalness” of their voices, with the most natural-sounding models ranked first. Developers like you and me vote and provide feedback that influences the rankings.</p>\n<p><img src=\"https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part1/8-text-to-speech-model-leaderboard.png\" /></p>\nTTS API Providers\n<p>What we just looked at are TTS models that are baked into whatever app we’re making. However, some models are consumable via API, so it’s possible to get the benefits of a TTS model without the added bloat if a particular model is made available by an API provider.</p>\n<p>Whether you decide to bundle TTS models in your app or integrate them via APIs is totally up to you. There is no right answer as far as saying one method is better than another — it’s more about the app’s requirements and whether the dependability of a baked-in model is worth the memory hit or vice-versa.</p>\n<p>All that being said, I want to call out a handful of TTS API providers for you to keep in your back pocket.</p>\n<h3>ElevenLabs</h3>\n<p><a href=\"https://elevenlabs.io/text-to-speech\">ElevenLab</a><a href=\"https://elevenlabs.io/text-to-speech\">s offers a TTS API</a> that uses neural networks to make voices sound natural. Voices can be customized for different languages and accents, leading to realistic, engaging voices.</p>\n<p><a href=\"https://elevenlabs.io/text-to-speech\">Try the model out for yourself on the ElevenLabs site</a>. You can enter a block of text and choose from a wide variety of voices that read the submitted text aloud.</p>\n<h3>Colossyan</h3>\n<p><a href=\"https://www.colossyan.com/text-to-speech\">Colossyan’s text-to-speech API</a> converts text into natural-sounding voice recordings in over 70 languages and accents. From there, the service allows you to match the audio to an avatar to produce something like a complete virtual presentation based on your voice — or someone else’s.</p>\n<p><img src=\"https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part1/9-colossyan-studio.png\" /></p>\n<p>Once again, this is encroaching on deepfake territory, but it’s really interesting to think of Colossyan’s service as a virtual casting call for actors to perform off a script.</p>\n<h4>Murf.ai</h4>\n<p><a href=\"https://murf.ai/studio/login\">Murf.ai</a> is yet another TTS API designed to generate voiceovers based on real human voices. The service provides a slew of premade voices you can use to generate audio for anything from explainer videos and audiobooks to course lectures and entire podcast episodes.</p>\n<p><img src=\"https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part1/10-examples-murfai-voice-options.png\" /></p>\n<h4>Amazon Polly</h4>\n<p>Amazon has its own TTS API called <a href=\"https://aws.amazon.com/polly/\">Polly</a>. You can <a href=\"https://aws.amazon.com/blogs/machine-learning/create-audio-for-content-in-multiple-languages-with-the-same-tts-voice-persona-in-amazon-polly/\">customize the voices</a> using lexicons and <a href=\"https://cloud.google.com/text-to-speech/docs/ssml\">Speech Synthesis Markup</a> (SSML) tags for establishing speaking styles with affordances for adjusting things like pitch, speed, and volume.</p>\n<h4>PlayHT</h4>\n<p>The <a href=\"https://docs.play.ht/reference/api-getting-started\">PlayHT TTS API</a> generates speech in 142 languages. Type what you want it to say, pick a voice, and download the output as an MP3 or WAV file.</p>\n<p><img src=\"https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part1/11-playht.png\" /></p>\nDemo: Building An Image-to-Audio Interface\n<p>So far, we have discussed the two primary components for generating audio from text: <strong>vision-language models</strong> and <strong>text-to-speech models</strong>. We’ve covered what they are, where they fit into the process of generating real-sounding speech, and various examples of each model. </p>\n<p>Now, it’s time to apply those concepts to the app we are building in this tutorial (and will improve in a second tutorial). We will use a VLM so the app can glean meaning and context from images, a TTS model to generate speech that mimics a human voice, and then integrate our work into a user interface for submitting images that will lead to generated speech output.</p>\n<p>I have decided to base our work on a VLM by Salesforce called <a href=\"https://huggingface.co/Salesforce/blip-image-captioning-large\">BLIP</a>, a TTS model from Kakao Enterprise called <a href=\"https://huggingface.co/spaces/kakao-enterprise/vits\">VITS</a>, and <a href=\"https://libraries.io/pypi/gradio\">Gradio</a> as a framework for the design interface. <a href=\"https://www.smashingmagazine.com/author/joas-pambou/\">I’ve covered Gradio extensively in other articles</a>, but the gist is that it is a Python library for building web interfaces — only it offers built-in tools for working with machine learning models that make Gradio ideal for a tutorial like this.</p>\n<p>You can use completely different models if you like. The whole point is less about the intricacies of a particular model than it is to demonstrate <strong>how the pieces generally come together</strong>.</p>\n<p>Oh, and one more detail worth noting: I am working with the code for all of this in <a href=\"https://colab.google\">Google Collab</a>. I’m using it because it’s hosted and ideal for demonstrations like this. But you can certainly work in a more traditional IDE, like VS Code.</p>\n<h3>Installing Libraries</h3>\n<p>First, we need to install the necessary libraries:</p>\n<pre><code>#python\n!pip install gradio pillow transformers scipy numpy\n</code></pre>\n\n<p>We can upgrade the transformers library to the latest version if we need to:</p>\n<pre><code>#python\n!pip install --upgrade transformers\n</code></pre>\n\n<p>Not sure if you need to upgrade? Here’s how to check the current version:</p>\n<pre><code>#python\nimport transformers\nprint(transformers.__version__)\n</code></pre>\n\n<p>OK, now we are ready to import the libraries:</p>\n<pre><code>#python\nimport gradio as gr\nfrom PIL import Image\nfrom transformers import pipeline\nimport scipy.io.wavfile as wavfile\nimport numpy as np\n</code></pre>\n\n<p>These libraries will help us process images, use models on the Hugging Face hub, handle audio files, and build the UI. </p>\n<h3>Creating Pipelines</h3>\n<p>Since we will pull our models directly from Hugging Face’s <a href=\"https://huggingface.co/docs/hub/models-the-hub\">model hub</a>, we can tap into them using <a href=\"https://huggingface.co/docs/transformers/main_classes/pipelines\">pipelines</a>. This way, we’re working with an API for tasks that involve natural language processing and computer vision without carrying the load in the app itself.</p>\n<p>We set up our pipeline like this:</p>\n<div>\n<pre><code>#python\ncaption_image = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-large\")\n</code></pre>\n</div>\n\n<p>This establishes a pipeline for us to access BLIP for converting images into textual descriptions. Again, you could establish a pipeline for any other model in the Hugging Face hub.</p>\n<p>We’ll need a pipeline connected to our TTS model as well:</p>\n<div>\n<pre><code>#python\nNarrator = pipeline(\"text-to-speech\", model=\"kakao-enterprise/vits-ljs\")\n</code></pre>\n</div>\n\n<p>Now, we have a pipeline where we can pass our image text to be converted into natural-sounding speech.</p>\n<h3>Converting Text to Speech</h3>\n<p>What we need now is a function that handles the audio conversion. Your code will differ depending on the TTS model in use, but here is how I approached the conversion based on the VITS model:</p>\n<div>\n<pre><code>#python\n\ndef generate_audio(text):\n  # Generate speech from the input text using the Narrator (VITS model)\n  Narrated_Text = Narrator(text)\n\n  # Extract the audio data and sampling rate\n  audio_data = np.array(Narrated_Text[\"audio\"][0])\n  sampling_rate = Narrated_Text[\"sampling_rate\"]\n\n  # Save the generated speech as a WAV file\n  wavfile.write(\"generated_audio.wav\", rate=sampling_rate, data=audio_data)\n\n  # Return the filename of the saved audio file\n  return \"generated_audio.wav\"\n</code></pre>\n</div>\n\n<p>That’s great, but we need to make sure there’s a bridge that connects the text that the app generates from an image to the speech conversion. We can write a function that uses BLIP to generate the text and then calls the <code>generate_audio()</code> function we just defined:</p>\n<div>\n<pre><code>#python\ndef caption_my_image(pil_image):\n  # Use BLIP to generate a text description of the input image\n  semantics = caption_image(images=pil_image)[0][\"generated_text\"]\n\n  # Generate audio from the text description\n  return generate_audio(semantics)\n</code></pre>\n</div>\n\n<h3>Building The User Interface</h3>\n<p>Our app would be pretty useless if there was no way to interact with it. This is where Gradio comes in. We will use it to create a form that accepts an image file as an input and then outputs the generated text for display as well as the corresponding file containing the speech.</p>\n<div>\n<pre><code>#python\n\nmain_tab = gr.Interface(\n  fn=caption_my_image,\n  inputs=[gr.Image(label=\"Select Image\", type=\"pil\")],\n  outputs=[gr.Audio(label=\"Generated Audio\")],\n  title=\" Image Audio Description App\",\n  description=\"This application provides audio descriptions for images.\"\n)\n\n# Information tab\ninfo_tab = gr.Markdown(\"\"\"\n  # Image Audio Description App\n  ### Purpose\n  This application is designed to assist visually impaired users by providing audio descriptions of images. It can also be used in various scenarios such as creating audio captions for educational materials, enhancing accessibility for digital content, and more.\n\n  ### Limits\n  - The quality of the description depends on the image clarity and content.\n  - The application might not work well with images that have complex scenes or unclear subjects.\n  - Audio generation time may vary depending on the input image size and content.\n  ### Note\n  - Ensure the uploaded image is clear and well-defined for the best results.\n  - This app is a prototype and may have limitations in real-world applications.\n\"\"\")\n\n# Combine both tabs into a single app \n demo = gr.TabbedInterface(\n  [main_tab, info_tab],\n  tab_names=[\"Main\", \"Information\"]\n)\n\ndemo.launch()\n</code></pre>\n</div>\n\n<p>The interface is quite plain and simple, but that’s OK since our work is purely for demonstration purposes. You can always add to this for your own needs. The important thing is that you now have a working application you can interact with.</p>\n<p>At this point, you could run the app and try it in Google Collab. You also have the option to deploy your app, though you’ll need hosting for it. <a href=\"https://huggingface.co/spaces/launch\">Hugging Face also has a feature called Spaces</a> that you can use to deploy your work and run it without Google Collab. There’s even a <a href=\"https://huggingface.co/blog/gradio-spaces#uploading-your-models-to-the-spaces\">guide</a> you can use to set up your own Space.</p>\n<p>Here’s the final app that you can try by uploading your own photo:</p>\n\n\n<p></p>\nComing Up…\n<p>We covered a lot of ground in this tutorial! In addition to learning about VLMs and TTS models at a high level, we looked at different examples of them and then covered how to find and compare models.</p>\n<p>But the rubber really met the road when we started work on our app. Together, we made a useful tool that generates text from an image file and then sends that text to a TTS model to convert it into speech that is announced out loud and downloadable as either an MP3 or WAV file.</p>\n<p>But we’re not done just yet! What if we could glean even more detailed information from images and our <strong>app not only describes the images but can also carry on a conversation about them</strong>?</p>\n<p>Sounds exciting, right? This is exactly what we’ll do in the second part of this tutorial.</p>","author":"","siteTitle":"Articles on Smashing Magazine — For Web Designers And Developers","siteHash":"ab069ca35bf300e9db0da36f49701f66485a5b0d2db0471dfeee07cef6204939","entryHash":"6c628b22bac6e6cc2151426e420591352b9ac006b0f5def384b208395fed1712","category":"Tech"}