{"title":"26Ã— Faster Inference with Layer-Condensed KV Cache for Large Language Models","link":"https://arxiv.org/abs/2405.10637","date":1716219224000,"content":"<a href=\"https://news.ycombinator.com/item?id=40416657\">Comments</a>","author":"","siteTitle":"Hacker News","siteHash":"37bb545430005dba450c1e40307450d8e4e791b434e83f3d38915ebad510fd50","entryHash":"d2b6d1360a17017f8d58b8ac511b17a16209b592ef0a2a7b5acb8f5f8c79f94c","category":"Tech"}