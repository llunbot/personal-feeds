{"title":"“Hallucinating” AI models help coin Cambridge Dictionary’s word of the year","link":"https://arstechnica.com/?p=1984726","date":1700240508000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2023/11/halluncinate_hero_1-800x450.jpg\" alt=\"A screenshot of the Cambridge Dictionary website where it announced its 2023 word of the year, \" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2023/11/halluncinate_hero_1.jpg\">Enlarge</a> <span>/</span> A screenshot of the Cambridge Dictionary website where it announced its 2023 word of the year, \"hallucinate.\" (credit: <a href=\"https://dictionary.cambridge.org/editorial/woty\">Cambridge Dictionary</a>)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>On Wednesday, Cambridge Dictionary <a href=\"https://www.cam.ac.uk/research/news/cambridge-dictionary-names-hallucinate-word-of-the-year-2023\">announced</a> that its 2023 word of the year is \"hallucinate,\" owing to the popularity of large language models (LLMs) like <a href=\"https://arstechnica.com/information-technology/2022/12/openai-invites-everyone-to-test-new-ai-powered-chatbot-with-amusing-results/\">ChatGPT</a>, which sometimes produce erroneous information. The Dictionary also published an <a href=\"https://dictionary.cambridge.org/editorial/woty\">illustrated site</a> explaining the term, saying, \"When an artificial intelligence hallucinates, it produces false information.\"</p>\n\n<p>\"The Cambridge Dictionary team chose hallucinate as its Word of the Year 2023 as it recognized that the new meaning gets to the heart of why people are talking about AI,\" the dictionary writes. \"Generative AI is a powerful tool but one we’re all still learning how to interact with safely and effectively—this means being aware of both its potential strengths and its current weaknesses.\"</p>\n<p>As we've <a href=\"https://arstechnica.com/information-technology/2023/04/why-ai-chatbots-are-the-ultimate-bs-machines-and-how-people-hope-to-fix-them/\">previously covered</a> in various articles, \"hallucination\" in relation to AI originated as a term of art in the machine-learning space. As LLMs entered mainstream use through applications like ChatGPT late last year, the term spilled over into general use and began to cause confusion among some, who saw it as unnecessary anthropomorphism. Cambridge Dictionary's first definition of hallucination (for humans) is \"to seem to see, hear, feel, or smell something that does not exist.\" It involves perception from a conscious mind, and some object to that association.</p></div><p><a href=\"https://arstechnica.com/?p=1984726#p3\">Read 8 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=1984726&amp;comments=1\">Comments</a></p>","author":"Benj Edwards","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"3ab630a5f4916dd73d617149868e0790fde7260c47edc5cc94d3930be5dd7402","category":"Tech"}