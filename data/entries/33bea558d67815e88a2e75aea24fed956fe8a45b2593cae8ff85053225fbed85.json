{"title":"NVIDIA เปิดตัวโมดูลปัญญาประดิษฐ์ H100 NVL อัดแรม 188GB เพื่อรัน AI ขนาดใหญ่","link":"https://www.blognone.com/node/133109","date":1679422272000,"content":"<div><div><div><p>NVIDIA เปิดตัวชิป H100 NVL หลังจาก<a href=\"https://www.blognone.com/node/127733\">เปิดตัวรุ่นแรกในตระกูลเมื่อปีที่แล้ว</a> โดยความพิเศษของโมดูลรุ่นใหม่นี้คือมันใส่แรมมาสูงถึง 188GB นับว่าสูงที่สุดจากเดิมที่ชิป H100 รองรับแรมเพียง 80GB เท่านั้น เหตุผลสำคัญคือโมเดลปัญญาประดิษฐ์ในกลุ่ม GPT ช่วงหลังมีขนาดใหญ่มากๆ ระดับแสนล้านพารามิเตอร์ การรันโมเดลระดับนี้ต้องการแรมขนาดใหญ่</p>\n<p>นอกจาก H100 NVL แล้ว NVIDIA ยังเปิดตัวเซิร์ฟเวอร์รุ่นอื่นๆ มาอีก 3 รุ่น ได้แก่</p>\n<ul>\n<li>NVIDIA L4 สำหรับงานวิดีโอเป็นหลัก ตัวการ์ด L4 มีแรม 24GB พลังประมวลผล 30 teraFLOPS ที่ FP32 สามารถใส่การ์ดสูงสุด 8 ใบ เหมาะสำหรับงานเรนเดอร์วิดีโอ, ภาพสามมิติ, บีบอัดวิดีโอ, หรือใช้ปัญญาประดิษฐ์ปรับปรุงคุณภาพภาพ</li>\n<li>NVIDIA L40 ยังไม่เปิดเผยขนาดแรมบนการ์ดและความแรง แต่ระบุว่าเหมาะกับงานปัญญาประดิษฐ์สร้างภาพ และงานสร้างโลกเสมือน</li>\n<li>NVIDIA Grace Hopper เป็นเซิรฟเวอร์สำหรับงานฐานข้อมูลเพื่อการรันปัญญาประดิษฐ์แนะนำข้อมูลที่เกี่ยวข้อง</li>\n</ul>\n<p>ตอนนี้ Google Cloud เตรียมให้บริการเครื่อง L4 เป็นรายแรก และทั้ง L4/L40 นั้นจะมีผู้ผลิตเซิร์ฟเวอร์ผลิตเครื่องขายหลายแบรนด์ ขณะที่ชิป H100 NVL นั้นต้องรอครึ่งหลังของปีนี้</p>\n<p>ที่มา - <a href=\"https://nvidianews.nvidia.com/news/nvidia-launches-inference-platforms-for-large-language-models-and-generative-ai-workloads\">NVIDIA</a></p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/9ff1c069a3489129f9a52b1c6eb51172.jpg\" /></p>\n</div></div></div><div><div>Topics: </div><div><div><a href=\"/topics/nvidia\">NVIDIA</a></div><div><a href=\"/topics/gpu\">GPU</a></div><div><a href=\"/topics/enterprise\">Enterprise</a></div></div></div>","author":"lew","siteTitle":"Blognone","siteHash":"ededadcf18490b3937e7dd89ebe8c00dc129addbdf1ebe4aff1f458146693da0","entryHash":"33bea558d67815e88a2e75aea24fed956fe8a45b2593cae8ff85053225fbed85","category":"Thai"}