{"title":"NVIDIA โชว์เบนช์มาร์คแรกของ Blackwell B200 รันโมเดล Llama 2 ดีขึ้น 4 เท่าจาก H100","link":"https://www.blognone.com/node/141727","date":1724931014000,"content":"<div><div><div><p>NVIDIA เผยผลเบนช์มาร์คแรกของ<a href=\"https://www.blognone.com/node/138768\">จีพียู Blackwell B200</a> โดยเป็นเบนช์มาร์ค <a href=\"https://mlcommons.org/benchmarks/training/\">MLPerf Inference: Datacenter</a> ที่นิยมใช้กันในวงการ AI ทดสอบกับ<a href=\"https://www.blognone.com/node/134893\">โมเดล Llama 2 70B</a> ผลคือได้ประสิทธิภาพต่อจีพียูสูงกว่าชิป H100 (Hopper) ประมาณ 4 เท่าตัว</p>\n<p>ผลการทดสอบของ NVIDIA ใช้เครื่องซีพียู Xeon Silver 4410Y ร่วมกับชิป B200 แรม 180GB ได้คะแนนออกมาดังนี้</p>\n<ul>\n<li>โหมด Offline (ชุดทดสอบส่งข้อมูลตัวอย่างทั้งหมดให้เซิร์ฟเวอร์รวดเดียว) ได้ 11,264 token/s (เพิ่ม 3.7x เทียบกับ H100)</li>\n<li>โหมด Server (ชุดทดสอบทยอยส่งข้อมูลตัวอย่างให้เซิร์ฟเวอร์ เลียนแบบการใช้งานจริง) ได้ 10,756 (เพิ่ม 4x เทียบกับ H100)</li>\n</ul>\n<p>NVIDIA บอกว่าประสิทธิภาพที่เพิ่มขึ้น มาจากฟีเจอร์ Blackwell FP4 Transformer Engine ที่ต้องแปลงโมเดลเป็นข้อมูลประเภท FP4 ก่อน ช่วยให้ประสิทธิภาพในการรันโมเดลเร็วขึ้นอย่างก้าวกระโดด เพราะจีพียูมีเอนจินสำหรับประมวลผล FP4 ในตัว</p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/ea9e531765bdb470339b6937a3a77db1.jpg\" /></p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/fc29ac05fc4540b156da3da8e38ed5c8.png\" /></p>\n<p>NVIDIA ยังโชว์ผลการรัน MLPerf กับ<a href=\"https://www.blognone.com/node/136718\">จีพียู H200 รุ่นท็อปสุดในปัจจุบันที่ใช้แรมความเร็วสูงแบบ HBM3e</a> มีแบนด์วิดท์แรมเพิ่มขึ้น 1.4x เทียบกับ H100</p>\n<p>การรันโมเดล Llama 2 70B โหมด Server โดยใช้จีพียู H200 จำนวน 8 ตัว ได้ผลลัพธ์ที่ 32,790 token/s หรือประมาณ 4,098 token/s ต่อจีพียูหนึ่งตัว</p>\n<p>ที่มา - <a href=\"https://developer.nvidia.com/blog/nvidia-blackwell-platform-sets-new-llm-inference-records-in-mlperf-inference-v4-1/\">NVIDIA</a></p>\n<p>หมายเหตุ: แผนภาพอธิบายโหมดการทำงานของ MLPerf, ภาพจาก <a href=\"https://developer.nvidia.com/blog/nvidia-mlperf-v05-ai-inference/\">NVIDIA</a></p>\n<p><img alt=\"No Description\" src=\"https://www.blognone.com/sites/default/files/externals/0bdc24a778d6b76f302eafe8fb7c756c.png\" /></p>\n</div></div></div><div><div>Topics: </div><div><div><a href=\"/topics/nvidia\">NVIDIA</a></div><div><a href=\"/topics/blackwell\">Blackwell</a></div><div><a href=\"/topics/gpu\">GPU</a></div><div><a href=\"/topics/benchmark\">Benchmark</a></div><div><a href=\"/topics/artificial-intelligence\">Artificial Intelligence</a></div></div></div>","author":"mk","siteTitle":"Blognone","siteHash":"ededadcf18490b3937e7dd89ebe8c00dc129addbdf1ebe4aff1f458146693da0","entryHash":"01ca88f518d607285e9ec3e9b5b08c84ad9a84d62b07749b5a83c9858ebaf8eb","category":"Thai"}