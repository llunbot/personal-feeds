{"title":"มาใช้งาน LiteLLM กัน","link":"https://www.somkiat.cc/litellm-llm-proxy-server/","date":1728195125000,"content":"<p><img width=\"150\" height=\"150\" src=\"https://www.somkiat.cc/wp-content/uploads/2024/10/litellm-01-150x150.png\" loading=\"lazy\" srcset=\"https://www.somkiat.cc/wp-content/uploads/2024/10/litellm-01-150x150.png 150w, https://www.somkiat.cc/wp-content/uploads/2024/10/litellm-01-75x75.png 75w\" /></p>\n<p>ปัญหาอย่างหนึ่งสำหรับการใช้งาน LLM provider ต่าง ๆ  ที่มีอยู่มากมาย<br />ทั้ง OpenAI, Antropic, Ollama, Groq เป็นต้น<br />แต่ละ provider จะมีการเตรียม API ที่ต่างกัน format ของ input/output ที่ต่างกัน<br />ดังนั้น <strong><a href=\"https://docs.litellm.ai/\" target=\"_blank\">LiteLLM</a></strong> จึงเกิดมาเพื่อจัดการปัญหานี้</p>\n\n\n\n<span></span>\n\n\n\n<p>ได้เตรียมสิ่งต่าง ๆ  เหล่านี้ไว้ให้</p>\n\n\n\n<ul>\n<li>รูปแบบของ input/output เดียวกัน ไม่ว่าจะใช้งาน LLM provider อะไร</li>\n\n\n\n<li>สามารถ track การใช้งานทั้ง token หรือ ราคาต่าง ๆ  ได้ เพราะว่า LiteLLM ทำตัวเป็น proxy นั่นเอง รวมทั้งเรื่องของ rate limit การใช้งานได้อีกด้วย</li>\n\n\n\n<li>มี Retry and fallback ให้ใช้งาน เพื่อช่วยให้ระบบ scale ได้ง่ายขึ้น</li>\n</ul>\n\n\n\n<p><strong>โครงสร้างการทำงานเป็นดังรูป</strong></p>\n\n\n\n<figure><a href=\"https://www.somkiat.cc/wp-content/uploads/2024/10/litellm-01.png\"><img src=\"https://www.somkiat.cc/wp-content/uploads/2024/10/litellm-01-1024x630.png\" width=\"698\" height=\"429\" /></a></figure>\n\n\n\n<p><strong>LiteLLM จะเตรียมสิ่งต่าง ๆ  หล่านี้ไว้ให้ ในการติดตั้งและใช้งาน</strong></p>\n\n\n\n<ul>\n<li><strong>LiteLLM Proxy server หรือ LLM Gateway</strong> มีความสามารถเช่น UI ในการจัดการ, Auth, Logging, Cost tracking และ RAte limit</li>\n\n\n\n<li><strong>LLM Python SDK</strong> ช่วยให้ง่ายต่อการเขียน code ด้วยภาษา Python ช่วยให้เราเขียน code ชุดเดียว แต่สามารถใช้งานพวก LLM provider ต่าง ๆ  ได้เลย ไม่ต้องไปเขียนตาม API ของแต่ละที่ เป็นอีกทางเลือกที่น่าสนใจ</li>\n</ul>\n\n\n\n<p>ลองใช้งานกันดู</p>\n\n\n\n<p><strong>หรือถ้าใครอยากได้ระบบ UI ที่มี interface แบบ Chat แล้วเชื่อมต่อมายัง LiteLLM proxy server</strong></p>\n\n\n\n<p>แถมยังสามารถเลือก LLM provider ได้อีก แนะนำ <a href=\"https://openwebui.com/\" target=\"_blank\">OpenWebUI</a> ใช้ง่ายดี</p>\n\n\n\n<figure><a href=\"https://www.somkiat.cc/wp-content/uploads/2024/10/openwebio-01.jpg\"><img src=\"https://www.somkiat.cc/wp-content/uploads/2024/10/openwebio-01.jpg\" width=\"575\" height=\"362\" /></a></figure>\n\n\n\n<p><strong>แต่สิ่งที่น่าสนใจ และ ผมนำมาใช้งานคือ <a href=\"https://docs.litellm.ai/docs/proxy/reliability\" target=\"_blank\">Load balancer และ fallback</a></strong></p>\n\n\n\n<p>เมื่อมี request เข้ามาจำนวนมาก<br />แน่นอนว่าจะติด limit ของ LLM provider แต่ละที่<br />ดังนั้นจึงแก้ไขปัญหานี้ด้วยการใช้ความสามารถของ LiteLLM มาช่วย</p>\n\n\n\n<figure><a href=\"https://www.somkiat.cc/wp-content/uploads/2024/10/litellm-02.png\"><img src=\"https://www.somkiat.cc/wp-content/uploads/2024/10/litellm-02-1024x347.png\" width=\"794\" height=\"268\" /></a></figure>\n\n\n\n<p></p>\n\n\n\n<p>ลองใช้งานกันดูครับ มี Docker compose ให้ใช้งานแบบง่าย ๆ  เลย<br />ขอให้สนุกครับ</p>\n","author":"somkiat","siteTitle":"cc :: somkiat","siteHash":"3a23a5a4389e1e40c6fbb16520a8cc20df5b3591c25145ce72aaa18b19e48201","entryHash":"f58c41afbcfe8bc4406c71f337746413d99d8c45656c6ed37545f21a7bed77f5","category":"Thai"}