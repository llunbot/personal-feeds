{"title":"Continuous batch enables 23x throughput in LLM inference and reduce p50 latency","link":"https://www.anyscale.com/blog/continuous-batching-llm-inference","date":1692087671000,"content":"<a href=\"https://news.ycombinator.com/item?id=37131477\">Comments</a>","author":"","siteTitle":"Hacker News","siteHash":"37bb545430005dba450c1e40307450d8e4e791b434e83f3d38915ebad510fd50","entryHash":"5054169662ab885ef96e936649e3643b544b926af3b1e161bfc2a39d1a7d7969","category":"Tech"}