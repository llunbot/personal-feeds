{"title":"Apple will soon scan all iCloud images for child abuse without decryption","link":"https://www.macworld.com/article/352710/apple-will-soon-scan-all-icloud-images-for-child-abuse-without-decryption.html","date":1628198427000,"content":"<div>\n<section><div></div></section><p><a href=\"https://go.redirectingat.com/?id=111346X1569486&amp;url=https://techcrunch.com/2021/08/05/apple-icloud-photos-scanning/&amp;xcust=1-1-352710-1-0-0&amp;sref=https://www.macworld.com/feed\">TechCrunch has confirmed</a> that Apple will soon roll out a new technology to scan photos uploaded to iCloud for child sexual abuse material (CSAM). The rollout will happen later this year as part of a collection of technologies meant to make its products and services safer for children to use.</p>\n\n\n\n<p>Most cloud services already scan images for material that violates its terms of service or the law, including CSAM. They can do this because, while the images may be stored encrypted, the companies have the encryption key. Apple’s longstanding policy on privacy gives users the ability to encrypt photos on-device and store them with Apple, without Apple having the ability to decrypt them.</p>\n\n\n\n<p>The company is therefore relying on a new technology called NeuralHash that will check to see if an image uploaded to iCloud matches known child abuse imagery, without decrypting the photo. It works entirely on your iPhone, iPad, or Mac by converting photos into a unique string of letters and numbers (a “hash”). Normally, any slight change to a photo would result in a different hash, but Apple’s technology is said to be such that small alterations (like a crop) will still result in the same hash. </p>\n\n\n\n<p>These hashes would be matched on-device to a database of hashes for images of child sexual abuse. The hashes can be matched invisibly, without knowing what the underlying image is or alerting the user in any way. The results of the matches are uploaded to Apple if a certain threshold is met. Only then can Apple decrypt the matching images, manually verify the contents, and disable a user’s account. Apple will then report the imagery to the National Center for Missing &amp; Exploited Children, which then passes it to law enforcement.</p>\n\n\n\n<p>In other words, it’s extremely unlikely that Apple will have the ability to decrypt a user’s photos. According to TechCrunch, Apple says there is a one in one trillion chance of a false positive, and there will be an appeals process in place for anyone who thinks their account was flagged by mistake. The technology is only <em>sort of</em> optional: You don’t have to use iCloud Photos, but if you do, you will not be able to disable the feature.</p>\n\n\n\n<p><a href=\"https://apple.sjv.io/c/321564/435031/7613?u=https://www.apple.com/child-safety/pdf/Apple_PSI_System_Security_Protocol_and_Analysis.pdf&amp;subid1=1-1-352710-1-0-0\">Apple has published a technical paper</a> detailing this NeuralHash technology. This new technology will roll out as part of <a href=\"https://www.macworld.com/article/350966/ios-15-super-guide-features-tips-tricks-updates-safari-facetime-focus-notification-summary.html\">iOS 15</a>, <a href=\"https://www.macworld.com/article/348175/ipados-15-features-features-multitasking-widgets-home-screen-app-library-compatibility-release-date-beta-how-to-install.html\">iPadOS 15</a>, and <a href=\"https://www.macworld.com/article/348192/macos-12-monterey-features-universal-control-shortcuts-safari-airplay-focus-compatibility-release-beta-how-to-install.html\">macOS Monterey</a> this fall.</p>\n</div>","author":"Jason Cross","siteTitle":"Macworld","siteHash":"37e84dd5a21fa961d6d6630e269546024dbb7741b2e2fadbe74f47383c70dfbb","entryHash":"55b8a3a0b87f91e12f65d16717d84fa3adb2bdb404765aaa9833318b1b5075f1","category":"Apple"}