{"title":"Using ChatGPT to make fake social media posts backfires on bad actors","link":"https://arstechnica.com/tech-policy/2024/10/using-chatgpt-to-make-fake-social-media-posts-backfires-on-bad-actors/","date":1728584662000,"content":"<p>Using ChatGPT to research cyber threats has backfired on bad actors, OpenAI revealed in a <a href=\"https://cdn.openai.com/threat-intelligence-reports/influence-and-cyber-operations-an-update_October-2024.pdf\">report</a> analyzing emerging trends in how AI is currently amplifying online security risks.</p>\n<p>Not only do ChatGPT prompts expose what platforms bad actors are targeting—and in at least one case enabled OpenAI to link a covert influence campaign on X and Instagram for the first time—but they can also reveal new tools that threat actors are testing to evolve their deceptive activity online, OpenAI claimed.</p>\n<p>OpenAI's report comes amid heightening scrutiny of its tools during a major election year where officials globally fear AI might be used to boost disinformation and propaganda like never before. Their report detailed 20 times OpenAI disrupted covert influence operations and deceptive networks attempting to use AI to sow discord or breach vulnerable systems.</p><p><a href=\"https://arstechnica.com/tech-policy/2024/10/using-chatgpt-to-make-fake-social-media-posts-backfires-on-bad-actors/\">Read full article</a></p>\n<p><a href=\"https://arstechnica.com/tech-policy/2024/10/using-chatgpt-to-make-fake-social-media-posts-backfires-on-bad-actors/#comments\">Comments</a></p>","author":"Ashley Belanger","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"7c0041218f9ae25a8079d2092dd75de8a730deae380c54f664ca0c30bb6f5742","category":"Tech"}