{"title":"Google upstages itself with Gemini 1.5 AI launch, one week after Ultra 1.0","link":"https://arstechnica.com/?p=2003673","date":1708029937000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/02/gemini_15-800x334.jpg\" alt=\"The Gemini 1.5 logo\" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2024/02/gemini_15.jpg\">Enlarge</a> <span>/</span> The Gemini 1.5 logo, released by Google. (credit: Google)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>One week after its last major AI announcement, Google appears to have upstaged itself. Last Thursday, Google <a href=\"https://arstechnica.com/information-technology/2024/02/google-debuts-more-powerful-ultra-1-0-ai-model-in-rebranded-gemini-chatbot/\">launched Gemini Ultra 1.0</a>, which supposedly represented the best AI language model Google could muster—available as part of the renamed \"Gemini\" AI assistant (formerly Bard). Today, Google <a href=\"https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/\">announced</a> Gemini Pro 1.5, which it says \"achieves comparable quality to 1.0 Ultra, while using less compute.\"</p>\n\n<p>Congratulations, Google, you've done it. You've undercut your own premiere AI product. While Ultra 1.0 is possibly still better than Pro 1.5 (what even are we saying here), Ultra was presented as a key selling point of its \"Gemini Advanced\" tier of its Google One subscription service. And now it's looking a lot less advanced than seven days ago. All this is on top of the <a href=\"https://arstechnica.com/information-technology/2024/02/google-debuts-more-powerful-ultra-1-0-ai-model-in-rebranded-gemini-chatbot/\">confusing name-shuffling</a> Google has been doing recently. (Just to be clear—although it's not really clarifying at all—the free version of Bard/Gemini currently uses the Pro 1.0 model. Got it?)</p>\n<p>Google claims that <a href=\"https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf\">Gemini 1.5</a> represents a new generation of LLMs that \"delivers a breakthrough in long-context understanding,\" and that it can process up to 1 million tokens, \"achieving the longest context window of any large-scale foundation model yet.\" Tokens are fragments of a word. The first part of the claim about \"understanding\" is contentious and subjective, but the second part is probably correct. OpenAI's <a href=\"https://arstechnica.com/information-technology/2023/11/openai-introduces-gpt-4-turbo-larger-memory-lower-cost-new-knowledge/\">GPT-4 Turbo</a> can reportedly handle 128,000 tokens in some circumstances, and 1 million is quite a bit more—about 700,000 words. A larger context window allows for processing longer documents and having longer conversations. (The <a href=\"https://arstechnica.com/information-technology/2023/12/google-launches-gemini-a-powerful-ai-model-it-says-can-surpass-gpt-4/\">Gemini 1.0 model family</a> handles 32,000 tokens max.)</p></div><p><a href=\"https://arstechnica.com/?p=2003673#p3\">Read 6 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=2003673&amp;comments=1\">Comments</a></p>","author":"Benj Edwards","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"eb46ce7677d2803bdccdf441ef9c8d723b02b30822677a5a5d7bf2dfb93ae0fe","category":"Tech"}