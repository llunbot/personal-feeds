{"title":"Nvidia introduces the H200, an AI-crunching monster GPU that may speed up ChatGPT","link":"https://arstechnica.com/?p=1983396","date":1699911884000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2023/11/nvidia_h200_hero_2-800x450.jpg\" alt=\"The Nvidia H200 GPU covered with a blue explosion.\" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2023/11/nvidia_h200_hero_2.jpg\">Enlarge</a> <span>/</span> Eight Nvidia H200 GPUs covered with a fanciful blue explosion that figuratively represents raw compute power bursting forth in a glowing flurry. (credit: Nvidia | Benj Edwards)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>On Monday, Nvidia <a href=\"https://nvidianews.nvidia.com/news/nvidia-supercharges-hopper-the-worlds-leading-ai-computing-platform\">announced</a> the HGX <a href=\"https://www.nvidia.com/en-gb/data-center/h200/\">H200</a> Tensor Core GPU, which utilizes the Hopper architecture to accelerate AI applications. It's a follow-up of the <a href=\"https://arstechnica.com/information-technology/2022/09/hopper-time-nvidias-most-powerful-ai-chip-yet-ships-in-october/\">H100 GPU</a>, released last year and previously Nvidia's most powerful AI GPU chip. If widely deployed, it could lead to far more powerful AI models—and faster response times for existing ones like <a href=\"https://arstechnica.com/information-technology/2022/12/openai-invites-everyone-to-test-new-ai-powered-chatbot-with-amusing-results/\">ChatGPT</a>—in the near future.</p>\n<p>According to experts, lack of computing power (often called \"compute\") has been a <a href=\"https://www.cnn.com/2023/08/06/tech/ai-chips-supply-chain/index.html\">major bottleneck</a> of AI progress this past year, hindering deployments of existing AI models and slowing the development of new ones. <a href=\"https://arstechnica.com/information-technology/2023/10/openai-may-jump-into-ai-hardware-amid-high-costs-supply-constraints/\">Shortages</a> of powerful GPUs that accelerate AI models are largely to blame. One way to alleviate the compute bottleneck is to make more chips, but you can also make AI chips more powerful. That second approach may make the H200 an attractive product for cloud providers.</p>\n\n<p>What's the H200 good for? Despite the \"G\" in the \"GPU\" name, data center GPUs like this typically aren't for graphics. GPUs are ideal for AI applications because they perform vast numbers of parallel matrix multiplications, which are necessary for neural networks to function. They are essential in the training portion of building an AI model and the \"inference\" portion, where people feed inputs into an AI model and it returns results.</p></div><p><a href=\"https://arstechnica.com/?p=1983396#p3\">Read 7 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=1983396&amp;comments=1\">Comments</a></p>","author":"Benj Edwards","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"4f46f65486755733038cdf4ad9a4b5a039815c8c02370a39b9a3588802f1ff7a","category":"Tech"}