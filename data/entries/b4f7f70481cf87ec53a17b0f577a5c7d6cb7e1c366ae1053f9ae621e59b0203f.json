{"title":"Microsoft CTO defies critics: AI progress not slowing down, itâ€™s just warming up","link":"https://arstechnica.com/?p=2036166","date":1721062983000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/07/kevin_scott-800x450.jpg\" alt=\"Kevin Scott, CTO and EVP of AI at Microsoft speaks onstage during Vox Media's 2023 Code Conference at The Ritz-Carlton, Laguna Niguel on September 27, 2023 in Dana Point, California.\" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2024/07/kevin_scott.jpg\">Enlarge</a> <span>/</span> Kevin Scott, CTO and EVP of AI at Microsoft speaks onstage during Vox Media's 2023 Code Conference at The Ritz-Carlton, Laguna Niguel on September 27, 2023 in Dana Point, California. (credit: <a href=\"https://www.gettyimages.com/detail/news-photo/kevin-scott-cto-and-evp-of-ai-at-microsoft-speaks-onstage-news-photo/1704526501\">Getty Images</a>)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>During an <a href=\"https://www.youtube.com/watch?v=aTQWymHp0n0\">interview</a> with Sequoia Capital's Training Data podcast published last Tuesday, Microsoft CTO Kevin Scott doubled down on his belief that so-called large language model (LLM) \"scaling laws\" will continue to drive AI progress, despite some skepticism in the field that progress has leveled out. Scott played a <a href=\"https://www.fastcompany.com/90957311/how-microsoft-cto-kevin-scott-helped-forge-the-companys-deal-with-openai\">key role</a> in forging a $13 billion technology-sharing <a href=\"https://arstechnica.com/information-technology/2023/01/openai-and-microsoft-reaffirm-shared-quest-for-powerful-ai-with-new-investment/\">deal</a> between Microsoft and OpenAI.</p>\n\n<p>\"Despite what other people think, we're not at diminishing marginal returns on scale-up,\" Scott said. \"And I try to help people understand there is an exponential here, and the unfortunate thing is you only get to sample it every couple of years because it just takes a while to build supercomputers and then train models on top of them.\"</p>\n<p>LLM scaling laws refer to patterns explored by OpenAI researchers in 2020 showing that the performance of language models tends to improve predictably as the models get larger (more parameters), are trained on more data, and have access to more computational power (compute). The laws suggest that simply scaling up model size and training data can lead to significant improvements in AI capabilities without necessarily requiring fundamental algorithmic breakthroughs.</p></div><p><a href=\"https://arstechnica.com/?p=2036166#p3\">Read 9 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=2036166&amp;comments=1\">Comments</a></p>","author":"Benj Edwards","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"b4f7f70481cf87ec53a17b0f577a5c7d6cb7e1c366ae1053f9ae621e59b0203f","category":"Tech"}