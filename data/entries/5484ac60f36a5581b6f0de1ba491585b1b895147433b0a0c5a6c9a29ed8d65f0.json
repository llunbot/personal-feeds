{"title":"Matrix multiplication breakthrough could lead to faster, more efficient AI models","link":"https://arstechnica.com/?p=2008905","date":1709932038000,"content":"<div>\n<figure>\n  <img src=\"https://cdn.arstechnica.net/wp-content/uploads/2024/03/matrix_hero_1-800x450.jpg\" alt=\"Futuristic huge technology tunnel and binary data.\" />\n      <p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2024/03/matrix_hero_1.jpg\">Enlarge</a> <span>/</span> When you do math on a computer, you fly through a numerical tunnel like thisâ€”figuratively, of course. (credit: <a href=\"https://www.gettyimages.com/detail/photo/futuristic-huge-technology-tunnel-royalty-free-image/1405821635\">Getty Images</a>)</p>  </figure>\n\n\n\n\n\n\n<div><a name=\"page-1\"></a></div>\n<p>Computer scientists have discovered a new way to multiply large matrices faster than ever before by eliminating a previously unknown inefficiency, reports <a href=\"https://www.quantamagazine.org/new-breakthrough-brings-matrix-multiplication-closer-to-ideal-20240307/\">Quanta Magazine</a>. This could eventually accelerate AI models like <a href=\"https://arstechnica.com/information-technology/2023/11/chatgpt-was-the-spark-that-lit-the-fire-under-generative-ai-one-year-ago-today/\">ChatGPT</a>, which rely heavily on matrix multiplication to function. The findings, presented in two recent papers, have led to what is reported to be the biggest improvement in matrix multiplication efficiency in over a decade.</p>\n<p>Multiplying two rectangular number arrays, known as <a href=\"https://en.wikipedia.org/wiki/Matrix_multiplication\">matrix multiplication</a>, plays a crucial role in today's AI models, including speech and image recognition, chatbots from every major vendor, AI image generators, and video synthesis models like <a href=\"https://arstechnica.com/information-technology/2024/02/openai-collapses-media-reality-with-sora-a-photorealistic-ai-video-generator/\">Sora</a>. Beyond AI, matrix math is so important to modern computing (think image processing and data compression) that even slight gains in efficiency could lead to computational and power savings.</p>\n\n<p>Graphics processing units (GPUs) excel in handling matrix multiplication tasks because of their ability to process many calculations at once. They break down large matrix problems into smaller segments and solve them concurrently using an algorithm.</p></div><p><a href=\"https://arstechnica.com/?p=2008905#p3\">Read 11 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=2008905&amp;comments=1\">Comments</a></p>","author":"Benj Edwards","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"5484ac60f36a5581b6f0de1ba491585b1b895147433b0a0c5a6c9a29ed8d65f0","category":"Tech"}