{"title":"ว่าด้วยเรื่องของ Prompt Caching","link":"https://www.somkiat.cc/prompt-caching/","date":1728531198000,"content":"<p><img width=\"150\" height=\"150\" src=\"https://www.somkiat.cc/wp-content/uploads/2024/10/prompt-caching-150x150.jpeg\" loading=\"lazy\" srcset=\"https://www.somkiat.cc/wp-content/uploads/2024/10/prompt-caching-150x150.jpeg 150w, https://www.somkiat.cc/wp-content/uploads/2024/10/prompt-caching-75x75.jpeg 75w\" /></p>\n<figure><a href=\"https://www.somkiat.cc/wp-content/uploads/2024/10/prompt-caching.jpeg\"><img src=\"https://www.somkiat.cc/wp-content/uploads/2024/10/prompt-caching.jpeg\" width=\"596\" height=\"340\" /></a></figure>\n\n\n\n<p>Prompt caching เป็นอีกหนึ่ง feature ที่น่าสนใจ<br />ที่ provider ต่าง ๆ  เพิ่มเข้ามา ทั้ง OpenAI, Gemini, Anthropic และ DeepSeek<br />ซึ่งจะทำ caching ของ prompt + context ต่าง ๆ  ให้เรา<br />เพื่อช่วยลดค่าใช้จ่ายลงไป รวมทั้งช่วยให้สามารถ share context ได้<br />และเพิ่ม performance ของการทำงานอีกด้วย <br />นั่นคือ ลด response time ลงไป</p>\n\n\n\n<span></span>\n\n\n\n<p><strong>โดยที่ OpenAI จะทำการ caching ให้กับทุก ๆ  API ที่ใช้งาน model เหล่านี้</strong></p>\n\n\n\n<ul>\n<li>gpt-4o (excludes gpt-4o-2024-05-13 and chatgpt-4o-latest)</li>\n\n\n\n<li>gpt-4o-mini</li>\n\n\n\n<li>o1-preview</li>\n\n\n\n<li>o1-mini</li>\n</ul>\n\n\n\n<p>ซึ่งจะทำ caching ให้อัตโนมัติ สำหรับ prompt ที่มีความยาวมากกว่า 1,024 tokens<br />จะเหมาะมาก ๆ  สำหรับ structured prompt</p>\n\n\n\n<figure><a href=\"https://www.somkiat.cc/wp-content/uploads/2024/10/openai-caching.jpg\"><img src=\"https://www.somkiat.cc/wp-content/uploads/2024/10/openai-caching.jpg\" width=\"599\" height=\"311\" /></a></figure>\n\n\n\n<p>สิ่งที่ OpenAI จะทำ caching ให้ประกอบไปด้วย</p>\n\n\n\n<ul>\n<li>Message</li>\n\n\n\n<li>Images</li>\n\n\n\n<li>Structured output</li>\n</ul>\n\n\n\n<p><strong>ส่วนของ Anthropic นั้นต้องเขียนใช้งาน caching ใน code เลย</strong></p>\n\n\n\n<p>โดยใช้งาน anthropic.beta.prompt_caching.messages.create()<br />จำนวนของ token ที่จะทำ caching ได้แบ่งตาม model ดังนี้</p>\n\n\n\n<ul>\n<li>Claude Sonnet &gt;= 1,024 tokens</li>\n\n\n\n<li>Claude Haiku  &gt;= 2,048 tokens</li>\n</ul>\n\n\n\n<p><strong>โดย <a href=\"https://cookbook.openai.com/examples/prompt_caching101\" target=\"_blank\">use cases</a> ที่เหมาะต่อการใช้งาน</strong></p>\n\n\n\n<ul>\n<li>Agent ที่ใช้งาน tool และ structured output</li>\n\n\n\n<li>Chatbot เพื่อเห็บ context ที่ใช้ในการพูดคุย</li>\n\n\n\n<li>กลุ่มของ Coding และ writing assistant</li>\n</ul>\n\n\n\n<p>ดังนั้นอะไรที่เป็น static จะใช้ประโยชน์จาก caching อย่างมาก</p>\n\n\n\n<p>ลองใช้งานกันดูครับ</p>\n\n\n\n<p><strong>Reference Websites</strong></p>\n\n\n\n<ul>\n<li>OpenAI :: <a href=\"https://openai.com/index/api-prompt-caching/\">Prompt Caching in the API</a></li>\n\n\n\n<li>Anthropic :: <a href=\"https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching\" target=\"_blank\">Prompt Caching (beta)</a></li>\n\n\n\n<li>Gemini :: <a href=\"https://ai.google.dev/gemini-api/docs/caching?lang=python\" target=\"_blank\">Context caching</a></li>\n\n\n\n<li>DeepSeek :: <a href=\"https://platform.deepseek.com/api-docs/news/news0802/\" target=\"_blank\">Context caching</a></li>\n</ul>\n\n\n\n<p></p>\n","author":"somkiat","siteTitle":"cc :: somkiat","siteHash":"3a23a5a4389e1e40c6fbb16520a8cc20df5b3591c25145ce72aaa18b19e48201","entryHash":"bbcacc461c44c2946adfa0280fa5781fc8469c3b5600e7277dddafa15080ff6a","category":"Thai"}