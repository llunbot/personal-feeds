{"title":"Apple defends iPhone photo scanning, calls it an “advancement” in privacy","link":"https://arstechnica.com/?p=1787043","date":1628872880000,"content":"<div>\n<figure><img src=\"https://cdn.arstechnica.net/wp-content/uploads/2021/08/getty-federighi-800x543.jpg\" /><p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2021/08/getty-federighi.jpg\">Enlarge</a> <span>/</span> Apple executive Craig Federighi speaks during the 2018 Apple Worldwide Developer Conference (WWDC) in San Jose, California. (credit: Getty Images | Justin Sullivan)</p>  </figure><div><a name=\"page-1\"></a></div>\n<p>Apple's decision to have iPhones and other Apple devices scan photos for child sexual abuse material (CSAM) has sparked criticism from security experts and privacy advocates—and from some Apple employees. But Apple believes its new system is an advancement in privacy that will \"enabl[e] a more private world,\" according to Craig Federighi, the company's senior VP of software engineering.</p>\n<p>Federighi defended the new system in an interview with The Wall Street Journal, saying that Apple is aiming to detect child sexual abuse photos in a way that protects user privacy more than other, more invasive scanning systems. The Journal <a href=\"https://www.wsj.com/articles/apple-executive-defends-tools-to-fight-child-porn-acknowledges-privacy-backlash-11628859600\">wrote today</a>:</p>\n<blockquote><p>While Apple's new efforts have drawn praise from some, the company has also received criticism. An executive at Facebook Inc.'s WhatsApp messaging service and others, including Edward Snowden, have called Apple's approach bad for privacy. The overarching concern is whether Apple can use software that identifies illegal material without the system being taken advantage of by others, such as governments, pushing for more private information—a suggestion Apple strongly denies and Mr. Federighi said will be protected against by \"multiple levels of auditability.\"</p>\n<p>\"We, who consider ourselves absolutely leading on privacy, see what we are doing here as an advancement of the state of the art in privacy, as enabling a more private world,\" Mr. Federighi said.</p></blockquote>\n<p>In a <a href=\"https://www.wsj.com/video/series/joanna-stern-personal-technology/apples-software-chief-explains-misunderstood-iphone-child-protection-features-exclusive/573D76B3-5ACF-4C87-ACE1-E99CECEFA82C?mod=hp_lead_pos5\">video of the interview</a>, Federighi said, \"[W]hat we're doing is we're finding illegal images of child pornography stored in iCloud. If you look at any other cloud service, they currently are scanning photos by looking at every single photo in the cloud and analyzing it. We wanted to be able to spot such photos in the cloud without looking at people's photos and came up with an architecture to do this.\" The Apple system is not a \"backdoor\" that breaks encryption and is \"much more private than anything that's been done in this area before,\" he said. Apple developed the architecture for identifying photos \"in the most privacy-protecting way we can imagine and in the most auditable and verifiable way possible,\" he said.</p></div><p><a href=\"https://arstechnica.com/?p=1787043#p3\">Read 29 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=1787043&amp;comments=1\">Comments</a></p>","author":"Jon Brodkin","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"2964737ef19e0cb7efb491c15c96beb023feed2f005d9c13aedeebcbfe585820","category":"Tech"}