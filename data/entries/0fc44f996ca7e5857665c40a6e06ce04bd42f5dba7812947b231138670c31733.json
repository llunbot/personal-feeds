{"title":"Accelerate foundation model training and fine-tuning with new Amazon SageMaker HyperPod recipes","link":"https://aws.amazon.com/blogs/aws/accelerate-foundation-model-training-and-fine-tuning-with-new-amazon-sagemaker-hyperpod-recipes/","date":1733336476000,"content":"<p>Today, we’re announcing the general availability of <a href=\"https://github.com/aws/sagemaker-hyperpod-recipes\">Amazon SageMaker HyperPod recipes</a> to help data scientists and developers of all skill sets to get started training and fine-tuning <a href=\"https://aws.amazon.com/what-is/foundation-models/\">foundation models</a> (FMs) in minutes with state-of-the-art performance. They can now access optimized recipes for training and fine-tuning popular publicly available FMs such as <a href=\"https://github.com/aws/sagemaker-hyperpod-recipes/blob/main/recipes_collection/recipes/fine-tuning/llama/hf_llama3_405b_seq128k_gpu_qlora.yaml\">Llama 3.1 405B</a>, <a href=\"https://github.com/aws/sagemaker-hyperpod-recipes/blob/main/recipes_collection/recipes/training/llama/hf_llama3_2_90b_seq8k_gpu_p5x32_pretrain.yaml\">Llama 3.2 90B</a>, or <a href=\"https://github.com/aws/sagemaker-hyperpod-recipes/blob/main/recipes_collection/recipes/training/mixtral/hf_mixtral_8x22b_seq8k_gpu_p5x32_pretrain.yaml\">Mixtral 8x22B</a>.</p> \n<p>At AWS re:Invent 2023, we <a href=\"https://aws.amazon.com/blogs/aws/introducing-amazon-sagemaker-hyperpod-a-purpose-built-infrastructure-for-distributed-training-at-scale/\">introduced SageMaker HyperPod</a> to reduce time to train FMs by up to 40 percent and scale across more than a thousand compute resources in parallel with preconfigured distributed training libraries. With SageMaker HyperPod, you can find the required accelerated compute resources for training, create the most optimal training plans, and run training workloads across different blocks of capacity based on the availability of compute resources.</p> \n<p>SageMaker HyperPod recipes include a training stack tested by AWS, removing tedious work experimenting with different model configurations, eliminating weeks of iterative evaluation and testing. The recipes automate several critical steps, such as loading training datasets, applying distributed training techniques, automating checkpoints for faster recovery from faults, and managing the end-to-end training loop.</p> \n<p>With a simple recipe change, you can seamlessly switch between GPU- or Trainium-based instances to further optimize training performance and reduce costs. You can easily run workloads in production on SageMaker HyperPod or SageMaker training jobs.</p> \n<p><u><strong>SageMaker HyperPod recipes in action</strong><br /> </u>To get started, visit the <a href=\"https://github.com/aws/sagemaker-hyperpod-recipes\">SageMaker HyperPod recipes GitHub repository</a> to browse training recipes for popular publicly available FMs.</p> \n<p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/12/03/2024-sagemaker-hyperpod-recipes-github-1.png\" width=\"1206\" height=\"1332\" /></p> \n<p>You only need to edit straightforward recipe parameters to specify an instance type and the location of your dataset in cluster configuration, then run the recipe with a single line command to achieve state-of-art performance.</p> \n<p>You need to edit the recipe config.yaml file to specify the model and cluster type after cloning the repository.</p> \n<pre><code>$ git clone --recursive https://github.com/aws/sagemaker-hyperpod-recipes.git\n$ cd sagemaker-hyperpod-recipes\n$ pip3 install -r requirements.txt.\n$ cd ./recipes_collections\n$ vim config.yaml</code></pre> \n<p>The recipes support <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-slurm.html\">SageMaker HyperPod with Slurm</a>, <a href=\"https://aws.amazon.com/blogs/aws/amazon-sagemaker-hyperpod-introduces-amazon-eks-support/\">SageMaker HyperPod with Amazon Elastic Kubernetes Service (Amazon EKS)</a>, and <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html\">SageMaker training jobs</a>. For example, you can set up a cluster type (Slurm orchestrator), a model name (Meta Llama 3.1 405B language model), an instance type (<code>ml.p5.48xlarge</code>), and your data locations, such as storing the training data, results, logs, and so on.</p> \n<pre><code>defaults:\n- <strong>cluster: slurm</strong> # support: slurm / k8s / sm_jobs\n- <strong>recipes: fine-tuning/llama/hf_llama3_405b_seq8k_gpu_qlora</strong> # name of model to be trained\ndebug: False # set to True to debug the launcher configuration\n<strong>instance_type: ml.p5.48xlarge</strong> # or other supported cluster instances\nbase_results_dir: # Location(s) to store the results, checkpoints, logs etc.</code></pre> \n<p>You can optionally adjust model-specific training parameters in this YAML file, which outlines the optimal configuration, including the number of accelerator devices, instance type, training precision, parallelization and sharding techniques, the optimizer, and logging to monitor experiments through <a href=\"https://www.tensorflow.org/tensorboard\">TensorBoard</a>.</p> \n<pre><code>run:\n  name: llama-405b\n  results_dir: ${base_results_dir}/${.name}\n  time_limit: \"6-00:00:00\"\nrestore_from_path: null\ntrainer:\n  devices: 8\n  num_nodes: 2\n  accelerator: gpu\n  precision: bf16\n  max_steps: 50\n  log_every_n_steps: 10\n  ...\nexp_manager:\n  exp_dir: # location for TensorBoard logging\n  name: helloworld \n  create_tensorboard_logger: True\n  create_checkpoint_callback: True\n  checkpoint_callback_params:\n    ...\n  auto_checkpoint: True # for automated checkpointing\nuse_smp: True \ndistributed_backend: smddp # optimized collectives\n# Start training from pretrained model\nmodel:\n  model_type: llama_v3\n  train_batch_size: 4\n  tensor_model_parallel_degree: 1\n  expert_model_parallel_degree: 1\n  # other model-specific params</code></pre> \n<p>To run this recipe in SageMaker HyperPod with Slurm, you must prepare the SageMaker HyperPod cluster following the <a href=\"https://catalog.workshops.aws/sagemaker-hyperpod/en-US/01-cluster\">cluster setup instruction</a>.</p> \n<p>Then, connect to the SageMaker HyperPod head node, access the Slurm controller, and copy the edited recipe. Next, you run a helper file to generate a Slurm submission script for the job that you can use for a dry run to inspect the content before starting the training job.</p> \n<pre><code>$ python3 main.py --config-path recipes_collection --config-name=config</code></pre> \n<p>After training completion, the trained model is automatically saved to your assigned data location.</p> \n<p>To run this recipe on SageMaker HyperPod with Amazon EKS, clone the recipe from the GitHub repository, install the requirements, and edit the recipe (<code>cluster: k8s</code>) on your laptop. Then, create a link between your laptop and running the EKS cluster and subsequently use the <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/smcluster-getting-started-cli.html\">HyperPod Command Line Interface (CLI)</a> to run the recipe.</p> \n<pre><code>$ hyperpod start-job –recipe fine-tuning/llama/hf_llama3_405b_seq8k_gpu_qlora \\\n--persistent-volume-claims fsx-claim:data \\\n--override-parameters \\\n'{\n  \"recipes.run.name\": \"hf-llama3-405b-seq8k-gpu-qlora\",\n  \"recipes.exp_manager.exp_dir\": \"/data/&lt;your_exp_dir&gt;\",\n  \"cluster\": \"k8s\",\n  \"cluster_type\": \"k8s\",\n  \"container\": \"658645717510.dkr.ecr.&lt;region&gt;.amazonaws.com/smdistributed-modelparallel:2.4.1-gpu-py311-cu121\",\n  \"recipes.model.data.train_dir\": \"&lt;your_train_data_dir&gt;\",\n  \"recipes.model.data.val_dir\": \"&lt;your_val_data_dir&gt;\",\n}'</code></pre> \n<p>You can also run recipe on SageMaker training jobs using <a href=\"https://sagemaker.readthedocs.io/en/stable/\">SageMaker Python SDK</a>. The following example is running PyTorch training scripts on SageMaker training jobs with overriding training recipes.</p> \n<pre><code>...\nrecipe_overrides = {\n    \"run\": {\n        \"results_dir\": \"/opt/ml/model\",\n    },\n    \"exp_manager\": {\n        \"exp_dir\": \"\",\n        \"explicit_log_dir\": \"/opt/ml/output/tensorboard\",\n        \"checkpoint_dir\": \"/opt/ml/checkpoints\",\n    },   \n    \"model\": {\n        \"data\": {\n            \"train_dir\": \"/opt/ml/input/data/train\",\n            \"val_dir\": \"/opt/ml/input/data/val\",\n        },\n    },\n}\npytorch_estimator = PyTorch(\n           output_path=&lt;output_path&gt;,\n           base_job_name=f\"llama-recipe\",\n           role=&lt;role&gt;,\n           instance_type=\"p5.48xlarge\",\n           training_recipe=\"fine-tuning/llama/hf_llama3_405b_seq8k_gpu_qlora\",\n           recipe_overrides=recipe_overrides,\n           sagemaker_session=sagemaker_session,\n           tensorboard_output_config=tensorboard_output_config,\n)\n...</code></pre> \n<p>As training progresses, the model checkpoints are stored on <a href=\"https://aws.amazon.com/s3\">Amazon Simple Storage Service (Amazon S3)</a> with the fully automated checkpointing capability, enabling faster recovery from training faults and instance restarts.</p> \n<p><strong><u>Now available</u></strong><br /> Amazon SageMaker HyperPod recipes are now available in the <a href=\"https://github.com/aws/sagemaker-hyperpod-recipes\">SageMaker HyperPod recipes GitHub repository</a>. To learn more, visit the <a href=\"https://aws.amazon.com/sagemaker-ai/hyperpod/\">SageMaker HyperPod product page</a> and the <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod.html\">Amazon SageMaker AI Developer Guide</a>.</p> \n<p>Give SageMaker HyperPod recipes a try and send feedback to <a href=\"https://repost.aws/tags/TAT80swPyVRPKPcA0rsJYPuA/amazon-sagemaker\">AWS re:Post for SageMaker</a> or through your usual AWS Support contacts.</p> \n<p>— <a href=\"https://twitter.com/channyun\">Channy</a></p>","author":"Channy Yun (윤석찬)","siteTitle":"AWS News Blog","siteHash":"6093e072e4117ec22616e844cb857d03ca62c57a411a8affc77cb5e8b6b15bf6","entryHash":"0fc44f996ca7e5857665c40a6e06ce04bd42f5dba7812947b231138670c31733","category":"Tech"}