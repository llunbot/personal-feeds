{"title":"Craig Federighi Acknowledges Confusion Around Apple Child Safety Features and Explains New Details About Safeguards","link":"https://www.macrumors.com/2021/08/13/federighi-confusion-around-child-safety-details/","date":1628861587000,"content":"Apple's senior vice president of software engineering, Craig Federighi, has today defended Apple's controversial planned <a href=\"https://www.macrumors.com/2021/08/05/apple-new-child-safety-features/\">child safety features</a> in a significant interview with <a href=\"https://www.wsj.com/articles/apple-executive-defends-tools-to-fight-child-porn-acknowledges-privacy-backlash-11628859600\"><em>The Wall Street Journal</em></a>, revealing a number of new details about the safegaurds built into Apple's system for scanning users' photos libraries for CSAM.\r\n<br />\r\n\r\n<br />\r\n<img src=\"https://images.macrumors.com/article-new/2021/06/craig-wwdc-2021-privacy.png\" />\r\n<br />\r\nFederighi admitted that Apple had handled last week's announcement of the two new tools, relating to explicit content in Messages for children and CSAM content to be stored in <a href=\"https://www.macrumors.com/guide/icloud-photo-library/\">iCloud Photos</a> libraries, poorly and acknowledged the confusion around the tools:<blockquote>It's really clear a lot of messages got jumbled pretty badly in terms of how things were understood. We wish that this would've come out a little more clearly for everyone because we feel very positive and strongly about what we're doing.\r\n<br />\r\n\r\n<br />\r\n[...]\r\n<br />\r\n\r\n<br />\r\nIn hindsight, introducing these two features at the same time was a recipe for this kind of confusion. By releasing them at the same time, people technically connected them and got very scared: what's happening with my messages? The answer is...nothing is happening with your messages.</blockquote>\r\n<br />\r\n\r\n<br />\r\nFederighi emphasized that Apple's system will be protected against being taken advantage of by governments with \"multiple levels of auditability.\"\r\n<br />\r\n\r\n<br />\r\n<div></div>\r\n<br />\r\nFederighi also revealed a number of new details around the system's safeguards, such as the fact that a user will need to meet around 30 matches for CSAM content in their <a href=\"https://www.macrumors.com/guide/photos/\">Photos</a> library before Apple is alerted to confirm if those images appear to be genuine instances of CSAM.\r\n<br />\r\n\r\n<br />\r\n<blockquote>If and only if you meet a threshold of something on the order of 30 known child pornographic images matching, only then does Apple know anything about your account and know anything about those images, and at that point, only knows about those images, not about any of your other images. This isn't doing some analysis for did you have a picture of your child in the bathtub? Or, for that matter, did you have a picture of some pornography of any other sort? This is literally only matching on the exact fingerprints of specific known child pornographic images.</blockquote>\r\n<br />\r\n\r\n<br />\r\nHe also pointed out the security advantage of placing the matching process on the <a href=\"https://www.macrumors.com/guide/iphone/\">iPhone</a> directly, rather than it occurring on <a href=\"https://www.macrumors.com/guide/icloud/\">iCloud</a>'s servers.\r\n<br />\r\n\r\n<br />\r\n<blockquote>Because it's on the [phone], security researchers are constantly able to introspect what’s happening in Apple’s [phone] software. So if any changes were made that were to expand the scope of this in some way —in a way that we had committed to not doing—there's verifiability, they can spot that that's happening.</blockquote>\r\n<br />\r\n\r\n<br />\r\nWhen asked if the database of images used to match CSAM content on users' devices could be compromised or have political material inserted, Federighi explained that the database is constructed through the intersection of images from multiple child safety organizations, with at least two being \"in distinct jurisdictions.\"\r\n<br />\r\n\r\n<br />\r\nThese child protection organizations, as well as an independent auditor, will be able to verify that the database of images only consists of content from those entities, according to Federighi.<div>Tags: <a href=\"https://www.macrumors.com/guide/the-wall-street-journal/\">The Wall Street Journal</a>, <a href=\"https://www.macrumors.com/guide/craig-federighi/\">Craig Federighi</a>, <a href=\"https://www.macrumors.com/guide/apple-child-safety-features/\">Apple child safety features</a></div><br />This article, \"<a href=\"https://www.macrumors.com/2021/08/13/federighi-confusion-around-child-safety-details/\">Craig Federighi Acknowledges Confusion Around Apple Child Safety Features and Explains New Details About Safeguards</a>\" first appeared on <a href=\"https://www.macrumors.com\">MacRumors.com</a><br /><br /><a href=\"https://forums.macrumors.com/threads/craig-federighi-acknowledges-confusion-around-apple-child-safety-features-and-explains-new-details-about-safeguards.2307441/\">Discuss this article</a> in our forums<br /><br /><div>\r\n<a href=\"http://feeds.macrumors.com/~ff/MacRumors-All?a=rzxRpXiv6RU:YR7D6imPN1s:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/MacRumors-All?d=6W8y8wAjSf4\" /></a> <a href=\"http://feeds.macrumors.com/~ff/MacRumors-All?a=rzxRpXiv6RU:YR7D6imPN1s:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/MacRumors-All?d=qj6IDK7rITs\" /></a>\r\n</div><img src=\"http://feeds.feedburner.com/~r/MacRumors-All/~4/rzxRpXiv6RU\" />","author":"Hartley Charlton","siteTitle":"MacRumors: Mac News and Rumors - All Stories","siteHash":"4c0f1b1ecc2ed084c9f5be50f1058e33a55cdf9b904dadc33a2071fc2d63e8c1","entryHash":"f412c91d75eb79b0c757825387b5bb0636f3b3b46c900afb8be4a6b089108f9c","category":"Apple"}