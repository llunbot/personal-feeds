{"title":"Tackle complex reasoning tasks with Mistral Large, now available on Amazon Bedrock","link":"https://aws.amazon.com/blogs/aws/tackle-complex-reasoning-tasks-with-mistral-large-now-available-on-amazon-bedrock/","date":1712119069000,"content":"<p>Last month, we announced the availability of two high-performing <a href=\"https://aws.amazon.com/bedrock/mistral/\">Mistral AI models, Mistral 7B and Mixtral 8x7B on Amazon Bedrock</a>. Mistral 7B, as the ﬁrst foundation model of Mistral, supports English text generation tasks with natural coding capabilities. Mixtral 8x7B is a popular, high-quality, sparse Mixture-of-Experts (MoE) model, that is ideal for text summarization, question and answering, text classification, text completion, and code generation.</p> \n<p>Today, we’re announcing the availability of Mistral Large on Amazon Bedrock. Mistral Large is ideal for complex tasks that require substantial reasoning capabilities, or ones that are highly specialized, such as Synthetic Text Generation or Code Generation.</p> \n<p>What you need to know about Mistral Large:</p> \n<ul> \n <li>It’s natively fluent in English, French, Spanish, German, and Italian, with a nuanced understanding of grammar and cultural context.</li> \n <li>It has a 32K token context window allows precise information recall from large documents.</li> \n <li>Its precise instruction-following enables you to design your moderation policies – the folks at Mistral AI used it to set up the system-level moderation of their beta assistant demonstrator <a href=\"https://chat.mistral.ai/\">le Chat</a>. Your first interaction with Large Language Models (LLMs) revolves around prompts. The art of crafting effective prompts is essential for generating desirable responses from LLMs and <a href=\"https://docs.mistral.ai/guides/prompting-capabilities/\">Mistral AI has a guide with example prompts</a> showing different prompting capabilities.</li> \n</ul> \n<p><span><strong>Getting started with Mistral Large</strong></span><br /> To get started with Mistral Large on Bedrock, you first need to get access to the model. On the Amazon Bedrock console, select <strong>Model access</strong>, and then select <strong>Manage model access</strong>. Next, select Mistral Large, and then select <strong>Save changes</strong>.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/03/29/Model-access-Mistral-Large-screenshot2.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/03/29/Model-access-Mistral-Large-screenshot2.png\" alt=\"Model-access-Mistral-Large-screenshot2\" width=\"2218\" height=\"966\" /></a></p> \n<p>Once you have access to Mistral Large, you can use the model on Bedrock. Refresh the base models table to view the updated status.<strong><br /> </strong></p> \n<p>To see the <strong>complex reasoning</strong> aspect of Mistral Large in action I tried the following prompt in the <strong>Playgrounds</strong> section:</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/03/29/Playground-screenshot-Mistral-Large.png\"><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/03/29/Playground-screenshot-Mistral-Large.png\" alt=\"Playground-screenshot-Mistral-Large\" width=\"2224\" height=\"1402\" /></a></p> \n<p><strong>Prompt</strong></p> \n<p><code>Calculate the difference in payment dates between the two customers whose payment amounts are closest to each other in the given dataset:</code></p> \n<p><code>'{</code><br /> <code>\"transaction_id\":{\"0\":\"T1001\",\"1\":\"T1002\",\"2\":\"T1003\",\"3\":\"T1004\",\"4\":\"T1005\"},</code><br /> <code>\"customer_id\":{\"0\":\"C001\",\"1\":\"C002\",\"2\":\"C003\",\"3\":\"C002\",\"4\":\"C001\"},</code><br /> <code>\"payment_amount\":{\"0\":125.5,\"1\":89.99,\"2\":120.0,\"3\":54.3,\"4\":210.2},</code><br /> <code>\"payment_date\":{\"0\":\"2021-10-05\",\"1\":\"2021-10-06\",\"2\":\"2021-10-07\",\"3\":\"2021-10-05\",\"4\":\"2021-10-08\"},</code><br /> <code>\"payment_status\":{\"0\":\"Paid\",\"1\":\"Unpaid\",\"2\":\"Paid\",\"3\":\"Paid\",\"4\":\"Pending\"}</code><br /> <code>}'</code></p> \n<p><strong>Output</strong></p> \n<p><code>To find the difference in payment dates between the two customers whose payment amounts are closest to each other, we first need to identify the two closest payment amounts and then find their respective payment dates.</code></p> \n<p><code>The payment amounts are: 125.5, 89.99, 120.0, 54.3, 210.2</code></p> \n<p><code>The two closest payment amounts are 125.5 and 120.0. The payment date for the payment amount 125.5 is '2021-10-05' and for the payment amount 120.0 is '2021-10-07'.</code></p> \n<p><code>The difference in payment dates can be calculated as follows:</code></p> \n<p><code>Date difference = |'2021-10-07' - '2021-10-05'| = 2 days</code></p> \n<p><code>So, the difference in payment dates between the two customers whose payment amounts are closest to each other is 2 days.</code></p> \n<p>Earlier in this post I mentioned that Mistral Large is natively fluent in several languages, with a nuanced understanding of grammar and cultural context. I tested the model’s understanding of the French language:</p> \n<p><strong>Prompt</strong></p> \n<p><code>Lequel est le plus lourd une livre de fer ou un kilogramme de plume</code></p> \n<p><strong>Output</strong></p> \n<p><code>Un kilogramme de plumes est plus lourd qu'une livre de fer.</code></p> \n<p><code>Pour comprendre cela, il est important de se rappeler que :</code></p> \n<p><code>1. Une livre (lb) est une unité de mesure de masse principalement utilisée aux États-Unis et dans d'autres systèmes de mesure impériaux, tandis qu'un kilogramme (kg) est l'unité de base de masse dans le système international d'unités (SI).</code></p> \n<p><code>2. 1 kilogramme est approximativement égal à 2,2 livres.</code></p> \n<p><code>Donc, un kilogramme de plumes est plus lourd qu'une livre de fer, car il correspond à environ 2,2 livres de plumes.</code></p> \n<p><span><strong>Programmatically interact with Mistral Large<br /> </strong></span>You can also use <a href=\"https://aws.amazon.com/cli/\">AWS Command Line Interface (CLI)</a> and <a href=\"https://aws.amazon.com/developer/tools/\">AWS Software Development Kit (SDK)</a> to make various calls using Amazon Bedrock APIs. Following, is a sample code in Python that interacts with Amazon Bedrock Runtime APIs with AWS SDK. If you specify in the prompt that “You will only respond with a JSON object with the key X, Y, and Z.”, you can use JSON format output in easy downstream tasks:</p> \n<pre><code>import boto3\nimport json\n\nbedrock = boto3.client(service_name=\"bedrock-runtime\")\n\nprompt = \"&lt;s&gt;[INST]You are a summarization system that can provide summaries with associated confidence \nscores. In clear and concise language, provide three short summaries of the following essay, \nalong with their confidence scores. You will only respond with a JSON object with the key Summary \nand Confidence. Do not provide explanations.[/INST]\" \n\nbody = json.dumps({\n    \"prompt\": prompt,\n    \"max_tokens\": 512,\n    \"top_p\": 0.8,\n    \"temperature\": 0.5,\n})\n\nmodelId = \"mistral.mistral-large-2402-v1:0\"\n\naccept = \"application/json\"\ncontentType = \"application/json\"\n\nresponse = bedrock.invoke_model(\n    body=body,\n    modelId=modelId,\n    accept=accept,\n    contentType=contentType\n)\n\nprint(json.loads(response.get('body').read()))</code></pre> \n<p>You can get JSON formatted output as like:</p> \n<pre><code>{ \n   \"Summaries\": [ \n      { \n         \"Summary\": \"The author discusses their early experiences with programming and writing, \nstarting with writing short stories and programming on an IBM 1401 in 9th grade. \nThey then moved on to working with microcomputers, building their own from a Heathkit, \nand eventually convincing their father to buy a TRS-80 in 1980. They wrote simple games, \na program to predict rocket flight trajectories, and a word processor.\", \n         \"Confidence\": 0.9 \n      }, \n      { \n         \"Summary\": \"The author began college as a philosophy major, but found it to be unfulfilling \nand switched to AI. They were inspired by a novel and a PBS documentary, as well as the \npotential for AI to create intelligent machines like those in the novel. Despite this \nexcitement, they eventually realized that the traditional approach to AI was flawed and \nshifted their focus to Lisp.\", \n         \"Confidence\": 0.85 \n      }, \n      { \n         \"Summary\": \"The author briefly worked at Interleaf, where they found that their Lisp skills \nwere highly valued. They eventually left Interleaf to return to RISD, but continued to work \nas a freelance Lisp hacker. While at RISD, they started painting still lives in their bedroom \nat night, which led to them applying to art schools and eventually attending the Accademia \ndi Belli Arti in Florence.\", \n         \"Confidence\": 0.9 \n      } \n   ] \n}</code></pre> \n<p>To learn more prompting capabilities in Mistral AI models, visit <a href=\"https://docs.mistral.ai/guides/prompting-capabilities/\">Mistral AI documentation</a>.</p> \n<p><span><strong>Now Available</strong></span><br /> Mistral Large, along with other Mistral AI models (Mistral 7B and Mixtral 8x7B), is <a href=\"https://aws.amazon.com/bedrock/mistral/\">available today on Amazon Bedrock</a> in the US East (N. Virginia), US West (Oregon), and Europe (Paris) Regions; check the <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/models-regions.html\">full Region list</a> for future updates.</p> \n<p>Share and learn with our generative AI community at <a href=\"https://community.aws/generative-ai\">community.aws</a>. Give Mistral Large a try in the <a href=\"https://console.aws.amazon.com/bedrock\">Amazon Bedrock console</a> today and send feedback to <a href=\"https://repost.aws/tags/TAQeKlaPaNRQ2tWB6P7KrMag/amazon-bedrock\">AWS re:Post for Amazon Bedrock</a> or through your usual AWS Support contacts.</p> \n<p><a href=\"https://aws.amazon.com/blogs/machine-learning/aws-and-mistral-ai-commit-to-democratizing-generative-ai-with-a-strengthened-collaboration/\">Read about our collaboration with Mistral AI</a> and what it means for our customers.</p> \n<p>– <a href=\"https://www.linkedin.com/in/veliswa-boya/\">Veliswa</a>.</p>","author":"Veliswa Boya","siteTitle":"AWS News Blog","siteHash":"6093e072e4117ec22616e844cb857d03ca62c57a411a8affc77cb5e8b6b15bf6","entryHash":"e63ea2924a5d244c40ea09fb173001eb4065b1015b1a95dfcac11a9b39bc3f8c","category":"Tech"}