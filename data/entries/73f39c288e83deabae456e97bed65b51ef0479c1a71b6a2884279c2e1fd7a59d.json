{"title":"New approaches to measuring Nomad performance","link":"https://www.hashicorp.com/blog/new-methods-measuring-nomad-performance","date":1722268800000,"content":"<p>During the work on Nomad’s 1.8 release, the engineering team spent some time analyzing HashiCorp Nomad’s performance, trying to identify bottlenecks and come up with guidelines for our users. In order to do that, we first asked ourselves: In what ways can we measure the performance of a workload orchestrator?</p>\n\n<p>Traditionally in engineering, we consider three elements of systems performance:</p>\n\n<ol>\n<li><strong>Throughput:</strong> The number of operations performed in a given time frame, commonly used in telecommunications when measuring data rate (bytes per second)</li>\n<li><strong>Response time:</strong> The time it takes for a service to respond to a user's input </li>\n<li><strong>Latency:</strong> The time a unit of work needs to wait before it gets processed by a service</li>\n</ol>\n\n<p>Even though Nomad comes with a comprehensive <a href=\"https://developer.hashicorp.com/nomad/docs/operations/metrics-reference\">set of metrics</a>, most of them are low-level and using them to understand server performance can be non-trivial for cluster operators. </p>\n\n<p>At a high level, Nomad servers operate like a <a href=\"https://en.wikipedia.org/wiki/Queue_(abstract_data_type)\">queue</a>. Every time a user submits a job, Nomad creates an evaluation — a unit of work — and this evaluation ends up in a queue for processing. This queue is called an <a href=\"https://www.hashicorp.com/blog/load-shedding-in-the-nomad-eval-broker#missed-heartbeats-and-evaluation-backlog\">evaluation broker</a>. All evaluations initially end up there, and that’s where scheduler workers pick them up in order to create deployment plans and put them in another queue — the plan queue — and create allocations. Allocations declare which tasks in a given job should be run on a given node. The diagram below illustrates this process at a high level:</p>\n<img src=\"https://www.datocms-assets.com/2885/1721748097-nomad-queue.png\" alt=\"Nomad\" /><p>Even though the diagram above illustrates two queues — the eval broker and the plan applier — the eval broker is the queue that captures throughput, response time, and latency, as of Nomad 1.8. The time spent in the plan applier queue also has to be taken into account because scheduler workers return the status to the eval broker, so in order to capture throughput, response time and latency, Nomad 1.8 added three new metrics to the evaluation broker: wait time, process time, and response time:</p>\n\n<h2>Wait time</h2>\n\n<p>The <code>wait_time</code> metric measures the time an evaluation spent in the evaluation broker queue before it got picked up by the scheduler worker. </p>\n\n<table>\n  <tr>\n   <td><strong>Metric</strong>\n   </td>\n   <td><strong>Description</strong>\n   </td>\n   <td><strong>Unit</strong>\n   </td>\n   <td><strong>Type</strong>\n   </td>\n   <td><strong>Labels</strong>\n   </td>\n  </tr>\n  <tr>\n   <td><code>nomad.nomad.broker.wait_time</code>\n   </td>\n   <td>Time elapsed while the evaluation was ready to be processed and waiting to be dequeued.\n   </td>\n   <td>ms\n   </td>\n   <td>Timer\n   </td>\n   <td>job, namespace, type, triggered_by, host\n   </td>\n  </tr>\n</table>\n\n<h2>Process time</h2>\n\n<p>The <code>process_time</code> metric measures how long it took the scheduler worker to process the evaluation, i.e. from the time it left the evaluation broker queue until you received a response. </p>\n\n<table>\n  <tr>\n   <td><strong>Metric</strong>\n   </td>\n   <td><strong>Description</strong>\n   </td>\n   <td><strong>Unit</strong>\n   </td>\n   <td><strong>Type</strong>\n   </td>\n   <td><strong>Labels</strong>\n   </td>\n  </tr>\n  <tr>\n   <td><code>nomad.nomad.broker.process_time</code>\n   </td>\n   <td>Time elapsed while the evaluation was dequeued and finished processing.\n   </td>\n   <td>ms\n   </td>\n   <td>Timer\n   </td>\n   <td>job, namespace, type, triggered_by, host\n   </td>\n  </tr>\n</table>\n\n<h2>Response time</h2>\n\n<p>The <code>response_time</code> metric measures the period of time from when the evaluation first enters the broker queue, until you hear back from the scheduler worker. The result of the evaluation is irrelevant here; only the time spent processing it matters. </p>\n\n<table>\n  <tr>\n   <td><strong>Metric</strong>\n   </td>\n   <td><strong>Description</strong>\n   </td>\n   <td><strong>Unit</strong>\n   </td>\n   <td><strong>Type</strong>\n   </td>\n   <td><strong>Labels</strong>\n   </td>\n  </tr>\n  <tr>\n   <td><code>nomad.nomad.broker.response_time</code>\n   </td>\n   <td>Time elapsed from when the evaluation was last enqueued to when it finished processing.\n   </td>\n   <td>ms\n   </td>\n   <td>Timer\n   </td>\n   <td>job, namespace, type, triggered_by, host\n   </td>\n  </tr>\n</table>\n\n<h1>What about throughput?</h1>\n\n<p>A Nomad server’s throughput can be easily captured with existing metrics. If you define throughput as “the number of successful work units completed in a given time,” then you can use the <code>nomad.nomad.eval.ack</code> metric. It measures how long it takes for the <code>Eval.Ack</code> RPC to complete, and if you’re interested only in how many times the RPC endpoint was called, you can just count the number of calls and divide by the interval time you’re interested in.</p>\n\n<blockquote>\n<p>Throughput = count(EvalAck)/IntervalTime</p>\n</blockquote>\n\n<p>The new Nomad metrics also have the ability to capture the mean utilization of scheduler workers via the <a href=\"https://en.wikipedia.org/wiki/Little%27s_law\">Utilization Law</a>.</p>\n\n<blockquote>\n<p>MeanUtilization = ProcessTime * MeanThroughtput</p>\n</blockquote>\n<img src=\"https://www.datocms-assets.com/2885/1721685887-graph1.png\" alt=\"Nomad\" /><h2>Final notes and further research</h2>\n\n<p>These new metrics introduced in Nomad 1.8 are designed to make it easier for operators to understand their Nomad servers’ performance. Note that these metrics only capture the performance of servers because the team’s primary objective was to create testing scenarios that are as reproducible as possible. There are also many hardware and networking factors that determine overall client performance, but we wanted to separate those factors from server-focused analysis. In the future we hope to revisit Nomad client performance, and of course we’re eager to hear any suggestions on how to improve Nomad metrics via the project's <a href=\"https://github.com/hashicorp/nomad/issues\">issues page</a>.</p>\n\n<p>If you’re interested in trying Nomad, you can download it from the <a href=\"https://www.nomadproject.io/\">project’s website</a>. It’s easy to install and we offer <a href=\"https://developer.hashicorp.com/nomad/tutorials?product_intent=nomad\">many tutorials</a>. To learn more about observability in Nomad, read <a href=\"https://developer.hashicorp.com/nomad/docs/operations/monitoring-nomad\">Monitoring Nomad</a>.</p>\n","author":"Piotr Kazmierczak","siteTitle":"HashiCorp Blog","siteHash":"219aa6310b3388f2335eba49871f4df9581f2c58eaeb5e498363b54e835b7001","entryHash":"73f39c288e83deabae456e97bed65b51ef0479c1a71b6a2884279c2e1fd7a59d","category":"Tech"}